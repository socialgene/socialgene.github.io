{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>SocialGene is a complex ETL workflow centered around analyzing protein and genomic context similarity; built for natural product drug discovery but also with broader applications.</p> <p>The general flow starts with creating a SocialGene Neo4j database. This is done using the Nextflow workflow which handles...</p> <ul> <li>Downloading proteins and/or genomes from NCBI, MIBiG, etc, or using local genomes (genbank format)</li> <li>Creating a set of non-redundant proteins from the input proteins/genomes</li> <li>Downloading HMM models from multiple sources (or using local HMMs), creating a non-redundant set of models, and upconverting to the latest HMMER format (optional)</li> <li>HMM-annotating the non-redundant proteins (optional)</li> <li>Comparing the non-redundant proteins via all-vs-all DIAMOND BLASTp (optional)</li> <li>Clustering the non-redundant proteins with MMseqs2 cascaded clustering (optional)</li> <li>Annotating input genomes with antiSMASH (optional)</li> <li>Downloading and linking all of NCBI Taxonomy (optional)</li> <li>Creating/Cleaning/Transforming all input and produced data for import into a Neo4j graph database</li> <li>Importing/Creating the Neo4j graph database (optional)</li> <li>And more...</li> </ul> <p>Most steps are optional which allows you to customize the output database to your needs.</p>"},{"location":"#what-is-the-output","title":"What is the output?","text":"<ol> <li>Output files for the processing steps. e.g. BLASTp database, MMseqs2 index, non-redundant HMM model file, etc.</li> <li>All outputs files are tab separated, gzipped files which are then imported into Neo4j</li> <li>A Neo4j database containing all the data and relationships</li> </ol> <p>The Neo4j database can be interacted with in multiple ways including:</p> <ol> <li>Through the SocialGene Python package (or Neo4j's Python package or other drivers)</li> <li>Through SocialGene's Django application (web-based GUI) (not yet released)</li> <li>Using \"Neo4j Browser\" or directly from a number of other tools that use or have Neo4j plugins (Cytoscape, Gephi, yFiles/yWorks, etc.)</li> </ol>"},{"location":"#components","title":"Components","text":"<ol> <li>A Nextflow/nf-core pipeline (github.com/socialgene/sgnf)<ul> <li>If you are trying to create your own SocialGene database this is where you want to start</li> <li>It coordinates data download, manipulation and the creation of a SocialGene database</li> </ul> </li> <li>A Python library (github.com/socialgene/sgpy)<ul> <li>Contains the code for most of the data transformations used to build the database</li> <li>Contains functions for manipulation sequence, domain, and genomic context data</li> <li>Contains functions for manipulating SocialGene/Neo4j databases <ul> <li>e.g. adding antismash results and MIBiG metadata which require graph-modifications so can't be directly imported within the Nextflow pipeline</li> </ul> </li> </ul> </li> </ol>"},{"location":"#citations","title":"Citations","text":"<p>Citing software is important because it helps developers justify to granting bodies (or bosses) that their software is useful and should be funded.</p> <p>Socialgene should be cited as:</p> <p>TODO:</p>"},{"location":"#let-us-know-youre-using-socialgene","title":"Let us know you're using SocialGene!","text":"Loading\u2026"},{"location":"doc_guide/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"doc_guide/#documentation-commands","title":"Documentation Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"feedback/","title":"Give Feedback","text":""},{"location":"feedback/#let-us-know-youre-using-socialgene","title":"Let us know you're using SocialGene!","text":"Loading\u2026"},{"location":"python_api/","title":"Reference","text":""},{"location":"system_requirements/","title":"System Requirements","text":""},{"location":"system_requirements/#system-requirements","title":"System Requirements","text":""},{"location":"system_requirements/#os","title":"OS","text":"<p>SocialGene was developed and tested on Ubuntu Linux. Because everything is Docker-ized it should work on Mac or Windows given you have Docker successfully installed on those systems; that said, support for those OSs will be less of a priority than linux.</p>"},{"location":"system_requirements/#cpus","title":"CPUs","text":"<p>Steps using MMseqs2, DIAMOND, HMMER, antiSMASH, etc benefit from having many CPUs available. </p>"},{"location":"system_requirements/#memory","title":"Memory","text":"<p>With larger inputs MMseqs2 and DIAMOND blastp can require significant amounts of RAM.</p>"},{"location":"system_requirements/#disk","title":"Disk","text":"<p>Additionally, and especially, for larger runs, a significant amount of disk space may be required.</p> <p>To get some idea for a larger run:</p> <p>When ~8,500 Streptomyces genomes were used as input for a SocialGene workflow that runs hmmsearch using the antismash, amrfinder, pfam, resfams,and tigrfam HMM databases; MMseqs2 clustering to 90, 70, and 50; incorporates all of NCBI taxonomy; incorporates all MIBiG BGCs; and runs antiSMASH on all input genomes...</p> <p>It required ~90 GB of scratch space (temp files that allow -resume in Nextflow but can be deleted after a successful run). And an additional ~90 GB in the specified <code>outdir</code>.</p> <ul> <li> <p><code>$outdir/socialgene_per_run</code> (70GB)</p> <ul> <li>39 GB antismash output</li> <li>16 GB downloaded input genomes</li> <li>11 GB MMSeqs2 cluster databases</li> <li>3 GB non-redundant FASTA</li> <li>400 MB non-redundant HMM models</li> <li>17 MB workflow stats</li> </ul> </li> <li> <p><code>$outdir/socialgene_neo4j</code> (23 GB)</p> <ul> <li>18.7 GB Neo4j database</li> <li>4.4 GB Neo4j import data</li> </ul> </li> </ul> <p>To run all of RefSeq required a few terabytes, as the &gt;300,000 genomes alone require more than a terabyte.</p> <p>The neo4j database itself will run most performant off of fast hard drives (e.g. ssd/nvme).</p>"},{"location":"system_requirements/#time","title":"Time","text":"<p>Hard to estimate but the ~8,500 Streptomyces example above ran on a single server (dual AMD\u00ae EPYC 7352 24-Core processors, with 1TB RAM) and took 1021.0 CPU hours but completed in 21 hours.</p> <p>I would recommend running the ultraquickstart example or your own input with reduced set of genomes first to help determine appropriate cpu/mem/time settings.</p>"},{"location":"system_requirements/#docker","title":"Docker","text":"<p>Currently there's no Conda distribution of Neo4j or the SocialGene Python package so the Nextflow pipeline will only work with Docker. If there's time/interest we can look at putting the SocialGene Python package onto bioconda and modifying the pipeline as necessary. However, there would still be no Conda distribution of Neo4j so that step of the Nextflow workflow would have to be done manually (using docker or manual installation of Neo4j), though the workflow will export the proper commands to do so.</p>"},{"location":"database_creation/nextflow/","title":"Nextflow","text":""},{"location":"database_creation/nextflow/#overview","title":"Overview","text":"<p>The nextflow workflow makes heavy use of the Python app (so code is testable and can run on its own). Its purpose is to reproducibly gather the required data, run hmmsearch, parse results, and create the neo4j database.</p>"},{"location":"database_creation/nextflow/#check-the-workflow-with-a-test-run","title":"Check the workflow with a test run","text":"<p>If you haven't already, create and enter socialgene's conda environment</p> shell <pre><code>make create_conda\nconda activate socialgene\n</code></pre> <p>Run the nextflow test pipeline. Change the outdirs to the desired locations.  </p> <p>Exectute the below code from socialgene's top directory</p> shell <pre><code> nextflow run nextflow \\\n        -profile test \\\n        --outdir_per_run \"/test/per_run_info\" \\\n        --outdir_neo4j \"/test/neo4j_database\" \\\n        --outdir_long_cache \"/test/long_term_storage\"\n</code></pre> <ul> <li><code>outdir_per_run</code> will contain information about the run (timings, memory used, etc)</li> <li><code>outdir_neo4j</code> will contain both the neo4j database and the files used to create it/import</li> <li>This directory structure is required for import step when creating the neo4j database. If you won't need to \"resume\" the nextflow pipeline the <code>import</code> directory can be deleted</li> <li><code>outdir_long_cache</code> contains things like the HMM models which shouldn't vary between runs of the workflow.</li> </ul>"},{"location":"database_creation/nextflow/#including-blastp-andor-mmseqs2","title":"Including BLASTp and/or MMSEQS2","text":"<p>BLASTp and MMSEQS inter-protein comparisons can be made be includeing the <code>--blastp</code> and/or <code>--mmseqs_steps</code> flags. The <code>--mmseqs_steps</code> arguement expects a comma-delimited string which represents the sequence identity levels to cluster to. For example, to cluster to 90%, 70% and 50% you would use <code>--mmseqs_steps '90,70,50'</code>. Additional DIAMONS and MMSEQS2 settings can be applied via nf-core - style args. </p> <p>Note</p> <p>It is necessary to to put the the numbers in <code>--mmseqs_steps '90,70,50'</code> in descending order</p> <p>Note</p> <p>BLASTp is very time/resource consuming and should only be used on limited datasets.</p> shell <pre><code>nextflow run nextflow \\\n  -profile test \\\n  --outdir_per_run \"/path/to/outdir_per_run\" \\\n  --outdir_neo4j \"/path/to/outdir_neo4j\" \\\n  --outdir_long_cache \"/path/to/outdir_long_cache\" \\\n  -resume \\\n  --fasta_splits 1 \\\n  --blastp true \\\n  --mmseqs_steps '90,70,50'\n</code></pre>"},{"location":"database_creation/nextflow/#introduction","title":"Introduction","text":"<p>nf-core/socialgene is a bioinformatics best-practice analysis pipeline for Repository-scale protein-hmm network annotation and analysis.</p> <p>The pipeline is built using Nextflow, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from nf-core/modules in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!</p> <p>On release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the nf-core website.</p>"},{"location":"database_creation/nextflow/#pipeline-summary","title":"Pipeline summary","text":"<ol> <li>Read QC (<code>FastQC</code>)</li> <li>Present QC for raw reads (<code>MultiQC</code>)</li> </ol>"},{"location":"database_creation/nextflow/#quick-start","title":"Quick Start","text":"<p>The main SocialGene conda environment contains nextflow so, if you haven't already, download miniconda and then create the SocialGene conda environment</p> shell <pre><code>conda env create --file https://raw.githubusercontent.com/chasemc/socialgene/main/nextflow/python_environment.yml\n</code></pre> shell <pre><code>conda activate socialgene\n</code></pre> <p>If you want to use the docker image you can create it with the following command:</p> shell <pre><code>docker build . --tag chasemc/socialgene\n</code></pre>"},{"location":"database_creation/nextflow/#run-the-nextflow-pipeline","title":"Run the Nextflow Pipeline","text":"shell <pre><code># set the directory all files will be placed in \nsg_outdir='/Users/chase/Documents/socialgene_test_run2'\n\n# Run the nextflow pipeline\nnextflow run nextflow \\\n    -profile test,conda \\\n    --outdir_per_run \"${sg_outdir}/per\" \\\n    --outdir_neo4j \"${sg_outdir}/neo\" \\\n    --outdir_long_cache '/Users/chase/Documents/socialgene_data/all_micromonospora/socialgene_results/longy' \\\n    --fasta_splits 0 \\\n    -resume \n</code></pre>"},{"location":"developing/python/amazon_containers/","title":"1","text":"<p>https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-cli.html</p> <p>Download amazon CLI</p> <p>https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</p> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre>"},{"location":"developing/python/classes/","title":"Classes","text":""},{"location":"developing/python/classes/#socialgene.base.socialgene","title":"<code>socialgene.base.socialgene</code>","text":""},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene","title":"<code>SocialGene</code>","text":"<p>               Bases: <code>Molbio</code>, <code>CompareProtein</code>, <code>SequenceParser</code>, <code>Neo4jQuery</code>, <code>HmmerParser</code></p> <p>Main class for building, sotring, working with protein and genomic info</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>class SocialGene(Molbio, CompareProtein, SequenceParser, Neo4jQuery, HmmerParser):\n    \"\"\"Main class for building, sotring, working with protein and genomic info\"\"\"\n\n    # _export_table_names exports tables for the nextflow pipeline\n    _export_table_names = [\n        \"table_protein_to_go\",\n        \"table_assembly_to_locus\",\n        \"table_loci\",\n        \"table_locus_to_protein\",\n        \"table_assembly\",\n        \"table_assembly_to_taxid\",\n        \"table_protein_ids\",\n    ]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.protein_comparison = []\n\n    ########################################\n    # Get info\n    ########################################\n\n    def get_loci_ids(self):\n        return list(self.loci.keys())\n\n    def get_all_gene_clusters(self):\n        for i in self.assemblies.values():\n            for j in i.loci.values():\n                for k in j.gene_clusters:\n                    yield k\n\n    @property\n    def protein_iter(self):\n        for i in self.proteins.values():\n            yield i\n\n    ########################################\n    # Filter\n    ########################################\n\n    def get_protein_domains_from_db(self, protein_id_list):\n        # Search neo4j for a protein with the same hash, if it exists,\n        # retrieve the domain info for it\n        get_protein_domains_result = self.query_neo4j(\n            cypher_name=\"get_protein_domains\",\n            param=protein_id_list,\n        )\n        temp = {\n            \"seq_pro_score\": float(0),\n            \"evalue\": float(0),\n            \"domain_bias\": float(0),\n            \"domain_score\": float(0),\n            \"seq_pro_bias\": float(0),\n            \"hmm_from\": int(0),\n            \"hmm_to\": int(0),\n            \"ali_from\": int(0),\n            \"ali_to\": int(0),\n            \"env_from\": int(0),\n            \"env_to\": int(0),\n        }\n        if get_protein_domains_result:\n            for protein in get_protein_domains_result:\n                for domain in protein[\"domains\"]:\n                    new_dict = temp | domain[\"domain_properties\"]\n                    new_dict[\"hmm_id\"] = domain[\"hmm_id\"]\n                    self.proteins[protein[\"p1.uid\"]].add_domain(\n                        **new_dict,\n                    )\n        else:\n            log.info(\n                \"None of the input protein ids were found or had domains in the database\"\n            )\n\n    def annotate_proteins_with_neo4j(\n        self,\n        protein_uids: List[str] = None,\n        chunk_size: int = 1000,\n        annotate_all: bool = False,\n        progress: bool = False,\n    ):\n        \"\"\"\n        The function `annotate_proteins_with_neo4j` takes a list of protein hash IDs, queries a database\n        for matching proteins, and retrieves their HMM annotations.\n\n        Args:\n          protein_uids (List[str]): A list of protein hash IDs. These are unique identifiers for proteins in the database that you want to annotate.\n          chunk_size (int): The `chunk_size` parameter determines the number of proteins to query at a time. Proteins are divided into chunks to improve efficiency when querying the database.  Defaults to 1000\n          annotate_all (bool): The `annotate_all` parameter is a boolean flag that determines whether to annotate all proteins in the database or not. If set to `True`, all proteins in the database will be annotated. If set to `False`, you need to provide a list of protein hash IDs in the `protein_uids. Defaults to False\n          progress (bool): The `progress` parameter is a boolean flag that determines whether or not to display a progress bar during the execution of the function. If `progress` is set to `True`, a progress bar will be displayed to track the progress of the function. If `progress` is set to `False`. Defaults to False\n\n        \"\"\"\n        if protein_uids is None and annotate_all:\n            protein_uids = self.get_all_feature_uids()\n        elif isinstance(protein_uids, str):\n            protein_uids = [protein_uids]\n        log.info(\n            f\"Searching database for HMM annotations of {len(protein_uids)} proteins.\"\n        )\n        search_result = search_protein_hash(protein_uids)\n        if not any([i for i in search_result.values()]):\n            log.info(\"No identical proteins found in the database.\")\n        else:\n            # log.info(\n            #     f\"{len([i for i in search_result.values() if i])} of {len(protein_uids)} searched proteins were found in the database, pulling their HMM annotations into python...\"\n            # )\n            # get the number of chunks needed to to query \"chunk_size\" proteins at a time\n            prot_len = len(protein_uids)\n            n_chunks = prot_len // chunk_size\n            if n_chunks == 0:\n                n_chunks = 1\n            if n_chunks &gt; (len(protein_uids) - 1):\n                chunked_list = chunk_a_list_with_numpy(\n                    input_list=protein_uids, n_chunks=n_chunks\n                )\n            else:\n                chunked_list = [protein_uids]\n            del protein_uids\n            if progress:\n                with Progress(transient=True) as pg:\n                    task = pg.add_task(\"Progress...\", total=n_chunks)\n                    for protein_id_list in chunked_list:\n                        self.get_protein_domains_from_db(\n                            protein_id_list=protein_id_list\n                        )\n                        pg.update(task, advance=1)\n            else:\n                for protein_id_list in chunked_list:\n                    self.get_protein_domains_from_db(protein_id_list=protein_id_list)\n        return search_result\n\n    def annotate(\n        self, use_neo4j_precalc: bool = False, neo4j_chunk_size: int = 1000, **kwargs\n    ):\n        \"\"\"\n        The `annotate` function is a convenience function that retrieves HMMER annotation for all\n        proteins in a Socialgene object, either from a Neo4j database or by using HMMER directly.\n\n        Args:\n          use_neo4j_precalc (bool): A boolean flag indicating whether to use precalculated domain\n        information from Neo4j for annotation. If set to True, the domains info will be retrieved from\n        Neo4j, and proteins not found in Neo4j will be analyzed with HMMER. If set to False, all\n        proteins will be analyzed with HMMER. Defaults to False\n          neo4j_chunk_size (int): The `neo4j_chunk_size` parameter determines the number of proteins\n        that will be sent to Neo4j at a time for annotation. Defaults to 1000\n        \"\"\"\n        if use_neo4j_precalc:\n            db_res = self.annotate_proteins_with_neo4j(\n                annotate_all=True,\n                chunk_size=neo4j_chunk_size,\n            )\n            n_not_in_db = [k for k, v in db_res.items() if not v]\n        else:\n            n_not_in_db = list(self.proteins.keys())\n        if n_not_in_db:\n            self.annotate_proteins_with_hmmscan(\n                protein_id_list=n_not_in_db,\n                **kwargs,\n            )\n\n    def annotate_proteins_with_hmmscan(\n        self, protein_id_list, hmm_filepath, cpus=None, **kwargs\n    ):\n        \"\"\"Run hmmscan on Protein objects\n\n        Args:\n            protein_id_list (list): list of protein hash ids to run hmmscan on\n            hmm_filepath (str): path to file of HMMs\n            cpus (int): number of parallel processes to spawn\n\n        \"\"\"\n        log.info(f\"Annotating {len(protein_id_list)} proteins with HMMER's hmmscan\")\n        if cpus is None:\n            if cpu_count() &gt; 2:\n                cpus = cpu_count() - 1\n            else:\n                cpus = 1\n        # create a list of proteins as FASTA\n        temp1 = [\n            self.proteins[i].fasta_string_defline_uid\n            for i in protein_id_list\n            if self.proteins[i].sequence\n        ]\n        if not temp1:\n            log.info(\"None of the input sequences contain an amino acid sequence\")\n            return\n        with tempfile.NamedTemporaryFile() as temp_path:\n            hmmer_temp = HMMER(hmm_filepath=hmm_filepath)\n            hmmer_temp.hmmscan(\n                fasta_path=\"-\",\n                input=\"\\n\".join(temp1).encode(),\n                domtblout_path=temp_path.name,\n                overwrite=True,\n                cpus=cpus,\n                **kwargs,\n            )\n            # parse the resulting domtblout file, saving results to the class proteins/domains\n            self.parse_hmmout(temp_path.name, hmmsearch_or_hmmscan=\"hmmscan\")\n\n    def get_proteins_from_neo4j(self):\n        try:\n            ncount = 0\n            with GraphDriver() as db:\n                for i in db.run(\n                    \"\"\"\n                    MATCH (p1:protein)\n                    RETURN p1.uid as uid\n                    \"\"\",\n                ):\n                    _ = self.add_protein(uid=i[\"uid\"])\n                    ncount += 1\n            log.info(f\"Retrieved {ncount} proteins from Neo4j\")\n        except Exception:\n            log.debug(\"Error trying to retrieve proteins from Neo4j\")\n\n    def add_sequences_from_neo4j(self):\n        try:\n            with GraphDriver() as db:\n                for i in db.run(\n                    \"\"\"\n                    MATCH (p1:protein)\n                    WHERE p1.uid in $uid\n                    RETURN p1.uid as uid, p1.sequence as sequence\n                    \"\"\",\n                    uid=[i.uid for i in self.protein_iter],\n                ):\n                    if i[\"uid\"] in self.proteins:\n                        self.proteins[i[\"uid\"]].sequence = i[\"sequence\"]\n        except Exception:\n            log.debug(f\"Error trying to retrieve domains for {self.uid}\")\n\n    def hydrate_from_proteins(self):\n        \"\"\"Given a SocialGene object with proteins, retrieve from a running Neo4j database all locus and asssembly info for those proteins\"\"\"\n        for result in Neo4jQuery.query_neo4j(\n            cypher_name=\"retrieve_protein_locations\",\n            param=list(self.proteins.keys()),\n        ):\n            self.add_assembly(uid=result[\"assembly\"], parent=self)\n            for locus in result[\"loci\"]:\n                _ = self.assemblies[result[\"assembly\"]].add_locus(\n                    external_id=locus[\"locus\"]\n                )\n                for feature in locus[\"features\"]:\n                    _ = (\n                        self.assemblies[result[\"assembly\"]]\n                        .loci[locus[\"locus\"]]\n                        .add_feature(\n                            type=\"protein\",\n                            uid=feature[\"external_id\"],\n                            start=feature[\"locus_start\"],\n                            end=feature[\"locus_end\"],\n                            strand=feature[\"strand\"],\n                        )\n                    )\n\n    def fill_given_locus_range(self, locus_uid, start, end):\n        \"\"\"Given a locus uid that's in the database, pull asssembly, locus, protein info for those proteins\"\"\"\n\n        with GraphDriver() as db:\n            assembly_uid = db.run(\n                \"\"\"\n                MATCH (a1:assembly)&lt;-[:ASSEMBLES_TO]-(n1:nucleotide {uid: $locus_uid})\n                RETURN a1.uid as a_uid\n            \"\"\",\n                locus_uid=locus_uid,\n            ).single()\n        if not assembly_uid:\n            raise ValueError(\"No assembly found in database\")\n        else:\n            assembly_uid = assembly_uid.value()\n        self.add_assembly(uid=assembly_uid, parent=self)\n        with GraphDriver() as db:\n            res = db.run(\n                \"\"\"\n                MATCH (n1:nucleotide {uid: $locus_uid})\n                RETURN n1 as nucleotide_node\n            \"\"\",\n                locus_uid=locus_uid,\n            ).single()\n        if not res:\n            raise ValueError(f\"{locus_uid} not found in database\")\n        external_id = res.value().get(\"external_id\")\n        self.assemblies[assembly_uid].add_locus(external_id=external_id)\n        self.assemblies[assembly_uid].loci[external_id].metadata.update(\n            dict(res.value())\n        )\n\n        with GraphDriver() as db:\n            res = db.run(\n                \"\"\"\n                MATCH (n1:nucleotide {uid: $locus_uid})-[e1:ENCODES]-&gt;(p1:protein)\n                WHERE e1.start &gt;= $start AND e1.start &lt;= $end\n                RETURN e1, p1\n            \"\"\",\n                locus_uid=locus_uid,\n                start=start,\n                end=end,\n            )\n            for feature in res:\n                _ = self.add_protein(uid=feature.value().end_node[\"uid\"])\n                self.assemblies[assembly_uid].loci[external_id].add_feature(\n                    type=\"protein\",\n                    uid=feature.value().end_node[\"uid\"],\n                    **feature.value(),\n                )\n\n        return {\"assembly\": assembly_uid, \"locus\": external_id}\n\n    def _drop_all_cross_origin(self):\n        # Drop likely cross-origin proteins (proteins that are longer than 100k bp)\n        # have to remove from both gene cluster and locus\n        for ak, av in self.assemblies.items():\n            for nk, nv in av.loci.items():\n                self.assemblies[ak].loci[nk].features = {\n                    i for i in nv.features if abs(i.end - i.start) &lt; 100000\n                }\n                for gene_cluster in self.assemblies[ak].loci[nk].gene_clusters:\n                    gene_cluster.features = {\n                        i\n                        for i in gene_cluster.features\n                        if abs(i.end - i.start) &lt; 100000\n                    }\n\n    def hydrate_protein_info(self):\n        \"\"\"Pull name (original identifier) and description of proteins from Neo4j\"\"\"\n        for result in Neo4jQuery.query_neo4j(\n            cypher_name=\"get_protein_info\",\n            param=list(self.proteins.keys()),\n        ):\n            self.proteins[result[\"external_id\"]].external_id = result[\"name\"]\n            self.proteins[result[\"external_id\"]].description = result[\"description\"]\n\n    ########################################\n    # File Outputs\n    ########################################\n\n    def export_all_domains_as_tsv(self, outpath, **kwargs):\n        \"\"\"\n        The function exports all domains as a TSV file, sorted by protein ID and mean envelope position.\n\n        Args:\n          outpath: The `outpath` parameter is the path to the output file where the TSV (Tab-Separated\n        Values) data will be written. It specifies the location and name of the file.\n        \"\"\"\n        _domain_counter = 0\n        with open_write(outpath, **kwargs) as f:\n            tsv_writer = csv.writer(f, delimiter=\"\\t\")\n            # sort to standardize the write order\n            ordered_prot_ids = list(self.proteins.keys())\n            ordered_prot_ids.sort()\n            for prot_id in ordered_prot_ids:\n                # sort to standardize the write order\n                for domain in self.proteins[\n                    prot_id\n                ].domain_list_sorted_by_mean_envelope_position:\n                    _domain_counter += 1\n                    _temp = [prot_id]\n                    _temp.extend(list(domain.all_attributes().values()))\n                    tsv_writer.writerow(_temp)\n        log.info(f\"Wrote {str(_domain_counter)} domains to {outpath}\")\n\n    def ferment_pickle(self, outpath):\n        \"\"\"\n        The function `ferment_pickle` saves a SocialGene object to a Python pickle file.\n\n        Args:\n          outpath: The `outpath` parameter is a string that represents the path where the pickled object will be saved. It should include the file name and extension. For example, \"/path/to/save/object.pickle\".\n        \"\"\"\n        with open(outpath, \"wb\") as handle:\n            pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def eat_pickle(inpath):\n        \"\"\"\n        The `eat_pickle` function reads a saved SocialGene pickle file from the given path and returns a SocialGene object.\n\n        Args:\n          inpath: The `inpath` parameter is a string that represents the path to the file from which we\n        want to load the pickled object.\n\n        Returns:\n          SocialGene object\n        \"\"\"\n        with open(inpath, \"rb\") as handle:\n            return pickle.load(handle)\n\n    def filter_proteins(self, hash_list: List):\n        \"\"\"Filter proteins by list of hash ids\n\n        Args:\n            hash_list (List): List of protein hash identifiers\n\n        Returns:\n            Generator: generator returning tuple of length two -&gt; Generator[(str, 'Protein'])\n        \"\"\"\n        return ((k, v) for k, v in self.proteins.items() if k in hash_list)\n\n    @property\n    def fasta_string_defline_uid(self):\n        for v in self.proteins.values():\n            yield f\"&gt;{v.uid}\\n{v.sequence}\\n\"\n\n    @property\n    def fasta_string_defline_external_id(self):\n        for v in self.proteins.values():\n            yield f\"&gt;{v.external_id}\\n{v.sequence}\\n\"\n\n    def write_fasta(\n        self,\n        outpath,\n        external_id: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Write all proteins to a FASTA file\n\n        Args:\n            outpath (str): path of file that FASTA entries will be appended to\n            external_id (bool, optional): Write protein identifiers as the hash (True) or the original identifier (False). Defaults to False.\n            **kwargs: see print(open_write.__doc__)\n        \"\"\"\n\n        with open_write(filepath=outpath, **kwargs) as handle:\n            counter = 0\n            if external_id:\n                fasta_gen = self.fasta_string_defline_external_id\n            else:\n                fasta_gen = self.fasta_string_defline_uid\n            for i in fasta_gen:\n                counter += 1\n                if \"compression\" in kwargs and kwargs[\"compression\"]:\n                    handle.write(i.encode())\n                else:\n                    handle.writelines(i)\n\n        log.info(f\"Wrote {str(counter)} proteins to {outpath}\")\n\n    def write_n_fasta(self, outdir, n_splits=1, **kwargs):\n        \"\"\"\n        The function `write_n_fasta` exports protein sequences split into multiple fasta files.\n\n        Args:\n          outdir: The `outdir` parameter is a string that specifies the directory where the fasta files\n        will be saved.\n          n_splits: The `n_splits` parameter in the `write_n_fasta` function determines the number of\n        fasta files the protein sequences will be split into. By default, it is set to 1, meaning all\n        the protein sequences will be written into a single fasta file. If you specify a value greater\n        than. Defaults to 1\n        \"\"\"\n\n        # this can be done with itertools.batched in python 3.12\n        def split(a, n):\n            # https://stackoverflow.com/a/2135920\n            k, m = divmod(len(a), n)\n            return (\n                a[i * k + min(i, m) : (i + 1) * k + min(i + 1, m)] for i in range(n)\n            )\n\n        protein_list = split(\n            [value for key, value in sorted(self.proteins.items(), reverse=False)],\n            n_splits,\n        )\n        counter = 1\n        for protein_group in protein_list:\n            with open_write(\n                Path(outdir, f\"fasta_split_{counter}.faa\"), **kwargs\n            ) as handle:\n                for i in protein_group:\n                    handle.writelines(f\"&gt;{i.uid}\\n{i.sequence}\\n\")\n            counter += 1\n\n    def _merge_proteins(self, sg_object):\n        \"\"\"\n        The function merges the proteins from another SOcialGene object into the current object.\n\n        Args:\n          sg_object: The `sg_object` parameter is an object of the same class as the current object. It\n        represents another object that contains a collection of proteins.\n        \"\"\"\n        self.proteins.update(sg_object.proteins)\n\n    def _merge_assemblies(self, sg_object):\n        \"\"\"\n        The function `_merge_assemblies` merges assemblies and loci from `sg_object` into `self` if they\n        do not already exist, and adds protein features to loci if they already exist.\n\n        Args:\n          sg_object: The `sg_object` parameter is an object of the same class as the current object.\n        \"\"\"\n        for assembly_k, assembly_v in sg_object.assemblies.items():\n            if assembly_k not in self.assemblies:\n                self.assemblies[assembly_k] = assembly_v\n            else:\n                for locus_k, locus_v in assembly_v.loci.items():\n                    if locus_k not in self.assemblies[assembly_k].loci:\n                        self.assemblies[assembly_k].loci[locus_k] = locus_v\n                    else:\n                        for feature in (\n                            sg_object.assemblies[assembly_k].loci[locus_k].features\n                        ):\n                            self.assemblies[assembly_k].loci[locus_k].add_feature(\n                                type=\"protein\",\n                                uid=feature.uid,\n                                start=feature.start,\n                                end=feature.end,\n                                strand=feature.strand,\n                            )\n\n    # def deep_merge_with_sg1(sg_object_1, sg_object_2):\n    def __add__(self, sg_object):\n        \"\"\"\n        The function merges proteins and assemblies from two objects and appends protein comparison data\n        to a list or dataframe.\n\n        Args:\n          sg_object: The `sg_object` parameter is an object of the same class as the current object. It\n        represents another instance of the class that you want to add to the current instance.\n        \"\"\"\n        self._merge_proteins(sg_object)\n        self._merge_assemblies(sg_object)\n        self.protein_comparison.extend(sg_object.protein_comparison)\n\n    def write_genbank(self, outpath, compression=None):\n        for assembly in self.assemblies.values():\n            for locus in assembly.loci.values():\n                record = SeqRecord(\n                    Seq(\"\"),\n                    id=locus.external_id,\n                    name=locus.external_id,\n                    description=\"A GenBank file generated by SocialGene.\",\n                    dbxrefs=[f\"Assembly:{locus.parent.uid}\"],\n                )\n                # Add annotation\n                for feature in locus.features_sorted_by_midpoint:\n                    biofeat = SeqFeature(\n                        FeatureLocation(\n                            start=feature.start - 1,\n                            end=feature.end,\n                            strand=feature.strand,\n                        ),\n                        type=feature.type,\n                        qualifiers={\n                            k: v\n                            for k, v in feature.all_attributes().items()\n                            if v and k not in [\"parent\", \"protein\"]\n                        }\n                        | {\"translation\": self.proteins[feature.uid].sequence},\n                    )\n                    record.features.append(biofeat)\n                record.annotations[\"molecule_type\"] = \"DNA\"\n                with open_write(outpath, \"a\", compression) as h:\n                    SeqIO.write(\n                        record,\n                        h,\n                        \"genbank\",\n                    )\n\n    def drop_all_non_protein_features(self, **kwargs):\n        \"\"\"Drop features from all assembly/loci that aren't proteins/pseudo-proteins\n\n        Returns:\n            list: list of features that were removed (if return_removed=True)\n        \"\"\"\n        for a_v in self.assemblies.values():\n            for l_v in a_v.loci.values():\n                l_v.drop_non_protein_features()\n\n    ########################################\n    # OUTPUTS FOR NEXTFLOW\n    ########################################\n\n    def write_table(\n        self,\n        outdir: str,\n        tablename: str,\n        filename: str = None,\n        include_sequences=False,\n        **kwargs,\n    ):\n        \"\"\"\n        The function writes a table to a specified output directory in TSV format, for import into Neo4j.\n\n        Args:\n          outdir (str): The `outdir` parameter is a string that specifies the directory where the output\n        file will be saved.\n          tablename (str): The `tablename` parameter is a string that specifies the name of the table to\n        be written.\n          filename (str): The `filename` parameter is an optional argument that specifies the name of\n        the file to be written. If no `filename` is provided, the `tablename` will be used as the\n        filename.\n        \"\"\"\n        if not filename:\n            filename = tablename\n        outpath = Path(outdir, filename)\n        with open_write(outpath, **kwargs) as handle:\n            tsv_writer = csv.writer(\n                handle, delimiter=\"\\t\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n            )\n            for i in getattr(self, tablename)(include_sequences=include_sequences):\n                tsv_writer.writerow(i)\n\n    def table_protein_to_go(self, **kwargs):\n        \"\"\"\n        The function `table_protein_to_go` iterates through the assemblies, loci, and features of a\n        given object, and yields tuples containing the protein hash and GO term for each feature that\n        has GO terms, for import into Neo4j.\n        \"\"\"\n        for av in self.assemblies.values():\n            for v in av.loci.values():\n                for feature in v.features:\n                    # not all features will have goterms so check here\n                    if feature.goterms:\n                        for goterm in feature.goterms:\n                            yield (\n                                feature.uid,\n                                goterm.removeprefix(\"GO:\").strip(),\n                            )\n\n    def table_locus_to_protein(self, **kwargs):\n        \"\"\"\n        The function `table_locus_to_protein` generates a table of protein information from each locus, for import into Neo4j.\n        \"\"\"\n        for ak, av in self.assemblies.items():\n            for k, loci in av.loci.items():\n                temp_list = list(loci.features)\n                # sort features by id then start to maintain consistent output\n                temp_list.sort(key=attrgetter(\"uid\"))\n                temp_list.sort(key=attrgetter(\"start\"))\n                for feature in temp_list:\n                    if feature.feature_is_protein():\n                        yield (\n                            feature.parent.uid,\n                            feature.uid,\n                            feature.external_id,\n                            feature.locus_tag,\n                            feature.start,\n                            feature.end,\n                            feature.strand,\n                            feature.description,\n                            feature.partial_on_complete_genome,\n                            feature.missing_start,\n                            feature.missing_stop,\n                            feature.internal_stop,\n                            feature.partial_in_the_middle_of_a_contig,\n                            feature.missing_N_terminus,\n                            feature.missing_C_terminus,\n                            feature.frameshifted,\n                            feature.too_short_partial_abutting_assembly_gap,\n                            feature.incomplete,\n                        )\n\n    def table_protein_ids(self, include_sequences=False, **kwargs):\n        \"\"\"Protein hash id table for import into Neo4j\"\"\"\n        for protein in self.proteins.values():\n            if include_sequences:\n                yield (protein.uid, protein.crc64, protein.sequence)\n            else:\n                yield (protein.uid, protein.crc64)\n\n    def table_assembly_to_locus(self, **kwargs):\n        \"\"\"Assembly to locus table for import into Neo4j\n\n        Args:\n            outdir (str, optional): Defaults to \".\".\n        \"\"\"\n        for ak, av in self.assemblies.items():\n            for k, v in av.loci.items():\n                #  [\"assembly\", \"internal_locus_id\"]\n                yield (ak, v.uid)\n\n    def table_loci(self, **kwargs):\n        \"\"\"\n        Generate a table of loci information.\n\n        Yields:\n            tuple: (internal_locus_id, external_locus_id, [info])\n        \"\"\"\n        for _av in self.assemblies.values():\n            for locus in _av.loci.values():\n                yield tuple(\n                    [locus.uid]\n                    + [locus.external_id]\n                    + list(locus.metadata.all_attributes().values())\n                )\n\n    def table_assembly(self, **kwargs):\n        \"\"\"Assembly table for import into Neo4j\n\n        Args:\n            outdir (str, optional): Defaults to \".\".\n        \"\"\"\n        for assembly in self.assemblies.values():\n            yield tuple(\n                [assembly.uid] + list(assembly.metadata.all_attributes().values())\n            )\n\n    def table_assembly_to_taxid(self, **kwargs):\n        \"\"\"Assembly table for import into Neo4j\n\n        Args:\n            outdir (str, optional): Defaults to \".\".\n        \"\"\"\n        for k, v in self.assemblies.items():\n            if v.taxid:\n                yield (k, v.taxid)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.__add__","title":"<code>__add__(sg_object)</code>","text":"<p>The function merges proteins and assemblies from two objects and appends protein comparison data to a list or dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>sg_object</code> <p>The <code>sg_object</code> parameter is an object of the same class as the current object. It</p> required <p>represents another instance of the class that you want to add to the current instance.</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def __add__(self, sg_object):\n    \"\"\"\n    The function merges proteins and assemblies from two objects and appends protein comparison data\n    to a list or dataframe.\n\n    Args:\n      sg_object: The `sg_object` parameter is an object of the same class as the current object. It\n    represents another instance of the class that you want to add to the current instance.\n    \"\"\"\n    self._merge_proteins(sg_object)\n    self._merge_assemblies(sg_object)\n    self.protein_comparison.extend(sg_object.protein_comparison)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.annotate","title":"<code>annotate(use_neo4j_precalc=False, neo4j_chunk_size=1000, **kwargs)</code>","text":"<p>The <code>annotate</code> function is a convenience function that retrieves HMMER annotation for all proteins in a Socialgene object, either from a Neo4j database or by using HMMER directly.</p> <p>Parameters:</p> Name Type Description Default <code>use_neo4j_precalc</code> <code>bool</code> <p>A boolean flag indicating whether to use precalculated domain</p> <code>False</code> <p>information from Neo4j for annotation. If set to True, the domains info will be retrieved from Neo4j, and proteins not found in Neo4j will be analyzed with HMMER. If set to False, all proteins will be analyzed with HMMER. Defaults to False   neo4j_chunk_size (int): The <code>neo4j_chunk_size</code> parameter determines the number of proteins that will be sent to Neo4j at a time for annotation. Defaults to 1000</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def annotate(\n    self, use_neo4j_precalc: bool = False, neo4j_chunk_size: int = 1000, **kwargs\n):\n    \"\"\"\n    The `annotate` function is a convenience function that retrieves HMMER annotation for all\n    proteins in a Socialgene object, either from a Neo4j database or by using HMMER directly.\n\n    Args:\n      use_neo4j_precalc (bool): A boolean flag indicating whether to use precalculated domain\n    information from Neo4j for annotation. If set to True, the domains info will be retrieved from\n    Neo4j, and proteins not found in Neo4j will be analyzed with HMMER. If set to False, all\n    proteins will be analyzed with HMMER. Defaults to False\n      neo4j_chunk_size (int): The `neo4j_chunk_size` parameter determines the number of proteins\n    that will be sent to Neo4j at a time for annotation. Defaults to 1000\n    \"\"\"\n    if use_neo4j_precalc:\n        db_res = self.annotate_proteins_with_neo4j(\n            annotate_all=True,\n            chunk_size=neo4j_chunk_size,\n        )\n        n_not_in_db = [k for k, v in db_res.items() if not v]\n    else:\n        n_not_in_db = list(self.proteins.keys())\n    if n_not_in_db:\n        self.annotate_proteins_with_hmmscan(\n            protein_id_list=n_not_in_db,\n            **kwargs,\n        )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.annotate_proteins_with_hmmscan","title":"<code>annotate_proteins_with_hmmscan(protein_id_list, hmm_filepath, cpus=None, **kwargs)</code>","text":"<p>Run hmmscan on Protein objects</p> <p>Parameters:</p> Name Type Description Default <code>protein_id_list</code> <code>list</code> <p>list of protein hash ids to run hmmscan on</p> required <code>hmm_filepath</code> <code>str</code> <p>path to file of HMMs</p> required <code>cpus</code> <code>int</code> <p>number of parallel processes to spawn</p> <code>None</code> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def annotate_proteins_with_hmmscan(\n    self, protein_id_list, hmm_filepath, cpus=None, **kwargs\n):\n    \"\"\"Run hmmscan on Protein objects\n\n    Args:\n        protein_id_list (list): list of protein hash ids to run hmmscan on\n        hmm_filepath (str): path to file of HMMs\n        cpus (int): number of parallel processes to spawn\n\n    \"\"\"\n    log.info(f\"Annotating {len(protein_id_list)} proteins with HMMER's hmmscan\")\n    if cpus is None:\n        if cpu_count() &gt; 2:\n            cpus = cpu_count() - 1\n        else:\n            cpus = 1\n    # create a list of proteins as FASTA\n    temp1 = [\n        self.proteins[i].fasta_string_defline_uid\n        for i in protein_id_list\n        if self.proteins[i].sequence\n    ]\n    if not temp1:\n        log.info(\"None of the input sequences contain an amino acid sequence\")\n        return\n    with tempfile.NamedTemporaryFile() as temp_path:\n        hmmer_temp = HMMER(hmm_filepath=hmm_filepath)\n        hmmer_temp.hmmscan(\n            fasta_path=\"-\",\n            input=\"\\n\".join(temp1).encode(),\n            domtblout_path=temp_path.name,\n            overwrite=True,\n            cpus=cpus,\n            **kwargs,\n        )\n        # parse the resulting domtblout file, saving results to the class proteins/domains\n        self.parse_hmmout(temp_path.name, hmmsearch_or_hmmscan=\"hmmscan\")\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.annotate_proteins_with_neo4j","title":"<code>annotate_proteins_with_neo4j(protein_uids=None, chunk_size=1000, annotate_all=False, progress=False)</code>","text":"<p>The function <code>annotate_proteins_with_neo4j</code> takes a list of protein hash IDs, queries a database for matching proteins, and retrieves their HMM annotations.</p> <p>Parameters:</p> Name Type Description Default <code>protein_uids</code> <code>List[str]</code> <p>A list of protein hash IDs. These are unique identifiers for proteins in the database that you want to annotate.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The <code>chunk_size</code> parameter determines the number of proteins to query at a time. Proteins are divided into chunks to improve efficiency when querying the database.  Defaults to 1000</p> <code>1000</code> <code>annotate_all</code> <code>bool</code> <p>The <code>annotate_all</code> parameter is a boolean flag that determines whether to annotate all proteins in the database or not. If set to <code>True</code>, all proteins in the database will be annotated. If set to <code>False</code>, you need to provide a list of protein hash IDs in the `protein_uids. Defaults to False</p> <code>False</code> <code>progress</code> <code>bool</code> <p>The <code>progress</code> parameter is a boolean flag that determines whether or not to display a progress bar during the execution of the function. If <code>progress</code> is set to <code>True</code>, a progress bar will be displayed to track the progress of the function. If <code>progress</code> is set to <code>False</code>. Defaults to False</p> <code>False</code> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def annotate_proteins_with_neo4j(\n    self,\n    protein_uids: List[str] = None,\n    chunk_size: int = 1000,\n    annotate_all: bool = False,\n    progress: bool = False,\n):\n    \"\"\"\n    The function `annotate_proteins_with_neo4j` takes a list of protein hash IDs, queries a database\n    for matching proteins, and retrieves their HMM annotations.\n\n    Args:\n      protein_uids (List[str]): A list of protein hash IDs. These are unique identifiers for proteins in the database that you want to annotate.\n      chunk_size (int): The `chunk_size` parameter determines the number of proteins to query at a time. Proteins are divided into chunks to improve efficiency when querying the database.  Defaults to 1000\n      annotate_all (bool): The `annotate_all` parameter is a boolean flag that determines whether to annotate all proteins in the database or not. If set to `True`, all proteins in the database will be annotated. If set to `False`, you need to provide a list of protein hash IDs in the `protein_uids. Defaults to False\n      progress (bool): The `progress` parameter is a boolean flag that determines whether or not to display a progress bar during the execution of the function. If `progress` is set to `True`, a progress bar will be displayed to track the progress of the function. If `progress` is set to `False`. Defaults to False\n\n    \"\"\"\n    if protein_uids is None and annotate_all:\n        protein_uids = self.get_all_feature_uids()\n    elif isinstance(protein_uids, str):\n        protein_uids = [protein_uids]\n    log.info(\n        f\"Searching database for HMM annotations of {len(protein_uids)} proteins.\"\n    )\n    search_result = search_protein_hash(protein_uids)\n    if not any([i for i in search_result.values()]):\n        log.info(\"No identical proteins found in the database.\")\n    else:\n        # log.info(\n        #     f\"{len([i for i in search_result.values() if i])} of {len(protein_uids)} searched proteins were found in the database, pulling their HMM annotations into python...\"\n        # )\n        # get the number of chunks needed to to query \"chunk_size\" proteins at a time\n        prot_len = len(protein_uids)\n        n_chunks = prot_len // chunk_size\n        if n_chunks == 0:\n            n_chunks = 1\n        if n_chunks &gt; (len(protein_uids) - 1):\n            chunked_list = chunk_a_list_with_numpy(\n                input_list=protein_uids, n_chunks=n_chunks\n            )\n        else:\n            chunked_list = [protein_uids]\n        del protein_uids\n        if progress:\n            with Progress(transient=True) as pg:\n                task = pg.add_task(\"Progress...\", total=n_chunks)\n                for protein_id_list in chunked_list:\n                    self.get_protein_domains_from_db(\n                        protein_id_list=protein_id_list\n                    )\n                    pg.update(task, advance=1)\n        else:\n            for protein_id_list in chunked_list:\n                self.get_protein_domains_from_db(protein_id_list=protein_id_list)\n    return search_result\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.drop_all_non_protein_features","title":"<code>drop_all_non_protein_features(**kwargs)</code>","text":"<p>Drop features from all assembly/loci that aren't proteins/pseudo-proteins</p> <p>Returns:</p> Name Type Description <code>list</code> <p>list of features that were removed (if return_removed=True)</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def drop_all_non_protein_features(self, **kwargs):\n    \"\"\"Drop features from all assembly/loci that aren't proteins/pseudo-proteins\n\n    Returns:\n        list: list of features that were removed (if return_removed=True)\n    \"\"\"\n    for a_v in self.assemblies.values():\n        for l_v in a_v.loci.values():\n            l_v.drop_non_protein_features()\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.eat_pickle","title":"<code>eat_pickle(inpath)</code>  <code>staticmethod</code>","text":"<p>The <code>eat_pickle</code> function reads a saved SocialGene pickle file from the given path and returns a SocialGene object.</p> <p>Parameters:</p> Name Type Description Default <code>inpath</code> <p>The <code>inpath</code> parameter is a string that represents the path to the file from which we</p> required <p>want to load the pickled object.</p> <p>Returns:</p> Type Description <p>SocialGene object</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>@staticmethod\ndef eat_pickle(inpath):\n    \"\"\"\n    The `eat_pickle` function reads a saved SocialGene pickle file from the given path and returns a SocialGene object.\n\n    Args:\n      inpath: The `inpath` parameter is a string that represents the path to the file from which we\n    want to load the pickled object.\n\n    Returns:\n      SocialGene object\n    \"\"\"\n    with open(inpath, \"rb\") as handle:\n        return pickle.load(handle)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.export_all_domains_as_tsv","title":"<code>export_all_domains_as_tsv(outpath, **kwargs)</code>","text":"<p>The function exports all domains as a TSV file, sorted by protein ID and mean envelope position.</p> <p>Parameters:</p> Name Type Description Default <code>outpath</code> <p>The <code>outpath</code> parameter is the path to the output file where the TSV (Tab-Separated</p> required <p>Values) data will be written. It specifies the location and name of the file.</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def export_all_domains_as_tsv(self, outpath, **kwargs):\n    \"\"\"\n    The function exports all domains as a TSV file, sorted by protein ID and mean envelope position.\n\n    Args:\n      outpath: The `outpath` parameter is the path to the output file where the TSV (Tab-Separated\n    Values) data will be written. It specifies the location and name of the file.\n    \"\"\"\n    _domain_counter = 0\n    with open_write(outpath, **kwargs) as f:\n        tsv_writer = csv.writer(f, delimiter=\"\\t\")\n        # sort to standardize the write order\n        ordered_prot_ids = list(self.proteins.keys())\n        ordered_prot_ids.sort()\n        for prot_id in ordered_prot_ids:\n            # sort to standardize the write order\n            for domain in self.proteins[\n                prot_id\n            ].domain_list_sorted_by_mean_envelope_position:\n                _domain_counter += 1\n                _temp = [prot_id]\n                _temp.extend(list(domain.all_attributes().values()))\n                tsv_writer.writerow(_temp)\n    log.info(f\"Wrote {str(_domain_counter)} domains to {outpath}\")\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.ferment_pickle","title":"<code>ferment_pickle(outpath)</code>","text":"<p>The function <code>ferment_pickle</code> saves a SocialGene object to a Python pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>outpath</code> <p>The <code>outpath</code> parameter is a string that represents the path where the pickled object will be saved. It should include the file name and extension. For example, \"/path/to/save/object.pickle\".</p> required Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def ferment_pickle(self, outpath):\n    \"\"\"\n    The function `ferment_pickle` saves a SocialGene object to a Python pickle file.\n\n    Args:\n      outpath: The `outpath` parameter is a string that represents the path where the pickled object will be saved. It should include the file name and extension. For example, \"/path/to/save/object.pickle\".\n    \"\"\"\n    with open(outpath, \"wb\") as handle:\n        pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.fill_given_locus_range","title":"<code>fill_given_locus_range(locus_uid, start, end)</code>","text":"<p>Given a locus uid that's in the database, pull asssembly, locus, protein info for those proteins</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def fill_given_locus_range(self, locus_uid, start, end):\n    \"\"\"Given a locus uid that's in the database, pull asssembly, locus, protein info for those proteins\"\"\"\n\n    with GraphDriver() as db:\n        assembly_uid = db.run(\n            \"\"\"\n            MATCH (a1:assembly)&lt;-[:ASSEMBLES_TO]-(n1:nucleotide {uid: $locus_uid})\n            RETURN a1.uid as a_uid\n        \"\"\",\n            locus_uid=locus_uid,\n        ).single()\n    if not assembly_uid:\n        raise ValueError(\"No assembly found in database\")\n    else:\n        assembly_uid = assembly_uid.value()\n    self.add_assembly(uid=assembly_uid, parent=self)\n    with GraphDriver() as db:\n        res = db.run(\n            \"\"\"\n            MATCH (n1:nucleotide {uid: $locus_uid})\n            RETURN n1 as nucleotide_node\n        \"\"\",\n            locus_uid=locus_uid,\n        ).single()\n    if not res:\n        raise ValueError(f\"{locus_uid} not found in database\")\n    external_id = res.value().get(\"external_id\")\n    self.assemblies[assembly_uid].add_locus(external_id=external_id)\n    self.assemblies[assembly_uid].loci[external_id].metadata.update(\n        dict(res.value())\n    )\n\n    with GraphDriver() as db:\n        res = db.run(\n            \"\"\"\n            MATCH (n1:nucleotide {uid: $locus_uid})-[e1:ENCODES]-&gt;(p1:protein)\n            WHERE e1.start &gt;= $start AND e1.start &lt;= $end\n            RETURN e1, p1\n        \"\"\",\n            locus_uid=locus_uid,\n            start=start,\n            end=end,\n        )\n        for feature in res:\n            _ = self.add_protein(uid=feature.value().end_node[\"uid\"])\n            self.assemblies[assembly_uid].loci[external_id].add_feature(\n                type=\"protein\",\n                uid=feature.value().end_node[\"uid\"],\n                **feature.value(),\n            )\n\n    return {\"assembly\": assembly_uid, \"locus\": external_id}\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.filter_proteins","title":"<code>filter_proteins(hash_list)</code>","text":"<p>Filter proteins by list of hash ids</p> <p>Parameters:</p> Name Type Description Default <code>hash_list</code> <code>List</code> <p>List of protein hash identifiers</p> required <p>Returns:</p> Name Type Description <code>Generator</code> <p>generator returning tuple of length two -&gt; Generator[(str, 'Protein'])</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def filter_proteins(self, hash_list: List):\n    \"\"\"Filter proteins by list of hash ids\n\n    Args:\n        hash_list (List): List of protein hash identifiers\n\n    Returns:\n        Generator: generator returning tuple of length two -&gt; Generator[(str, 'Protein'])\n    \"\"\"\n    return ((k, v) for k, v in self.proteins.items() if k in hash_list)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.hydrate_from_proteins","title":"<code>hydrate_from_proteins()</code>","text":"<p>Given a SocialGene object with proteins, retrieve from a running Neo4j database all locus and asssembly info for those proteins</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def hydrate_from_proteins(self):\n    \"\"\"Given a SocialGene object with proteins, retrieve from a running Neo4j database all locus and asssembly info for those proteins\"\"\"\n    for result in Neo4jQuery.query_neo4j(\n        cypher_name=\"retrieve_protein_locations\",\n        param=list(self.proteins.keys()),\n    ):\n        self.add_assembly(uid=result[\"assembly\"], parent=self)\n        for locus in result[\"loci\"]:\n            _ = self.assemblies[result[\"assembly\"]].add_locus(\n                external_id=locus[\"locus\"]\n            )\n            for feature in locus[\"features\"]:\n                _ = (\n                    self.assemblies[result[\"assembly\"]]\n                    .loci[locus[\"locus\"]]\n                    .add_feature(\n                        type=\"protein\",\n                        uid=feature[\"external_id\"],\n                        start=feature[\"locus_start\"],\n                        end=feature[\"locus_end\"],\n                        strand=feature[\"strand\"],\n                    )\n                )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.hydrate_protein_info","title":"<code>hydrate_protein_info()</code>","text":"<p>Pull name (original identifier) and description of proteins from Neo4j</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def hydrate_protein_info(self):\n    \"\"\"Pull name (original identifier) and description of proteins from Neo4j\"\"\"\n    for result in Neo4jQuery.query_neo4j(\n        cypher_name=\"get_protein_info\",\n        param=list(self.proteins.keys()),\n    ):\n        self.proteins[result[\"external_id\"]].external_id = result[\"name\"]\n        self.proteins[result[\"external_id\"]].description = result[\"description\"]\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.table_assembly","title":"<code>table_assembly(**kwargs)</code>","text":"<p>Assembly table for import into Neo4j</p> <p>Parameters:</p> Name Type Description Default <code>outdir</code> <code>str</code> <p>Defaults to \".\".</p> required Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def table_assembly(self, **kwargs):\n    \"\"\"Assembly table for import into Neo4j\n\n    Args:\n        outdir (str, optional): Defaults to \".\".\n    \"\"\"\n    for assembly in self.assemblies.values():\n        yield tuple(\n            [assembly.uid] + list(assembly.metadata.all_attributes().values())\n        )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.table_assembly_to_locus","title":"<code>table_assembly_to_locus(**kwargs)</code>","text":"<p>Assembly to locus table for import into Neo4j</p> <p>Parameters:</p> Name Type Description Default <code>outdir</code> <code>str</code> <p>Defaults to \".\".</p> required Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def table_assembly_to_locus(self, **kwargs):\n    \"\"\"Assembly to locus table for import into Neo4j\n\n    Args:\n        outdir (str, optional): Defaults to \".\".\n    \"\"\"\n    for ak, av in self.assemblies.items():\n        for k, v in av.loci.items():\n            #  [\"assembly\", \"internal_locus_id\"]\n            yield (ak, v.uid)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.table_assembly_to_taxid","title":"<code>table_assembly_to_taxid(**kwargs)</code>","text":"<p>Assembly table for import into Neo4j</p> <p>Parameters:</p> Name Type Description Default <code>outdir</code> <code>str</code> <p>Defaults to \".\".</p> required Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def table_assembly_to_taxid(self, **kwargs):\n    \"\"\"Assembly table for import into Neo4j\n\n    Args:\n        outdir (str, optional): Defaults to \".\".\n    \"\"\"\n    for k, v in self.assemblies.items():\n        if v.taxid:\n            yield (k, v.taxid)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.table_loci","title":"<code>table_loci(**kwargs)</code>","text":"<p>Generate a table of loci information.</p> <p>Yields:</p> Name Type Description <code>tuple</code> <p>(internal_locus_id, external_locus_id, [info])</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def table_loci(self, **kwargs):\n    \"\"\"\n    Generate a table of loci information.\n\n    Yields:\n        tuple: (internal_locus_id, external_locus_id, [info])\n    \"\"\"\n    for _av in self.assemblies.values():\n        for locus in _av.loci.values():\n            yield tuple(\n                [locus.uid]\n                + [locus.external_id]\n                + list(locus.metadata.all_attributes().values())\n            )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.table_locus_to_protein","title":"<code>table_locus_to_protein(**kwargs)</code>","text":"<p>The function <code>table_locus_to_protein</code> generates a table of protein information from each locus, for import into Neo4j.</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def table_locus_to_protein(self, **kwargs):\n    \"\"\"\n    The function `table_locus_to_protein` generates a table of protein information from each locus, for import into Neo4j.\n    \"\"\"\n    for ak, av in self.assemblies.items():\n        for k, loci in av.loci.items():\n            temp_list = list(loci.features)\n            # sort features by id then start to maintain consistent output\n            temp_list.sort(key=attrgetter(\"uid\"))\n            temp_list.sort(key=attrgetter(\"start\"))\n            for feature in temp_list:\n                if feature.feature_is_protein():\n                    yield (\n                        feature.parent.uid,\n                        feature.uid,\n                        feature.external_id,\n                        feature.locus_tag,\n                        feature.start,\n                        feature.end,\n                        feature.strand,\n                        feature.description,\n                        feature.partial_on_complete_genome,\n                        feature.missing_start,\n                        feature.missing_stop,\n                        feature.internal_stop,\n                        feature.partial_in_the_middle_of_a_contig,\n                        feature.missing_N_terminus,\n                        feature.missing_C_terminus,\n                        feature.frameshifted,\n                        feature.too_short_partial_abutting_assembly_gap,\n                        feature.incomplete,\n                    )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.table_protein_ids","title":"<code>table_protein_ids(include_sequences=False, **kwargs)</code>","text":"<p>Protein hash id table for import into Neo4j</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def table_protein_ids(self, include_sequences=False, **kwargs):\n    \"\"\"Protein hash id table for import into Neo4j\"\"\"\n    for protein in self.proteins.values():\n        if include_sequences:\n            yield (protein.uid, protein.crc64, protein.sequence)\n        else:\n            yield (protein.uid, protein.crc64)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.table_protein_to_go","title":"<code>table_protein_to_go(**kwargs)</code>","text":"<p>The function <code>table_protein_to_go</code> iterates through the assemblies, loci, and features of a given object, and yields tuples containing the protein hash and GO term for each feature that has GO terms, for import into Neo4j.</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def table_protein_to_go(self, **kwargs):\n    \"\"\"\n    The function `table_protein_to_go` iterates through the assemblies, loci, and features of a\n    given object, and yields tuples containing the protein hash and GO term for each feature that\n    has GO terms, for import into Neo4j.\n    \"\"\"\n    for av in self.assemblies.values():\n        for v in av.loci.values():\n            for feature in v.features:\n                # not all features will have goterms so check here\n                if feature.goterms:\n                    for goterm in feature.goterms:\n                        yield (\n                            feature.uid,\n                            goterm.removeprefix(\"GO:\").strip(),\n                        )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.write_fasta","title":"<code>write_fasta(outpath, external_id=False, **kwargs)</code>","text":"<p>Write all proteins to a FASTA file</p> <p>Parameters:</p> Name Type Description Default <code>outpath</code> <code>str</code> <p>path of file that FASTA entries will be appended to</p> required <code>external_id</code> <code>bool</code> <p>Write protein identifiers as the hash (True) or the original identifier (False). Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>see print(open_write.doc)</p> <code>{}</code> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def write_fasta(\n    self,\n    outpath,\n    external_id: bool = False,\n    **kwargs,\n):\n    \"\"\"Write all proteins to a FASTA file\n\n    Args:\n        outpath (str): path of file that FASTA entries will be appended to\n        external_id (bool, optional): Write protein identifiers as the hash (True) or the original identifier (False). Defaults to False.\n        **kwargs: see print(open_write.__doc__)\n    \"\"\"\n\n    with open_write(filepath=outpath, **kwargs) as handle:\n        counter = 0\n        if external_id:\n            fasta_gen = self.fasta_string_defline_external_id\n        else:\n            fasta_gen = self.fasta_string_defline_uid\n        for i in fasta_gen:\n            counter += 1\n            if \"compression\" in kwargs and kwargs[\"compression\"]:\n                handle.write(i.encode())\n            else:\n                handle.writelines(i)\n\n    log.info(f\"Wrote {str(counter)} proteins to {outpath}\")\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.write_n_fasta","title":"<code>write_n_fasta(outdir, n_splits=1, **kwargs)</code>","text":"<p>The function <code>write_n_fasta</code> exports protein sequences split into multiple fasta files.</p> <p>Parameters:</p> Name Type Description Default <code>outdir</code> <p>The <code>outdir</code> parameter is a string that specifies the directory where the fasta files</p> required <p>will be saved.   n_splits: The <code>n_splits</code> parameter in the <code>write_n_fasta</code> function determines the number of fasta files the protein sequences will be split into. By default, it is set to 1, meaning all the protein sequences will be written into a single fasta file. If you specify a value greater than. Defaults to 1</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def write_n_fasta(self, outdir, n_splits=1, **kwargs):\n    \"\"\"\n    The function `write_n_fasta` exports protein sequences split into multiple fasta files.\n\n    Args:\n      outdir: The `outdir` parameter is a string that specifies the directory where the fasta files\n    will be saved.\n      n_splits: The `n_splits` parameter in the `write_n_fasta` function determines the number of\n    fasta files the protein sequences will be split into. By default, it is set to 1, meaning all\n    the protein sequences will be written into a single fasta file. If you specify a value greater\n    than. Defaults to 1\n    \"\"\"\n\n    # this can be done with itertools.batched in python 3.12\n    def split(a, n):\n        # https://stackoverflow.com/a/2135920\n        k, m = divmod(len(a), n)\n        return (\n            a[i * k + min(i, m) : (i + 1) * k + min(i + 1, m)] for i in range(n)\n        )\n\n    protein_list = split(\n        [value for key, value in sorted(self.proteins.items(), reverse=False)],\n        n_splits,\n    )\n    counter = 1\n    for protein_group in protein_list:\n        with open_write(\n            Path(outdir, f\"fasta_split_{counter}.faa\"), **kwargs\n        ) as handle:\n            for i in protein_group:\n                handle.writelines(f\"&gt;{i.uid}\\n{i.sequence}\\n\")\n        counter += 1\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.socialgene.SocialGene.write_table","title":"<code>write_table(outdir, tablename, filename=None, include_sequences=False, **kwargs)</code>","text":"<p>The function writes a table to a specified output directory in TSV format, for import into Neo4j.</p> <p>Parameters:</p> Name Type Description Default <code>outdir</code> <code>str</code> <p>The <code>outdir</code> parameter is a string that specifies the directory where the output</p> required <p>file will be saved.   tablename (str): The <code>tablename</code> parameter is a string that specifies the name of the table to be written.   filename (str): The <code>filename</code> parameter is an optional argument that specifies the name of the file to be written. If no <code>filename</code> is provided, the <code>tablename</code> will be used as the filename.</p> Source code in <code>socialgene/base/socialgene.py</code> <pre><code>def write_table(\n    self,\n    outdir: str,\n    tablename: str,\n    filename: str = None,\n    include_sequences=False,\n    **kwargs,\n):\n    \"\"\"\n    The function writes a table to a specified output directory in TSV format, for import into Neo4j.\n\n    Args:\n      outdir (str): The `outdir` parameter is a string that specifies the directory where the output\n    file will be saved.\n      tablename (str): The `tablename` parameter is a string that specifies the name of the table to\n    be written.\n      filename (str): The `filename` parameter is an optional argument that specifies the name of\n    the file to be written. If no `filename` is provided, the `tablename` will be used as the\n    filename.\n    \"\"\"\n    if not filename:\n        filename = tablename\n    outpath = Path(outdir, filename)\n    with open_write(outpath, **kwargs) as handle:\n        tsv_writer = csv.writer(\n            handle, delimiter=\"\\t\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n        )\n        for i in getattr(self, tablename)(include_sequences=include_sequences):\n            tsv_writer.writerow(i)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Assembly","title":"<code>Assembly</code>","text":"<p>Container class holding a dictionary of loci (ie genes/proteins)</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>class Assembly:\n    \"\"\"Container class holding a dictionary of loci (ie genes/proteins)\"\"\"\n\n    __slots__ = [\n        \"parent\",\n        \"loci\",\n        \"taxid\",\n        \"metadata\",\n        \"uid\",\n        \"taxonomy\",\n        \"name\",\n    ]\n\n    def __init__(self, uid, parent=None):\n        super().__init__()\n        self.parent = parent\n        self.uid = uid\n        self.loci = {}\n        self.taxid = None\n        self.taxonomy = Taxonomy()\n        self.metadata = LocusAssemblyMetadata()\n        self.name = uid\n\n    @property\n    def feature_uid_set(self):\n        \"\"\"Return all protein hashes within assembly\n        Returns:\n            set: protein hashes\n        \"\"\"\n        return set().union(*[i.feature_uid_set for i in self.loci.values()])\n\n    def all_attributes(self):\n        return {s: getattr(self, s) for s in sorted(self.__slots__) if hasattr(self, s)}\n\n    def add_locus(self, external_id: str = None):\n        \"\"\"Add a locus to an assembly object\"\"\"\n        if external_id is None:\n            external_id = str(uuid4())\n        if external_id not in self.loci:\n            self.loci[external_id] = Locus(parent=self, external_id=external_id)\n        else:\n            log.debug(f\"{external_id} already present\")\n\n    def fill_taxonomy_from_db(self):\n        try:\n            with GraphDriver() as db:\n                res = db.run(\n                    \"\"\"\n                        MATCH (a1:assembly {uid: $uid})-[:IS_TAXON]-&gt;(t2:taxid)-[:TAXON_PARENT*1..]-&gt;(t1:taxid)\n                        WHERE t1.rank in [\"genus\", \"family\", \"order\", \"class\", \"phylum\", \"clade\", \"superkingdom\"]\n                        WITH  apoc.map.fromLists([t2.rank],[t2.name]) as a, apoc.map.fromLists([t1.rank],[t1.name]) as b\n                        return  apoc.map.mergeList(collect(a)+collect(b)) as tax_dict\n                        \"\"\",\n                    uid=str(self.uid),\n                ).value()\n                # Dict comprehension appends '_' to the keys, to match the args in Taxonomy\n                self.taxonomy = Taxonomy(**{f\"{k}_\": v for k, v in res[0].items()})\n        except Exception:\n            log.debug(f\"Error trying to retrieve taxonomy for {self.uid}\")\n\n    def fill_properties(self):\n        try:\n            with GraphDriver() as db:\n                res = db.run(\n                    \"\"\"\n                        MATCH (a1:assembly {uid: $uid})\n                        return properties(a1)\n                        \"\"\",\n                    uid=self.uid,\n                ).value()[0]\n                self.metadata.update(res)\n        except Exception:\n            log.debug(f\"Error trying to retrieve taxonomy for {self.uid}\")\n\n    def get_locus_by_uid(self, uid):\n        for k, v in self.loci.items():\n            if v.uid == uid:\n                return v\n\n    @property\n    def gene_clusters(self):\n        for locus in self.loci.values():\n            for gene_cluster in locus.gene_clusters:\n                yield gene_cluster\n\n    @property\n    def fasta_string_defline_uid(self):\n        for v in self.loci.values():\n            yield v.fasta_string_defline_uid\n\n    @property\n    def fasta_string_defline_external_id(self):\n        for v in self.loci.values():\n            yield v.fasta_string_defline_external_id\n\n    def _neo4j_node(\n        self,\n    ):\n        n = ASSEMBLY_NODE()\n        dict_to_add = {\"uid\": self.uid}\n        dict_to_add = dict_to_add | {\n            k: v\n            for k, v in self.metadata.all_attributes().items()\n            if k in ASSEMBLY_NODE.property_specification\n        }\n        n.fill_from_dict(dict_to_add)\n        return n\n\n    def add_to_neo4j(self, create=False):\n        for i in self.loci.values():\n            i.add_to_neo4j()\n        if self.taxid:\n            taxnode = TAXID_NODE()\n            taxnode.fill_from_dict({\"uid\": self.taxid})\n            IS_TAXON_REL(start=self._neo4j_node(), end=taxnode).add_to_neo4j(\n                create=False\n            )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Assembly.feature_uid_set","title":"<code>feature_uid_set</code>  <code>property</code>","text":"<p>Return all protein hashes within assembly Returns:     set: protein hashes</p>"},{"location":"developing/python/classes/#socialgene.base.molbio.Assembly.add_locus","title":"<code>add_locus(external_id=None)</code>","text":"<p>Add a locus to an assembly object</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def add_locus(self, external_id: str = None):\n    \"\"\"Add a locus to an assembly object\"\"\"\n    if external_id is None:\n        external_id = str(uuid4())\n    if external_id not in self.loci:\n        self.loci[external_id] = Locus(parent=self, external_id=external_id)\n    else:\n        log.debug(f\"{external_id} already present\")\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Domain","title":"<code>Domain</code>","text":"<p>Class for holding information about a domain/motif annotation</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>class Domain:\n    \"\"\"Class for holding information about a domain/motif annotation\"\"\"\n\n    __slots__ = [\n        \"hmm_id\",\n        \"env_from\",  # page 38 of the HMMER User Guide (http://eddylab.org/software/hmmer/Userguide.pdf) suggests using envelope (not hmm_from/to, ali_from/to)\n        \"env_to\",\n        \"seq_pro_score\",\n        \"evalue\",\n        \"i_evalue\",\n        \"domain_bias\",\n        \"domain_score\",\n        \"seq_pro_bias\",\n        \"hmm_from\",\n        \"hmm_to\",\n        \"ali_from\",\n        \"ali_to\",\n        \"exponentialized\",\n    ]\n\n    def __init__(\n        self,\n        hmm_id: str = None,\n        env_from: int = None,\n        env_to: int = None,\n        seq_pro_score: float = None,\n        evalue: float = None,\n        i_evalue: float = None,\n        domain_bias: float = None,\n        domain_score: float = None,\n        seq_pro_bias: float = None,\n        hmm_from: int = None,\n        hmm_to: int = None,\n        ali_from: int = None,\n        ali_to: int = None,\n        exponentialized: bool = True,\n        **kwargs,  # this kwarg isn't accessed but is here so that calling Domain with dict unpacking with extra args doesn't fail\n    ):\n        \"\"\"\n        Class for holding information about a domain/motif annotation\n\n        Args:\n          hmm_id (str): The `hmm_id` parameter is a string that represents the identifier of the hidden\n        Markov model (HMM) associated with the domain.\n          env_from (int): The `env_from` parameter represents the starting position of the domain in the\n        target sequence.\n          env_to (int): The `env_to` parameter represents the end position of the environment (sequence)\n        in the domain. It is an integer value.\n          seq_pro_score (float): The `seq_pro_score` parameter is a floating-point number that\n        represents the sequence profile score of the domain.\n          evalue (float): The `evalue` parameter is a floating-point number that represents the E-value\n        of the domain. The E-value is a statistical measure that indicates the expected number of\n        domains with a similar score or better that would occur by chance in a database of the same\n        size.\n          i_evalue (float): The `i_evalue` parameter is a floating-point number that represents the\n        independent E-value of a domain. It is used to assess the statistical significance of the match\n        between the domain and the sequence.\n          domain_bias (float): The `domain_bias` parameter is a float that represents the bias score of\n        a domain. It is used to measure the likelihood that a domain is present in a sequence due to\n        chance rather than functional significance.\n          domain_score (float): The `domain_score` parameter is a float that represents the score of the\n        domain.\n          seq_pro_bias (float): The `seq_pro_bias` parameter is a float value that represents the\n        sequence profile bias of a domain. It is used to measure the bias in the sequence profile\n        alignment.\n          hmm_from (int): The `hmm_from` parameter represents the starting position of the domain in the\n        HMM (Hidden Markov Model) profile. It is an integer value.\n          hmm_to (int): The `hmm_to` parameter is an integer that represents the ending position of the\n        hidden Markov model (HMM) alignment in the domain.\n          ali_from (int): The `ali_from` parameter represents the starting position of the alignment in\n        the sequence alignment. It is an integer value.\n          ali_to (int): The `ali_to` parameter represents the ending position of the alignment in the\n        sequence.\n          exponentialized (bool): A boolean flag indicating whether the evalue and i_evalue should be\n        exponentialized or not. If set to True, the evalue and i_evalue will be converted to exponential\n        form. Defaults to True\n        \"\"\"\n        super(Domain, self).__init__()\n        if not isinstance(exponentialized, bool):\n            raise ValueError(\n                f\"exponentialized mus be bool, was {type(exponentialized)}\"\n            )\n        self.exponentialized = exponentialized\n        if exponentialized:\n            self.evalue = find_exp(evalue)\n            self.i_evalue = find_exp(i_evalue)\n        else:\n            self.evalue = round(float(evalue), 1)\n            self.i_evalue = round(float(i_evalue), 1)\n        # some of are rounded because of differences between pyhmmer and hmmer results\n        self.hmm_id = str(hmm_id)\n        self.env_from = int(env_from)\n        self.env_to = int(env_to)\n        self.seq_pro_score = round(float(seq_pro_score), 1)\n        self.domain_bias = round(float(domain_bias), 1)\n        self.domain_score = round(float(domain_score), 1)\n        self.seq_pro_bias = round(float(seq_pro_bias), 1)\n        self.hmm_from = int(hmm_from)\n        self.hmm_to = int(hmm_to)\n        self.ali_from = int(ali_from)\n        self.ali_to = int(ali_to)\n\n    def all_attributes(self):\n        ord_dict = OrderedDict()\n        ord_dict[\"hmm_id\"] = str(self.hmm_id)\n        ord_dict[\"env_from\"] = int(self.env_from)\n        ord_dict[\"env_to\"] = int(self.env_to)\n        ord_dict[\"seq_pro_score\"] = round(self.seq_pro_score, 1)\n        ord_dict[\"evalue\"] = round(self.evalue, 1)\n        ord_dict[\"i_evalue\"] = round(self.i_evalue, 1)\n        ord_dict[\"domain_bias\"] = round(self.domain_bias, 1)\n        ord_dict[\"domain_score\"] = round(self.domain_score, 1)\n        ord_dict[\"seq_pro_bias\"] = round(self.seq_pro_bias, 1)\n        ord_dict[\"hmm_from\"] = int(self.hmm_from)\n        ord_dict[\"hmm_to\"] = int(self.hmm_to)\n        ord_dict[\"ali_from\"] = int(self.ali_from)\n        ord_dict[\"ali_to\"] = int(self.ali_to)\n        ord_dict[\"exponentialized\"] = bool(self.exponentialized)\n        return ord_dict\n\n    def tsv_attributes(self):\n        ord_dict = OrderedDict()\n        ord_dict[\"hmm_id\"] = str(self.hmm_id)\n        ord_dict[\"env_from\"] = int(self.env_from)\n        ord_dict[\"env_to\"] = int(self.env_to)\n        ord_dict[\"seq_pro_score\"] = round(self.seq_pro_score, 1)\n        ord_dict[\"evalue\"] = round(self.evalue, 1)\n        ord_dict[\"i_evalue\"] = round(self.i_evalue, 1)\n        ord_dict[\"domain_bias\"] = round(self.domain_bias, 1)\n        ord_dict[\"domain_score\"] = round(self.domain_score, 1)\n        ord_dict[\"seq_pro_bias\"] = round(self.seq_pro_bias, 1)\n        ord_dict[\"hmm_from\"] = int(self.hmm_from)\n        ord_dict[\"hmm_to\"] = int(self.hmm_to)\n        ord_dict[\"ali_from\"] = int(self.ali_from)\n        ord_dict[\"ali_to\"] = int(self.ali_to)\n        ord_dict[\"exponentialized\"] = bool(self.exponentialized)\n        return ord_dict\n\n    def get_hmm_id(self):\n        return self.hmm_id\n\n    @property\n    def domain_within_threshold(self):\n        \"\"\"Check if a protein's domain annotation is within the threshold for inclusion (currently i_evalue based)\n        Returns:\n            bool: is less than or equal to the set ievalue cutoff?\n        \"\"\"\n        if self.exponentialized:\n            return self.i_evalue &lt;= find_exp(env_vars[\"HMMSEARCH_IEVALUE\"])\n        else:\n            return self.i_evalue &lt;= env_vars[\"HMMSEARCH_IEVALUE\"]\n\n    def __hash__(self):\n        \"\"\"Used to prevent adding duplicate domains\n        Returns:\n            hash: hash for set()\n        \"\"\"\n        return hash((self.hmm_id, self.env_from, self.env_to, self.i_evalue))\n\n    def __eq__(self, other):  # pragma: no cover\n        if not isinstance(other, type(self)):\n            return NotImplemented\n        return (\n            self.hmm_id == other.hmm_id\n            and self.env_from == other.env_from\n            and self.env_to == other.env_to\n            and self.i_evalue == other.i_evalue\n        )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Domain.domain_within_threshold","title":"<code>domain_within_threshold</code>  <code>property</code>","text":"<p>Check if a protein's domain annotation is within the threshold for inclusion (currently i_evalue based) Returns:     bool: is less than or equal to the set ievalue cutoff?</p>"},{"location":"developing/python/classes/#socialgene.base.molbio.Domain.__hash__","title":"<code>__hash__()</code>","text":"<p>Used to prevent adding duplicate domains Returns:     hash: hash for set()</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def __hash__(self):\n    \"\"\"Used to prevent adding duplicate domains\n    Returns:\n        hash: hash for set()\n    \"\"\"\n    return hash((self.hmm_id, self.env_from, self.env_to, self.i_evalue))\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Domain.__init__","title":"<code>__init__(hmm_id=None, env_from=None, env_to=None, seq_pro_score=None, evalue=None, i_evalue=None, domain_bias=None, domain_score=None, seq_pro_bias=None, hmm_from=None, hmm_to=None, ali_from=None, ali_to=None, exponentialized=True, **kwargs)</code>","text":"<p>Class for holding information about a domain/motif annotation</p> <p>Parameters:</p> Name Type Description Default <code>hmm_id</code> <code>str</code> <p>The <code>hmm_id</code> parameter is a string that represents the identifier of the hidden</p> <code>None</code> <p>Markov model (HMM) associated with the domain.   env_from (int): The <code>env_from</code> parameter represents the starting position of the domain in the target sequence.   env_to (int): The <code>env_to</code> parameter represents the end position of the environment (sequence) in the domain. It is an integer value.   seq_pro_score (float): The <code>seq_pro_score</code> parameter is a floating-point number that represents the sequence profile score of the domain.   evalue (float): The <code>evalue</code> parameter is a floating-point number that represents the E-value of the domain. The E-value is a statistical measure that indicates the expected number of domains with a similar score or better that would occur by chance in a database of the same size.   i_evalue (float): The <code>i_evalue</code> parameter is a floating-point number that represents the independent E-value of a domain. It is used to assess the statistical significance of the match between the domain and the sequence.   domain_bias (float): The <code>domain_bias</code> parameter is a float that represents the bias score of a domain. It is used to measure the likelihood that a domain is present in a sequence due to chance rather than functional significance.   domain_score (float): The <code>domain_score</code> parameter is a float that represents the score of the domain.   seq_pro_bias (float): The <code>seq_pro_bias</code> parameter is a float value that represents the sequence profile bias of a domain. It is used to measure the bias in the sequence profile alignment.   hmm_from (int): The <code>hmm_from</code> parameter represents the starting position of the domain in the HMM (Hidden Markov Model) profile. It is an integer value.   hmm_to (int): The <code>hmm_to</code> parameter is an integer that represents the ending position of the hidden Markov model (HMM) alignment in the domain.   ali_from (int): The <code>ali_from</code> parameter represents the starting position of the alignment in the sequence alignment. It is an integer value.   ali_to (int): The <code>ali_to</code> parameter represents the ending position of the alignment in the sequence.   exponentialized (bool): A boolean flag indicating whether the evalue and i_evalue should be exponentialized or not. If set to True, the evalue and i_evalue will be converted to exponential form. Defaults to True</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def __init__(\n    self,\n    hmm_id: str = None,\n    env_from: int = None,\n    env_to: int = None,\n    seq_pro_score: float = None,\n    evalue: float = None,\n    i_evalue: float = None,\n    domain_bias: float = None,\n    domain_score: float = None,\n    seq_pro_bias: float = None,\n    hmm_from: int = None,\n    hmm_to: int = None,\n    ali_from: int = None,\n    ali_to: int = None,\n    exponentialized: bool = True,\n    **kwargs,  # this kwarg isn't accessed but is here so that calling Domain with dict unpacking with extra args doesn't fail\n):\n    \"\"\"\n    Class for holding information about a domain/motif annotation\n\n    Args:\n      hmm_id (str): The `hmm_id` parameter is a string that represents the identifier of the hidden\n    Markov model (HMM) associated with the domain.\n      env_from (int): The `env_from` parameter represents the starting position of the domain in the\n    target sequence.\n      env_to (int): The `env_to` parameter represents the end position of the environment (sequence)\n    in the domain. It is an integer value.\n      seq_pro_score (float): The `seq_pro_score` parameter is a floating-point number that\n    represents the sequence profile score of the domain.\n      evalue (float): The `evalue` parameter is a floating-point number that represents the E-value\n    of the domain. The E-value is a statistical measure that indicates the expected number of\n    domains with a similar score or better that would occur by chance in a database of the same\n    size.\n      i_evalue (float): The `i_evalue` parameter is a floating-point number that represents the\n    independent E-value of a domain. It is used to assess the statistical significance of the match\n    between the domain and the sequence.\n      domain_bias (float): The `domain_bias` parameter is a float that represents the bias score of\n    a domain. It is used to measure the likelihood that a domain is present in a sequence due to\n    chance rather than functional significance.\n      domain_score (float): The `domain_score` parameter is a float that represents the score of the\n    domain.\n      seq_pro_bias (float): The `seq_pro_bias` parameter is a float value that represents the\n    sequence profile bias of a domain. It is used to measure the bias in the sequence profile\n    alignment.\n      hmm_from (int): The `hmm_from` parameter represents the starting position of the domain in the\n    HMM (Hidden Markov Model) profile. It is an integer value.\n      hmm_to (int): The `hmm_to` parameter is an integer that represents the ending position of the\n    hidden Markov model (HMM) alignment in the domain.\n      ali_from (int): The `ali_from` parameter represents the starting position of the alignment in\n    the sequence alignment. It is an integer value.\n      ali_to (int): The `ali_to` parameter represents the ending position of the alignment in the\n    sequence.\n      exponentialized (bool): A boolean flag indicating whether the evalue and i_evalue should be\n    exponentialized or not. If set to True, the evalue and i_evalue will be converted to exponential\n    form. Defaults to True\n    \"\"\"\n    super(Domain, self).__init__()\n    if not isinstance(exponentialized, bool):\n        raise ValueError(\n            f\"exponentialized mus be bool, was {type(exponentialized)}\"\n        )\n    self.exponentialized = exponentialized\n    if exponentialized:\n        self.evalue = find_exp(evalue)\n        self.i_evalue = find_exp(i_evalue)\n    else:\n        self.evalue = round(float(evalue), 1)\n        self.i_evalue = round(float(i_evalue), 1)\n    # some of are rounded because of differences between pyhmmer and hmmer results\n    self.hmm_id = str(hmm_id)\n    self.env_from = int(env_from)\n    self.env_to = int(env_to)\n    self.seq_pro_score = round(float(seq_pro_score), 1)\n    self.domain_bias = round(float(domain_bias), 1)\n    self.domain_score = round(float(domain_score), 1)\n    self.seq_pro_bias = round(float(seq_pro_bias), 1)\n    self.hmm_from = int(hmm_from)\n    self.hmm_to = int(hmm_to)\n    self.ali_from = int(ali_from)\n    self.ali_to = int(ali_to)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Feature","title":"<code>Feature</code>","text":"<p>               Bases: <code>Location</code></p> <p>Container class for describing a feature on a locus</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>class Feature(Location):\n    \"\"\"Container class for describing a feature on a locus\"\"\"\n\n    __slots__ = [\n        \"parent\",\n        \"uid\",\n        \"external_id\",\n        \"type\",\n        \"locus_tag\",\n        \"description\",\n        \"note\",\n        \"goterms\",\n        \"partial_on_complete_genome\",\n        \"missing_start\",\n        \"missing_stop\",\n        \"internal_stop\",\n        \"partial_in_the_middle_of_a_contig\",\n        \"missing_N_terminus\",\n        \"missing_C_terminus\",\n        \"frameshifted\",\n        \"too_short_partial_abutting_assembly_gap\",\n        \"incomplete\",\n        \"protein\",\n    ]\n\n    def __init__(\n        self,\n        parent=None,\n        uid: str = None,\n        external_id: str = None,\n        type: str = None,\n        locus_tag: str = None,\n        description=None,\n        note=None,\n        goterms=None,\n        partial_on_complete_genome=None,\n        missing_start=None,\n        missing_stop=None,\n        internal_stop=None,\n        partial_in_the_middle_of_a_contig=None,\n        missing_N_terminus=None,\n        missing_C_terminus=None,\n        frameshifted=None,\n        too_short_partial_abutting_assembly_gap=None,\n        incomplete=None,\n        **kwargs,\n    ):\n        \"\"\"\n        The above function is a constructor for a class that represents a feature on a locus, with\n        various attributes and optional arguments.\n\n        Args:\n          uid (str): A string representing the hash value of the protein.\n          external_id (str): The unique identifier for the protein associated with the feature.\n          type (str): The \"type\" parameter is used to specify the type of feature. In this case, it is\n        used to specify the type of the feature on a locus, such as \"protein\".\n          locus_tag (str): The locus_tag parameter is a string that represents the unique identifier for\n        a feature on a locus. It is typically used in genomics to identify a specific gene or protein\n        within a genome.\n          description: A description of the feature on a locus.\n          note: The `note` parameter is used to provide additional information or comments about the\n        feature. It is an optional parameter and can be any string value.\n          goterms: The `goterms` parameter is used to store the Gene Ontology terms associated with the\n        feature. Gene Ontology (GO) terms are standardized terms used to describe the function,\n        location, and involvement of genes and gene products in biological processes.\n          partial_on_complete_genome: A boolean flag indicating whether the feature is partial on a\n        complete genome.\n          missing_start: A boolean indicating whether the start of the feature is missing.\n          missing_stop: A boolean flag indicating whether the stop codon of the feature is missing.\n          internal_stop: A boolean flag indicating whether the feature has an internal stop codon.\n          partial_in_the_middle_of_a_contig: This parameter is used to indicate whether the feature is\n        partial and located in the middle of a contig.\n          missing_N_terminus: This parameter is used to indicate whether the N-terminus (the starting\n        end) of the feature is missing. It is a boolean value, where True indicates that the N-terminus\n        is missing and False indicates that it is not missing.\n          missing_C_terminus: The parameter \"missing_C_terminus\" is used to indicate whether the feature\n        is missing the C-terminus (the end) of the protein sequence. It is a boolean value that can be\n        set to True or False.\n          frameshifted: The \"frameshifted\" parameter is a boolean flag that indicates whether the\n        feature has a frameshift mutation. A frameshift mutation occurs when the reading frame of a gene\n        is disrupted by the insertion or deletion of nucleotides, causing a shift in the codon reading\n        frame and potentially altering the amino\n          too_short_partial_abutting_assembly_gap: This parameter is used to indicate whether the\n        feature is too short and abuts an assembly gap.\n          incomplete: A boolean flag indicating whether the feature is incomplete.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.parent = parent\n        self.uid = uid\n        self.external_id = external_id\n        self.type = type\n        self.locus_tag = locus_tag\n        self.description = description\n        self.note = note\n        self.goterms = goterms\n        self.partial_on_complete_genome = partial_on_complete_genome\n        self.missing_start = missing_start\n        self.missing_stop = missing_stop\n        self.internal_stop = internal_stop\n        self.partial_in_the_middle_of_a_contig = partial_in_the_middle_of_a_contig\n        self.missing_N_terminus = missing_N_terminus\n        self.missing_C_terminus = missing_C_terminus\n        self.frameshifted = frameshifted\n        self.too_short_partial_abutting_assembly_gap = (\n            too_short_partial_abutting_assembly_gap\n        )\n        self.incomplete = incomplete\n        if self.feature_is_protein():\n            # Check if there is a SocialGene object in the parent chain\n            # If there is, link the feature protein attribute to the SocialGene protein object\n            sg = None\n            current_object = self\n            for _ in range(1, 100):\n                if (\n                    f\"{current_object.__class__.__module__}.{current_object.__class__.__name__}\"\n                    == \"socialgene.base.socialgene.SocialGene\"\n                ):\n                    sg = current_object\n                    break\n                else:\n                    if hasattr(current_object, \"parent\"):\n                        current_object = current_object.parent\n            if sg:\n                if self.uid in sg.proteins:\n                    self.protein = sg.proteins[self.uid]\n\n    def all_attributes(self):\n        return {s: getattr(self, s) for s in sorted(self.__slots__) if hasattr(self, s)}\n\n    def feature_is_protein(self):\n        \"\"\"Check if the feature \"is a protein\"\n\n        Returns:\n            bool: True if socialgene thinks it's a protein, False if not\n        \"\"\"\n        # types of genbank file features to classify as a \"protein\"\n        types_list = [\"protein\", \"CDS\"]\n        return any([True for i in types_list if i == self.type])\n\n    @property\n    def fasta_string_defline_uid(self):\n        try:\n            if self.protein:\n                return f\"&gt;{self.protein.uid}\\n{self.protein.sequence}\\n\"\n        except AttributeError:\n            log.debug(f\"Erorr in fasta_string_defline_uid for {self}\")\n\n    @property\n    def fasta_string_defline_external_id(self):\n        try:\n            if self.protein:\n                return f\"&gt;{self.protein.external_id}\\n{self.protein.sequence}\\n\"\n        except AttributeError:\n            log.debug(f\"Erorr in fasta_string_defline_external_id for {self}\")\n\n    def __hash__(self):\n        \"\"\"Used to prevent adding duplicate features to a locus (for hash in set() in Assembly.add_locus())\n\n        Returns:\n            hash: hash for set()\n        \"\"\"\n        return hash((self.end, self.start, self.uid, self.strand, self.type))\n\n    def __eq__(self, other):\n        \"\"\"Used for set() in Assembly.add_locus()\"\"\"\n        if not isinstance(other, type(self)):\n            return NotImplemented\n        return (\n            self.parent == other.parent\n            and self.end == other.end\n            and self.start == other.start\n            and self.uid == other.uid\n            and self.strand == other.strand\n            and self.type == other.type\n        )\n\n    def __lt__(self, other):\n        return self.start &lt; other.start and self.parent == other.parent\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Feature.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Used for set() in Assembly.add_locus()</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"Used for set() in Assembly.add_locus()\"\"\"\n    if not isinstance(other, type(self)):\n        return NotImplemented\n    return (\n        self.parent == other.parent\n        and self.end == other.end\n        and self.start == other.start\n        and self.uid == other.uid\n        and self.strand == other.strand\n        and self.type == other.type\n    )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Feature.__hash__","title":"<code>__hash__()</code>","text":"<p>Used to prevent adding duplicate features to a locus (for hash in set() in Assembly.add_locus())</p> <p>Returns:</p> Name Type Description <code>hash</code> <p>hash for set()</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def __hash__(self):\n    \"\"\"Used to prevent adding duplicate features to a locus (for hash in set() in Assembly.add_locus())\n\n    Returns:\n        hash: hash for set()\n    \"\"\"\n    return hash((self.end, self.start, self.uid, self.strand, self.type))\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Feature.__init__","title":"<code>__init__(parent=None, uid=None, external_id=None, type=None, locus_tag=None, description=None, note=None, goterms=None, partial_on_complete_genome=None, missing_start=None, missing_stop=None, internal_stop=None, partial_in_the_middle_of_a_contig=None, missing_N_terminus=None, missing_C_terminus=None, frameshifted=None, too_short_partial_abutting_assembly_gap=None, incomplete=None, **kwargs)</code>","text":"<p>The above function is a constructor for a class that represents a feature on a locus, with various attributes and optional arguments.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>A string representing the hash value of the protein.</p> <code>None</code> <code>external_id</code> <code>str</code> <p>The unique identifier for the protein associated with the feature.</p> <code>None</code> <code>type</code> <code>str</code> <p>The \"type\" parameter is used to specify the type of feature. In this case, it is</p> <code>None</code> <p>used to specify the type of the feature on a locus, such as \"protein\".   locus_tag (str): The locus_tag parameter is a string that represents the unique identifier for a feature on a locus. It is typically used in genomics to identify a specific gene or protein within a genome.   description: A description of the feature on a locus.   note: The <code>note</code> parameter is used to provide additional information or comments about the feature. It is an optional parameter and can be any string value.   goterms: The <code>goterms</code> parameter is used to store the Gene Ontology terms associated with the feature. Gene Ontology (GO) terms are standardized terms used to describe the function, location, and involvement of genes and gene products in biological processes.   partial_on_complete_genome: A boolean flag indicating whether the feature is partial on a complete genome.   missing_start: A boolean indicating whether the start of the feature is missing.   missing_stop: A boolean flag indicating whether the stop codon of the feature is missing.   internal_stop: A boolean flag indicating whether the feature has an internal stop codon.   partial_in_the_middle_of_a_contig: This parameter is used to indicate whether the feature is partial and located in the middle of a contig.   missing_N_terminus: This parameter is used to indicate whether the N-terminus (the starting end) of the feature is missing. It is a boolean value, where True indicates that the N-terminus is missing and False indicates that it is not missing.   missing_C_terminus: The parameter \"missing_C_terminus\" is used to indicate whether the feature is missing the C-terminus (the end) of the protein sequence. It is a boolean value that can be set to True or False.   frameshifted: The \"frameshifted\" parameter is a boolean flag that indicates whether the feature has a frameshift mutation. A frameshift mutation occurs when the reading frame of a gene is disrupted by the insertion or deletion of nucleotides, causing a shift in the codon reading frame and potentially altering the amino   too_short_partial_abutting_assembly_gap: This parameter is used to indicate whether the feature is too short and abuts an assembly gap.   incomplete: A boolean flag indicating whether the feature is incomplete.</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def __init__(\n    self,\n    parent=None,\n    uid: str = None,\n    external_id: str = None,\n    type: str = None,\n    locus_tag: str = None,\n    description=None,\n    note=None,\n    goterms=None,\n    partial_on_complete_genome=None,\n    missing_start=None,\n    missing_stop=None,\n    internal_stop=None,\n    partial_in_the_middle_of_a_contig=None,\n    missing_N_terminus=None,\n    missing_C_terminus=None,\n    frameshifted=None,\n    too_short_partial_abutting_assembly_gap=None,\n    incomplete=None,\n    **kwargs,\n):\n    \"\"\"\n    The above function is a constructor for a class that represents a feature on a locus, with\n    various attributes and optional arguments.\n\n    Args:\n      uid (str): A string representing the hash value of the protein.\n      external_id (str): The unique identifier for the protein associated with the feature.\n      type (str): The \"type\" parameter is used to specify the type of feature. In this case, it is\n    used to specify the type of the feature on a locus, such as \"protein\".\n      locus_tag (str): The locus_tag parameter is a string that represents the unique identifier for\n    a feature on a locus. It is typically used in genomics to identify a specific gene or protein\n    within a genome.\n      description: A description of the feature on a locus.\n      note: The `note` parameter is used to provide additional information or comments about the\n    feature. It is an optional parameter and can be any string value.\n      goterms: The `goterms` parameter is used to store the Gene Ontology terms associated with the\n    feature. Gene Ontology (GO) terms are standardized terms used to describe the function,\n    location, and involvement of genes and gene products in biological processes.\n      partial_on_complete_genome: A boolean flag indicating whether the feature is partial on a\n    complete genome.\n      missing_start: A boolean indicating whether the start of the feature is missing.\n      missing_stop: A boolean flag indicating whether the stop codon of the feature is missing.\n      internal_stop: A boolean flag indicating whether the feature has an internal stop codon.\n      partial_in_the_middle_of_a_contig: This parameter is used to indicate whether the feature is\n    partial and located in the middle of a contig.\n      missing_N_terminus: This parameter is used to indicate whether the N-terminus (the starting\n    end) of the feature is missing. It is a boolean value, where True indicates that the N-terminus\n    is missing and False indicates that it is not missing.\n      missing_C_terminus: The parameter \"missing_C_terminus\" is used to indicate whether the feature\n    is missing the C-terminus (the end) of the protein sequence. It is a boolean value that can be\n    set to True or False.\n      frameshifted: The \"frameshifted\" parameter is a boolean flag that indicates whether the\n    feature has a frameshift mutation. A frameshift mutation occurs when the reading frame of a gene\n    is disrupted by the insertion or deletion of nucleotides, causing a shift in the codon reading\n    frame and potentially altering the amino\n      too_short_partial_abutting_assembly_gap: This parameter is used to indicate whether the\n    feature is too short and abuts an assembly gap.\n      incomplete: A boolean flag indicating whether the feature is incomplete.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.parent = parent\n    self.uid = uid\n    self.external_id = external_id\n    self.type = type\n    self.locus_tag = locus_tag\n    self.description = description\n    self.note = note\n    self.goterms = goterms\n    self.partial_on_complete_genome = partial_on_complete_genome\n    self.missing_start = missing_start\n    self.missing_stop = missing_stop\n    self.internal_stop = internal_stop\n    self.partial_in_the_middle_of_a_contig = partial_in_the_middle_of_a_contig\n    self.missing_N_terminus = missing_N_terminus\n    self.missing_C_terminus = missing_C_terminus\n    self.frameshifted = frameshifted\n    self.too_short_partial_abutting_assembly_gap = (\n        too_short_partial_abutting_assembly_gap\n    )\n    self.incomplete = incomplete\n    if self.feature_is_protein():\n        # Check if there is a SocialGene object in the parent chain\n        # If there is, link the feature protein attribute to the SocialGene protein object\n        sg = None\n        current_object = self\n        for _ in range(1, 100):\n            if (\n                f\"{current_object.__class__.__module__}.{current_object.__class__.__name__}\"\n                == \"socialgene.base.socialgene.SocialGene\"\n            ):\n                sg = current_object\n                break\n            else:\n                if hasattr(current_object, \"parent\"):\n                    current_object = current_object.parent\n        if sg:\n            if self.uid in sg.proteins:\n                self.protein = sg.proteins[self.uid]\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Feature.feature_is_protein","title":"<code>feature_is_protein()</code>","text":"<p>Check if the feature \"is a protein\"</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if socialgene thinks it's a protein, False if not</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def feature_is_protein(self):\n    \"\"\"Check if the feature \"is a protein\"\n\n    Returns:\n        bool: True if socialgene thinks it's a protein, False if not\n    \"\"\"\n    # types of genbank file features to classify as a \"protein\"\n    types_list = [\"protein\", \"CDS\"]\n    return any([True for i in types_list if i == self.type])\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.FeatureCollection","title":"<code>FeatureCollection</code>","text":"Source code in <code>socialgene/base/molbio.py</code> <pre><code>class FeatureCollection:\n    __slots__ = [\n        \"parent\",\n        \"features\",\n    ]\n\n    def add_feature(self, **kwargs):\n        \"\"\"Add a feature to a locus\"\"\"\n        self.features.add(Feature(parent=self, **kwargs))\n\n    def all_attributes(self):\n        return {s: getattr(self, s) for s in sorted(self.__slots__) if hasattr(self, s)}\n\n    @property\n    def feature_uid_set(self):\n        return {i.uid for i in self.features}\n\n    @property\n    def features_sorted_by_midpoint(self):\n        \"\"\"Sorts features by mid-coordinate of each feature\"\"\"\n        for i in sorted(list(self.features), key=lambda i: int((i.end + i.start) / 2)):\n            yield i\n\n    def drop_non_protein_features(self):\n        self.features = {i for i in self.features if i.type == \"protein\"}\n\n    def get_feature_by_uid(self, uid):\n        for k, v in self.features.items():\n            if v.uid == uid:\n                return v\n\n    @property\n    def proteins(self):\n        sg = None\n        current_object = self\n        for i in range(1, 100):\n            if (\n                str(type(current_object))\n                == \"&lt;class 'socialgene.base.socialgene.SocialGene'&gt;\"\n            ):\n                sg = current_object\n                break\n            else:\n                current_object = current_object.parent\n        return {\n            i: sg.proteins.get(i, None)\n            for i in {i.uid for i in self.features}\n            if i in sg.proteins\n        }\n\n    @property\n    def protein_iter(self):\n        for i in self.proteins.values():\n            yield i\n\n    @property\n    def fasta_string_defline_uid(self):\n        for v in self.proteins.values():\n            yield f\"&gt;{v.uid}\\n{v.sequence}\\n\"\n\n    @property\n    def fasta_string_defline_external_id(self):\n        for v in self.proteins.values():\n            yield f\"&gt;{v.external_id}\\n{v.sequence}\\n\"\n\n    def _neo4j_node(self, **kwargs):\n        n = NUCLEOTIDE_NODE()\n        dict_to_add = {\"uid\": self.uid, \"external_id\": self.external_id}\n        dict_to_add = dict_to_add | dict(self.metadata.all_attributes())\n        n.fill_from_dict(dict_to_add)\n        return n\n\n    def add_to_neo4j(self):\n        # create node\n        n = self._neo4j_node()\n        n.add_to_neo4j(create=False)\n        protein_nodes = set()\n        encodes_rels = set()\n        # connect to proteins\n        for i in self.features:\n            protein_nodes.add(i.protein._neo4j_node())\n            temp_rel = ENCODES_REL(start=n, end=i.protein._neo4j_node())\n            properties = {\n                k: v\n                for k, v in i.all_attributes().items()\n                if k in ENCODES_REL.property_specification\n            }\n            properties = properties | {\n                \"start\": i.start,\n                \"end\": i.end,\n                \"strand\": i.strand,\n            }\n            temp_rel.fill_from_dict(properties)\n            encodes_rels.add(temp_rel)\n        if protein_nodes:\n            protein_nodes = list(protein_nodes)\n            protein_nodes[0].add_multiple_to_neo4j(protein_nodes, create=False)\n        if encodes_rels:\n            encodes_rels = list(encodes_rels)\n            encodes_rels[0].add_multiple_to_neo4j(encodes_rels, create=False)\n        # connect to assembly\n        assembly_node = self.parent._neo4j_node()\n        assembly_node.add_to_neo4j(create=False)\n        ASSEMBLES_TO_REL(start=n, end=assembly_node).add_to_neo4j(create=False)\n\n    def write_genbank(self, outpath, start=float(\"-inf\"), end=float(\"inf\")):\n        record = SeqRecord(\n            Seq(\"\"),\n            id=self.external_id,\n            name=self.external_id,\n            description=\"A GenBank file generated by SocialGene.\",\n            dbxrefs=[f\"Assembly:{self.parent.uid}\"],\n        )\n        # Add annotation\n        for feature in self.features_sorted_by_midpoint:\n            if start:\n                if int(feature.start) &lt; int(start):\n                    continue\n            if end:\n                if int(feature.end) &gt; int(end):\n                    continue\n            biofeat = SeqFeature(\n                FeatureLocation(\n                    start=feature.start\n                    - 1,  # biopython modifies the start position by 1\n                    end=feature.end,\n                    strand=feature.strand,\n                ),\n                type=feature.type,\n                qualifiers={\n                    k: v\n                    for k, v in feature.all_attributes().items()\n                    if v and k != \"parent\"\n                }\n                | {\"translation\": self.protein.sequence},\n            )\n            record.features.append(biofeat)\n        record.annotations[\"molecule_type\"] = \"DNA\"\n        SeqIO.write(\n            record,\n            outpath,\n            \"genbank\",\n        )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.FeatureCollection.features_sorted_by_midpoint","title":"<code>features_sorted_by_midpoint</code>  <code>property</code>","text":"<p>Sorts features by mid-coordinate of each feature</p>"},{"location":"developing/python/classes/#socialgene.base.molbio.FeatureCollection.add_feature","title":"<code>add_feature(**kwargs)</code>","text":"<p>Add a feature to a locus</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def add_feature(self, **kwargs):\n    \"\"\"Add a feature to a locus\"\"\"\n    self.features.add(Feature(parent=self, **kwargs))\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Location","title":"<code>Location</code>","text":"Source code in <code>socialgene/base/molbio.py</code> <pre><code>class Location:\n    __slots__ = [\"start\", \"end\", \"strand\"]\n    # TODO: handle zero indexing\n\n    def __init__(\n        self,\n        start: int = None,\n        end: int = None,\n        strand: int = None,\n        **kwargs,  # this kwarg isn't accessed but is here so that calling Location with dict unpacking with extra args doesn't fail\n    ):\n        \"\"\"Class describing genomic coordinates and strand direction.\n\n        Args:\n            start (int, optional): start coordinate\n            end (int, optional): end coordinate\n            strand (int, optional): DNA strand\n        \"\"\"\n        self.start = start\n        self.end = end\n        self.strand = strand\n\n    def all_attributes(self):\n        return {s: getattr(self, s) for s in sorted(self.__slots__) if hasattr(self, s)}\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Location.__init__","title":"<code>__init__(start=None, end=None, strand=None, **kwargs)</code>","text":"<p>Class describing genomic coordinates and strand direction.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start coordinate</p> <code>None</code> <code>end</code> <code>int</code> <p>end coordinate</p> <code>None</code> <code>strand</code> <code>int</code> <p>DNA strand</p> <code>None</code> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def __init__(\n    self,\n    start: int = None,\n    end: int = None,\n    strand: int = None,\n    **kwargs,  # this kwarg isn't accessed but is here so that calling Location with dict unpacking with extra args doesn't fail\n):\n    \"\"\"Class describing genomic coordinates and strand direction.\n\n    Args:\n        start (int, optional): start coordinate\n        end (int, optional): end coordinate\n        strand (int, optional): DNA strand\n    \"\"\"\n    self.start = start\n    self.end = end\n    self.strand = strand\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Locus","title":"<code>Locus</code>","text":"<p>               Bases: <code>FeatureCollection</code></p> <p>Container holding a set() of genomic features</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>class Locus(FeatureCollection):\n    \"\"\"Container holding a set() of genomic features\"\"\"\n\n    __slots__ = [\n        \"parent\",\n        \"features\",\n        \"metadata\",\n        \"external_id\",\n        \"uid\",\n        \"gene_clusters\",\n    ]\n\n    def __init__(self, parent=None, external_id=None):\n        super().__init__()\n        self.parent = parent\n        self.features = set()\n        self.metadata = LocusAssemblyMetadata()\n        self.external_id = external_id\n        self.uid = self.calc_uid()\n        self.gene_clusters = list()\n\n    def calc_uid(self):\n        return hasher.hasher(f\"{self.parent.uid}___{self.external_id}\")\n\n    def add_bgcs_by_feature(self, features, **kwargs):\n        if not all([isinstance(i, Feature) for i in features]):\n            raise ValueError(\n                f\"All features must be of type Feature, not {[type(i) for i in features if not isinstance(i, Feature)]}\"\n            )\n        self.gene_clusters.append(GeneCluster(features, parent=self, **kwargs))\n\n    def add_bgcs_by_start_end(self, start, end, **kwargs):\n        features = {i for i in self.features if i.start &gt;= start and i.end &lt;= end}\n        if features:\n            self.add_bgcs_by_feature(features=features, **kwargs)\n\n    def write_genbank(self, outpath, start=None, end=None):\n        record = SeqRecord(\n            Seq(\"\"),\n            id=self.external_id,\n            name=self.external_id,\n            description=\"A GenBank file generated by SocialGene.\",\n            dbxrefs=[f\"Assembly:{self.parent.uid}\"],\n        )\n        # Add annotation\n        for feature in self.features_sorted_by_midpoint:\n            if start:\n                if int(feature.start) &lt; int(start):\n                    continue\n            if end:\n                if int(feature.end) &gt; int(end):\n                    continue\n            if feature.type == \"protein\":\n                feat_type = \"CDS\"\n            else:\n                feat_type = feature.type\n            biofeat = SeqFeature(\n                FeatureLocation(\n                    start=feature.start - 1,\n                    end=feature.end,\n                    strand=feature.strand,\n                ),\n                type=feat_type,\n                qualifiers={\n                    k: v\n                    for k, v in feature.all_attributes().items()\n                    if v and k != \"parent\"\n                }\n                | {\"translation\": self.parent.parent.proteins[feature.uid].sequence},\n            )\n            record.features.append(biofeat)\n        record.annotations[\"molecule_type\"] = \"DNA\"\n        SeqIO.write(\n            record,\n            outpath,\n            \"genbank\",\n        )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Molbio","title":"<code>Molbio</code>","text":"<p>Class for inheriting by SocialGene()</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>class Molbio:\n    \"\"\"Class for inheriting by SocialGene()\"\"\"\n\n    def __init__(self):\n        super().__init__()  # TODO: ?\n\n        self.assemblies = {}  # TODO: ?\n        self.proteins = {}  # TODO: ?\n\n    def get_all_feature_uids(self):\n        \"\"\"Return a list of all proteins uids\n\n        Returns:\n            list: List of all proteins uids\n        \"\"\"\n        return [i.uid for i in self.proteins.values()]\n\n    def add_protein(\n        self,\n        return_uid=True,\n        **kwargs,\n    ):\n        \"\"\"Add a protein to the protein dictionary\n\n        Args:\n            no_return (bool, optional): Whether the function should return the protein's uid. Defaults to False.\n\n        Returns:\n            str: Protein's hash\n        \"\"\"\n        temp_protein = Protein(**kwargs)\n        # only add protein if it doesn't already exist\n        if temp_protein.uid not in self.proteins:\n            # deepcopy teo ensure instances aren't shared\n            self.proteins[temp_protein.uid] = temp_protein\n        if return_uid:\n            return temp_protein.uid\n\n    def add_assembly(self, uid: str = None, parent=None):\n        \"\"\"Add an assembly to a SocialGene object\n\n        Args:\n            uid (str, optional): Assembly identifier, should be unique across parsed input. Defaults to None.\n        \"\"\"\n        if uid is None:\n            uid = str(uuid4())\n        if uid not in self.assemblies:\n            self.assemblies[uid] = Assembly(uid=uid, parent=parent)\n        else:\n            log.debug(f\"{uid} already present\")\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Molbio.add_assembly","title":"<code>add_assembly(uid=None, parent=None)</code>","text":"<p>Add an assembly to a SocialGene object</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Assembly identifier, should be unique across parsed input. Defaults to None.</p> <code>None</code> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def add_assembly(self, uid: str = None, parent=None):\n    \"\"\"Add an assembly to a SocialGene object\n\n    Args:\n        uid (str, optional): Assembly identifier, should be unique across parsed input. Defaults to None.\n    \"\"\"\n    if uid is None:\n        uid = str(uuid4())\n    if uid not in self.assemblies:\n        self.assemblies[uid] = Assembly(uid=uid, parent=parent)\n    else:\n        log.debug(f\"{uid} already present\")\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Molbio.add_protein","title":"<code>add_protein(return_uid=True, **kwargs)</code>","text":"<p>Add a protein to the protein dictionary</p> <p>Parameters:</p> Name Type Description Default <code>no_return</code> <code>bool</code> <p>Whether the function should return the protein's uid. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Protein's hash</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def add_protein(\n    self,\n    return_uid=True,\n    **kwargs,\n):\n    \"\"\"Add a protein to the protein dictionary\n\n    Args:\n        no_return (bool, optional): Whether the function should return the protein's uid. Defaults to False.\n\n    Returns:\n        str: Protein's hash\n    \"\"\"\n    temp_protein = Protein(**kwargs)\n    # only add protein if it doesn't already exist\n    if temp_protein.uid not in self.proteins:\n        # deepcopy teo ensure instances aren't shared\n        self.proteins[temp_protein.uid] = temp_protein\n    if return_uid:\n        return temp_protein.uid\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Molbio.get_all_feature_uids","title":"<code>get_all_feature_uids()</code>","text":"<p>Return a list of all proteins uids</p> <p>Returns:</p> Name Type Description <code>list</code> <p>List of all proteins uids</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def get_all_feature_uids(self):\n    \"\"\"Return a list of all proteins uids\n\n    Returns:\n        list: List of all proteins uids\n    \"\"\"\n    return [i.uid for i in self.proteins.values()]\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Protein","title":"<code>Protein</code>","text":"<p>               Bases: <code>ProteinSequence</code></p> <p>Container class for describing a single protein</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>class Protein(\n    ProteinSequence,\n):\n    \"\"\"Container class for describing a single protein\"\"\"\n\n    # seqlen is included as a variable so it can be provided (e.g. a sequence isn't provided and proteins are created manually)\n    __slots__ = [\"description\", \"external_id\", \"domains\"]\n\n    def __init__(\n        self,\n        description: str = None,\n        external_id: str = None,\n        domains: set = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"_summary_\n\n        Args:\n            description (str, optional): Protein description.\n            external_id (str, optional): Non-hash-id descriptor (usually a database accession, e.g. NCBI's).\n            seqlen (int, optional): Amino acid sequence length\n            domains (Set, optional): Set of Domain() objects.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.description = description\n        self.external_id = external_id\n        self.domains = domains if domains is not None else set()\n\n    def __eq__(self, other):  # pragma: no cover\n        if not isinstance(other, type(self)):\n            return NotImplemented\n        return self.uid == other.uid\n\n    def __hash__(self):\n        return hash(self.uid)\n\n    def all_attributes(self):\n        return {\n            s: getattr(self, s) for s in sorted(self.__slots__) if hasattr(self, s)\n        } | {\"seq_len\": self.seq_len}\n\n    def add_domain(\n        self,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Add a domain to a protein if it meets threshold (ievalue)\n\n        Returns:\n            bool: used for counting # of domains added to protein\n        \"\"\"\n        self.domains.add(Domain(*args, **kwargs))\n\n    def add_domains_from_neo4j(self):\n        try:\n            with GraphDriver() as db:\n                for i in db.run(\n                    \"\"\"\n                    MATCH (p1:protein)&lt;-[a1:ANNOTATES]-(h1:hmm)\n                    where p1.uid in [\"_xvHFg1H1f43WxPcZT1P8Qbcx60chSxh\"]\n                    RETURN apoc.map.merge({hmm_id:h1.uid}, properties(a1))\n                    \"\"\",\n                    uid=str(self.uid),\n                ):\n                    self.add_domain(**i.value())\n        except Exception:\n            log.debug(f\"Error trying to retrieve domains for {self.uid}\")\n\n    def add_sequences_from_neo4j(self):\n        try:\n            with GraphDriver() as db:\n                for i in db.run(\n                    \"\"\"\n                    MATCH (p1:protein {uid: $uid})\n                    RETURN p1.sequence as sequence\n                    \"\"\",\n                    uid=str(self.uid),\n                ):\n                    if x := i.value():\n                        self.sequence = x\n        except Exception:\n            log.debug(f\"Error trying to retrieve sequences for {self.uid}\")\n\n    @property\n    def domain_list_sorted_by_mean_envelope_position(self):\n        # (ie can't sort a set())\n        # 'if' is to check whether domains is None\n        # A tuple is passed so that the HMM annnotations are first sorted by the envelope midpoint\n        # then ties are sorted alphabetically based on hmm_id for consistency\n        return sorted(\n            list(self.domains),\n            key=lambda tx: ((tx.env_from + tx.env_to) / 2, tx.hmm_id),\n        )\n\n    @property\n    def domain_vector(\n        self,\n    ):\n        \"\"\"Get the domain uids for a protein as an ordered list\n\n        list: list of domain uids\n        \"\"\"\n        if not self.domains:\n            log.debug(f\"Tried to get domains from domain-less protein {self}\")\n            return []\n        return [\n            i.get_hmm_id() for i in self.domain_list_sorted_by_mean_envelope_position\n        ]\n\n    def filter_domains(self):\n        \"\"\"Prune all domains in all proteins that don't meet the inclusion threshold (currently HMMER's i_evalue)\"\"\"\n        _before_count = len(self.domains)\n        temp = []\n        self.domains = {i for i in self.domains if i.domain_within_threshold}\n\n        del temp\n        log.debug(\n            f\"Removed {str(_before_count - len(self.domains))} domains from {self.external_id}\"\n        )\n\n    @property\n    def fasta_string_defline_uid(self):\n        return f\"&gt;{self.uid}\\n{self.sequence}\\n\"\n\n    @property\n    def fasta_string_defline_external_id(self):\n        return f\"&gt;{self.external_id}\\n{self.sequence}\\n\"\n\n    def _neo4j_node(self, include_sequences=True) -&gt; PROTEIN_NODE:\n        n = PROTEIN_NODE()\n        dict_to_add = {\"uid\": self.uid, \"crc64\": self.crc64}\n        if include_sequences:\n            dict_to_add[\"sequence\"] = self.sequence\n        n.fill_from_dict(dict_to_add)\n        return n\n\n    def add_to_neo4j(self, include_sequences=True):\n        n = self._neo4j_node(include_sequences=include_sequences)\n        n.add_to_neo4j(create=False)\n        for i in self.domains:\n            hmmnode = HMM_NODE()\n            hmmnode.fill_from_dict({\"uid\": i.hmm_id})\n            rel = ANNOTATES_REL(start=hmmnode, end=self._neo4j_node())\n            rel.fill_from_dict(\n                {\n                    k: v\n                    for k, v in i.all_attributes().items()\n                    if k in ANNOTATES_REL.property_specification\n                }\n            )\n            rel.add_to_neo4j(create=False)\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Protein.domain_vector","title":"<code>domain_vector</code>  <code>property</code>","text":"<p>Get the domain uids for a protein as an ordered list</p> <p>list: list of domain uids</p>"},{"location":"developing/python/classes/#socialgene.base.molbio.Protein.__init__","title":"<code>__init__(description=None, external_id=None, domains=None, *args, **kwargs)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>Protein description.</p> <code>None</code> <code>external_id</code> <code>str</code> <p>Non-hash-id descriptor (usually a database accession, e.g. NCBI's).</p> <code>None</code> <code>seqlen</code> <code>int</code> <p>Amino acid sequence length</p> required <code>domains</code> <code>Set</code> <p>Set of Domain() objects.</p> <code>None</code> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def __init__(\n    self,\n    description: str = None,\n    external_id: str = None,\n    domains: set = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"_summary_\n\n    Args:\n        description (str, optional): Protein description.\n        external_id (str, optional): Non-hash-id descriptor (usually a database accession, e.g. NCBI's).\n        seqlen (int, optional): Amino acid sequence length\n        domains (Set, optional): Set of Domain() objects.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.description = description\n    self.external_id = external_id\n    self.domains = domains if domains is not None else set()\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Protein.add_domain","title":"<code>add_domain(*args, **kwargs)</code>","text":"<p>Add a domain to a protein if it meets threshold (ievalue)</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>used for counting # of domains added to protein</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def add_domain(\n    self,\n    *args,\n    **kwargs,\n):\n    \"\"\"Add a domain to a protein if it meets threshold (ievalue)\n\n    Returns:\n        bool: used for counting # of domains added to protein\n    \"\"\"\n    self.domains.add(Domain(*args, **kwargs))\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Protein.filter_domains","title":"<code>filter_domains()</code>","text":"<p>Prune all domains in all proteins that don't meet the inclusion threshold (currently HMMER's i_evalue)</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def filter_domains(self):\n    \"\"\"Prune all domains in all proteins that don't meet the inclusion threshold (currently HMMER's i_evalue)\"\"\"\n    _before_count = len(self.domains)\n    temp = []\n    self.domains = {i for i in self.domains if i.domain_within_threshold}\n\n    del temp\n    log.debug(\n        f\"Removed {str(_before_count - len(self.domains))} domains from {self.external_id}\"\n    )\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.ProteinSequence","title":"<code>ProteinSequence</code>","text":"<p>Class used for working with protein sequences and can be initialized with either a sequence or a uid.</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>class ProteinSequence:\n    \"\"\"Class used for working with protein sequences and can be initialized with either a sequence or a uid.\"\"\"\n\n    _amino_acids = [\n        \"A\",\n        \"R\",\n        \"N\",\n        \"D\",\n        \"C\",\n        \"Q\",\n        \"E\",\n        \"G\",\n        \"H\",\n        \"I\",\n        \"L\",\n        \"K\",\n        \"M\",\n        \"F\",\n        \"P\",\n        \"S\",\n        \"T\",\n        \"W\",\n        \"Y\",\n        \"V\",\n        \"X\",\n        \"Z\",\n        \"J\",\n        \"U\",\n        \"B\",\n        \"O\",\n        \"*\",\n    ]\n    __slots__ = [\"uid\", \"__crc64\", \"__md5\", \"_seq_len\", \"sequence\"]\n\n    def __init__(self, sequence: str = None, uid: str = None, seq_len: int = None):\n        \"\"\"Class for holding an amino acid sequence (protein)\n\n        Args:\n            sequence (str, optional): amino acid sequence\n            uid (str, optional): a uid can be provided if a sequence isn't\n\n        Raises:\n            ValueError: Must provide either a sequence or a uid\n        \"\"\"\n        # the sequence input variable has a default so that domtblout can create a socialgene object\n        self.sequence = sequence\n        if isinstance(self.sequence, type(None)):\n            if isinstance(uid, type(None)):\n                raise ValueError(\"Must provide either a sequence or a uid\")\n            else:\n                self.uid = uid\n        else:\n            self._assign_hash()\n        if seq_len:\n            self._seq_len = seq_len\n\n    @property\n    def seq_len(self):\n        \"\"\"Return the length of the protein\n\n        Returns:\n            int: number of amino acids\n        \"\"\"\n        return len(self.sequence)\n\n    @seq_len.setter\n    def seq_len(self, value):\n        if isinstance(value, int):\n            self._seq_len = value\n        else:\n            self._seq_len = len(self.sequence)\n\n    def all_attributes(self):\n        \"\"\"\n        The function returns a dictionary containing the attributes of an object.\n\n        Returns:\n          The `__dict__` method is returning a dictionary that contains the names and values of all the\n        attributes of the object. The attributes are obtained using the `getattr` function and are filtered\n        to only include attributes that are defined in the `__slots__` list and that exist in the object.\n        The dictionary is sorted based on the names of the attributes.\n        \"\"\"\n        return {s: getattr(self, s) for s in sorted(self.__slots__) if hasattr(self, s)}\n\n    def _one_letter_amino_acids(self):\n        \"\"\"Create an ordered dictionary of amino acids. Used to count AAs in a protein sequence.\n\n        Returns:\n            dict: amino acid count\n        \"\"\"\n        return OrderedDict({i: 0 for i in self._amino_acids})\n\n    def _amino_acid_count(self):\n        \"\"\"Create a '-' separated string of an amino acid count\n\n        Returns:\n            str: sequence of amino acids\n        \"\"\"\n        if self.sequence is None:\n            return \"-\".join([\"0\" for i in self._one_letter_amino_acids()])\n        else:\n            return \"-\".join(\n                [str(self.sequence.count(i)) for i in self._one_letter_amino_acids()]\n            )\n\n    def _assign_hash(self):\n        \"\"\"\n        The function assigns a hash value to a sequence of amino acids.\n        \"\"\"\n        self._standardize_sequence()\n        self.uid = hasher.hash_aminos(self.sequence)\n\n    @property\n    def crc64(self):\n        return hasher.hash_aminos(self.sequence, algo=\"crc64\")\n\n    @property\n    def md5(self):\n        return hasher.hash_aminos(self.sequence, algo=\"md5\")\n\n    def _standardize_sequence(self):\n        \"\"\"\n        The function converts the protein sequence to uppercase and checks if all characters are valid\n        amino acids, raising an error if an unknown character is found.\n        \"\"\"\n        self.sequence = self.sequence.upper()\n        self.sequence = self.sequence.replace(\" \", \"\")\n        self.sequence = self.sequence.strip(\"*\")\n        if not all([i in self._amino_acids for i in set(self.sequence)]):\n            log.error(self.sequence)\n            raise ValueError(\"Unknown character/letter in protein sequence\")\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.ProteinSequence.seq_len","title":"<code>seq_len</code>  <code>property</code> <code>writable</code>","text":"<p>Return the length of the protein</p> <p>Returns:</p> Name Type Description <code>int</code> <p>number of amino acids</p>"},{"location":"developing/python/classes/#socialgene.base.molbio.ProteinSequence.__init__","title":"<code>__init__(sequence=None, uid=None, seq_len=None)</code>","text":"<p>Class for holding an amino acid sequence (protein)</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>str</code> <p>amino acid sequence</p> <code>None</code> <code>uid</code> <code>str</code> <p>a uid can be provided if a sequence isn't</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Must provide either a sequence or a uid</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def __init__(self, sequence: str = None, uid: str = None, seq_len: int = None):\n    \"\"\"Class for holding an amino acid sequence (protein)\n\n    Args:\n        sequence (str, optional): amino acid sequence\n        uid (str, optional): a uid can be provided if a sequence isn't\n\n    Raises:\n        ValueError: Must provide either a sequence or a uid\n    \"\"\"\n    # the sequence input variable has a default so that domtblout can create a socialgene object\n    self.sequence = sequence\n    if isinstance(self.sequence, type(None)):\n        if isinstance(uid, type(None)):\n            raise ValueError(\"Must provide either a sequence or a uid\")\n        else:\n            self.uid = uid\n    else:\n        self._assign_hash()\n    if seq_len:\n        self._seq_len = seq_len\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.ProteinSequence.all_attributes","title":"<code>all_attributes()</code>","text":"<p>The function returns a dictionary containing the attributes of an object.</p> <p>Returns:</p> Type Description <p>The <code>__dict__</code> method is returning a dictionary that contains the names and values of all the</p> <p>attributes of the object. The attributes are obtained using the <code>getattr</code> function and are filtered to only include attributes that are defined in the <code>__slots__</code> list and that exist in the object. The dictionary is sorted based on the names of the attributes.</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>def all_attributes(self):\n    \"\"\"\n    The function returns a dictionary containing the attributes of an object.\n\n    Returns:\n      The `__dict__` method is returning a dictionary that contains the names and values of all the\n    attributes of the object. The attributes are obtained using the `getattr` function and are filtered\n    to only include attributes that are defined in the `__slots__` list and that exist in the object.\n    The dictionary is sorted based on the names of the attributes.\n    \"\"\"\n    return {s: getattr(self, s) for s in sorted(self.__slots__) if hasattr(self, s)}\n</code></pre>"},{"location":"developing/python/classes/#socialgene.base.molbio.Taxonomy","title":"<code>Taxonomy</code>","text":"<p>Class is a reserved word so just underscore all ranks to be consistent</p> Source code in <code>socialgene/base/molbio.py</code> <pre><code>class Taxonomy:\n    \"Class is a reserved word so just underscore all ranks to be consistent\"\n    __slots__ = [\n        \"species_\",\n        \"genus_\",\n        \"family_\",\n        \"order_\",\n        \"class_\",\n        \"phylum_\",\n        \"clade_\",\n        \"superkingdom_\",\n    ]\n\n    def __init__(\n        self,\n        species_: str = None,\n        genus_: str = None,\n        family_: str = None,\n        order_: str = None,\n        class_: str = None,\n        phylum_: str = None,\n        clade_: str = None,\n        superkingdom_: str = None,\n        **kwargs,\n    ) -&gt; None:\n        self.species_ = species_\n        self.genus_ = genus_\n        self.family_ = family_\n        self.order_ = order_\n        self.class_ = class_\n        self.phylum_ = phylum_\n        self.clade_ = clade_\n        self.superkingdom_ = superkingdom_\n</code></pre>"},{"location":"developing/python/extending_socialgene/","title":"Extending socialgene","text":"<pre><code>from socialgene.neo4j.neo4j_element import Neo4jElement, Node, Relationship\n\n\nclass nodeA(Node):\n    def __init__(self, **kwargs):\n        super().__init__(\n            neo4j_label=\"nodeA\",\n            description=\"example node A\",\n            property_specification={\n                \"uid\": str,\n                \"parentmass\": float,\n            },\n            **kwargs\n        )\n\n\nclass nodeB(Node):\n    def __init__(self, **kwargs):\n        super().__init__(\n            neo4j_label=\"nodeB\",\n            description=\"example node B\",\n            property_specification={\n                \"uid\": str,\n                \"charge\": int,\n            },\n            **kwargs\n        )\n\n\nclass Rel(Relationship):\n    def __init__(self, **kwargs):\n        super().__init__(\n            neo4j_label=\"REL\",\n            description=\"example relationship\",\n            start_class=nodeA,\n            end_class=nodeB,\n            **kwargs\n        )\n\n\na = nodeA(properties={\"uid\": \"test\", \"parentmass\": 1.0})\nb = nodeB(properties={\"uid\": \"test\", \"charge\": 1})\n\nz = Rel(start=a, end=b)\n</code></pre>"},{"location":"developing/python/general/","title":"Using the main SocialGene class","text":"<p>Import and create the class:</p> <pre><code>from socialgene.base.socialgene import SocialGene\nmy_bgc = SocialGene()\n</code></pre> <p>Load a Genbank file:</p> <pre><code>my_bgc.parse_genbank(\"/home/chase/Downloads/diazaquinomycin.gbk\", append=False)\n</code></pre> <p>Annotate the proteins with pyhmmer:</p> <pre><code>protein_ids = list(my_bgc.bgc.proteins.keys())\nmy_bgc.bgc.pyhmmer_hmmsearch(hash_id_list=protein_ids)\n</code></pre>"},{"location":"developing/python/setup/","title":"Setup","text":"<p>Download the SocialGene repository</p> shell <pre><code>git clone https://github.com/chasemc/socialgene.git\n</code></pre> <p>Move to the SocialGene directory....</p> shell <p><pre><code>cd socialgene\n</code></pre> Open whatever environment you usually work in (venv, conda, etc.) that works with pip install.</p> <p>Install normally, but with update on save:</p> shell <pre><code># pip install -e .[ci]\nmake install\n</code></pre>"},{"location":"developing/python/testing/","title":"Testing the Python library","text":"<p>Unit tests are conducted using pytest and can be initiated from the top directory by running <code>make pytest</code></p>"},{"location":"django/advanced/","title":"socialgeneweb","text":""},{"location":"django/advanced/#info","title":"Info","text":"<p>99% of this templated from the cokiecutter template, when in doubt, or for more info, refer to the detailed cookiecutter-django Docker documentation</p>"},{"location":"django/advanced/#settings","title":"Settings","text":"<p>Advanced settings available from the cookiecutter template can be found here:</p> <ul> <li>http://cookiecutter-django.readthedocs.io/en/latest/settings.html</li> </ul>"},{"location":"django/advanced/#setting-up-users","title":"Setting Up Users","text":"<ul> <li> <p>To create a normal user account, just go to Sign Up and fill out the form. Once you submit it, you'll see a \"Verify Your E-mail Address\" page. Go to your console to see a simulated email verification message. Copy the link into your browser. Now the user's email should be verified and ready to go.</p> </li> <li> <p>To create an superuser account, use this command:</p> </li> </ul> shell <pre><code>python manage.py createsuperuser\n</code></pre> <p>For convenience, you can keep your normal user logged in on Chrome and your superuser logged in on Firefox (or similar), so that you can see how the site behaves for both kinds of users.</p>"},{"location":"django/advanced/#type-checks","title":"Type checks","text":"<p>Running type checks with mypy:</p> shell <pre><code>mypy socialgeneweb\n</code></pre>"},{"location":"django/advanced/#test-coverage","title":"Test coverage","text":"<p>To run the tests, check your test coverage, and generate an HTML coverage report:</p> shell <pre><code>coverage run -m pytest\ncoverage html\nopen htmlcov/index.html\n</code></pre>"},{"location":"django/advanced/#running-tests-with-pytest","title":"Running tests with py.test","text":"shell <pre><code>pytest\n</code></pre>"},{"location":"django/advanced/#live-reloading-and-sass-css-compilation","title":"Live reloading and Sass CSS compilation","text":"<p>Moved to <code>Live reloading and SASS compilation</code>_.</p> <p>.. _<code>Live reloading and SASS compilation</code>: http://cookiecutter-django.readthedocs.io/en/latest/live-reloading-and-sass-compilation.html</p>"},{"location":"django/advanced/#celery","title":"Celery","text":"<p>This app comes with Celery.</p> <p>To run a celery worker:</p> shell <pre><code>cd socialgeneweb\ncelery -A config.celery_app worker -l info\n</code></pre> <p>Please note: For Celery's import magic to work, it is important where the celery commands are run. If you are in the same folder with manage.py, you should be right.</p>"},{"location":"django/advanced/#email-server","title":"Email Server","text":"<p>In development, it is often nice to be able to see emails that are being sent from your application. For that reason local SMTP server <code>MailHog</code>_ with a web interface is available as docker container.</p> <p>Container mailhog will start automatically when you will run all docker containers. Please check <code>cookiecutter-django Docker documentation</code>_ for more details how to start all containers.</p> <p>With MailHog running, to view messages that are sent by your application, open your browser and go to <code>http://127.0.0.1:8025</code></p> <p>.. _mailhog: https://github.com/mailhog/MailHog</p>"},{"location":"django/advanced/#sentry","title":"Sentry","text":"<p>Sentry is an error logging aggregator service. You can sign up for a free account at  https://sentry.io/signup/?code=cookiecutter  or download and host it yourself. The system is setup with reasonable defaults, including 404 logging and integration with the WSGI application.</p> <p>You must set the DSN url in production.</p>"},{"location":"django/advanced/#deployment","title":"Deployment","text":"<p>The following details how to deploy this application.</p>"},{"location":"django/advanced/#docker","title":"Docker","text":"<p>See detailed <code>cookiecutter-django Docker documentation</code>_.</p> <p>.. _<code>cookiecutter-django Docker documentation</code>: http://cookiecutter-django.readthedocs.io/en/latest/deployment-with-docker.html</p>"},{"location":"django/advanced_start/","title":"Advanced start","text":""},{"location":"django/advanced_start/#adjusting-parameters-before-starting","title":"Adjusting parameters before starting","text":"<p>Make sure you're in the top SocialGene directory () and have adjusted any necessary parameters in <code>common_parameters.env</code>. Set the <code>HMM_LOCATION</code> variable to the full path of the HMMs file created from the Nextflow pipeline (<code>/home/chase/my_outdir/socialgene_long_cache/hmm_hash/socialgene_nr_hmms_file_1_of_1.hmm</code>) You shouldn't need to but if you changed any of the <code>HMMSEARCH...</code> parameters while creating the database, those settings should be exactly the same when launching the Django app. Additionally the file containing the HMMs created from the Nextflow pipeline should be.</p> <p>Neo4j memory constraints are also set from within the <code>common_parameters.env</code> file. See Neo4j memory configuration for info on how to determine optimal values.</p> shell <pre><code>NEO4J_dbms_memory_pagecache_size=3g\nNEO4J_dbms_memory_heap_initial__size=4g\nNEO4J_dbms_memory_heap_max__size=4g\n</code></pre>"},{"location":"django/advanced_start/#building-and-running-the-djang-docker-containers","title":"Building and running the Djang Docker containers","text":"<p>To start the Django and peripheral docker containers:</p> shell <pre><code>make django_up\n</code></pre> <p>Note: the first time running docker compose will take a while to download/build all the images</p>"},{"location":"django/advanced_start/#connecting-to-django","title":"Connecting to Django","text":"<p>To use the app, open a browser and go to the url: <code>127.0.0.1:8009</code></p> <p>Stop all the containers with</p> shell <pre><code>make down\n</code></pre> <p>You can check the status of all running containers using the following:</p> shell <pre><code>docker ps\n</code></pre>"},{"location":"django/advanced_start/#querying-the-database-directly-from-neo4js-browser-view","title":"Querying the database directly, from Neo4j's browser view","text":"<p>When complete, you have the option of viewing the database within Neo4j's browser (but you'll need to know Cypher to do much there https://neo4j.com/developer/cypher/)</p> <p>Note: if the Django app is already running on your computer you can skip running Docker commands below.</p>"},{"location":"django/advanced_start/#neo4j-memory-configuration","title":"Neo4j memory configuration","text":"<p>You will need to select the amount of memory Neo4j can use, this is set within the <code>NEO4J MEMORY LIMITS</code>section of the <code>common_parameters.env</code> file.</p> <p>Noe4j can help you determine the correct values. To do so run the following, substituting the amount of RAM you want to dedicate to Neo4j in <code>ram_to_provide_to_neo4j</code></p> shell <pre><code>ram_to_provide_to_neo4j=40G\n\ndocker run \\\n    --user=\"$(id -u)\":\"$(id -g)\" \\\n    --env NEO4J_AUTH=neo4j/test \\\n    --interactive \\\n    --tty \\\n    neo4j:5.1.0 \\\n        neo4j-admin \\\n            server memory-recommendation \\\n                --memory=$ram_to_provide_to_neo4j \\\n                --verbose \\\n                --docker\n</code></pre> <p>For more detailed info on memory settings in Neo4j refer to the Neo4j memrec documentation page.</p>"},{"location":"django/advanced_start/#launching-neo4j-alone","title":"Launching Neo4j alone","text":"<p>An example of launching the database directly: Set sg_neoloc below to the \"neo4j\" directory, this is the directory containing \"import\", \"data\", etc</p> shell <pre><code>sg_neoloc='/home/chase/Documents/socialgene_data/older/mibig2/socialgene_neo4j'\npipeline_version='latest'\n\nmkdir -p $sg_neoloc/conf\necho 'apoc.export.file.enabled=true' &gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.import.file.enabled=true' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.export.file.use_neo4j_config=false' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.import.file.use_neo4j_config=false' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'server.directories.import=/opt/conda/bin/neo4j/import' &gt;&gt; $sg_neoloc/conf/neo4j.conf\necho 'server.directories.export=/opt/conda/bin/neo4j/import' &gt;&gt; $sg_neoloc/conf/neo4j.conf\n\n# curl https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/5.1.0/apoc-5.1.0-extended.jar &gt; $sg_neoloc/plugins/apoc-5.1.0-extended.jar\n# wget https://github.com/xerial/sqlite-jdbc/releases/download/3.40.1.0/sqlite-jdbc-3.40.1.0.jar\n\n\ndocker run \\\n    --user=$(id -u):$(id -g) \\\n    -p7474:7474 -p7687:7687 \\\n    -v $sg_neoloc/data:/opt/conda/bin/neo4j/data \\\n    -v $sg_neoloc/logs:/opt/conda/bin/neo4j/logs \\\n    -v $sg_neoloc/import:/opt/conda/bin/neo4j/import \\\n    -v $sg_neoloc/plugins:/opt/conda/bin/neo4j/plugins \\\n    -v $sg_neoloc/conf:/opt/conda/bin/neo4j/conf \\\n        --env NEO4J_AUTH=neo4j/test \\\n        --env NEO4J_PLUGINS='[\"apoc\"]' \\\n        --env NEO4J_dbms_security_procedures_unrestricted=algo.*,apoc.*,n10s.*, \\\n        --env NEO4J_dbms_security_procedures_allowlist=algo.*,apoc.*,n10s.* \\\n        --env NEO4J_server_config_strict__validation_enabled=false \\\n        --env NEO4J_server_memory_heap_initial__size='100g' \\\n        --env NEO4J_server_memory_heap_max__size='100g' \\\n        --env NEO4J_server_memory_pagecache_size='800g' \\\n        --env NEO4J_server_jvm_additional='-XX:+ExitOnOutOfMemoryError' \\\n    chasemc2/sgnf-sgpy:$pipeline_version\n</code></pre> <p>You can then open the Neo4j database in a web browser by typing <code>localhost:7474</code> in the url bar.</p> <p>To make SocialGene queries faster you can add a couple indices for protein and hmm IDs using the following commands in the neo4j browser:</p> <p><code>CREATE CONSTRAINT ON (n:protein) ASSERT n.id IS UNIQUE</code></p> <p><code>CREATE CONSTRAINT ON (n:hmm) ASSERT n.id IS UNIQUE</code></p>"},{"location":"django/advanced_start/#other-make-commands","title":"Other MAKE commands","text":"<p>More make commands can be found by looking through the <code>Makefile</code> file or by running</p> shell <pre><code>make help\n</code></pre>"},{"location":"django/contributing/","title":"Contributing","text":""},{"location":"django/contributing/#celery-task-handler-info","title":"Celery task handler info","text":"<ul> <li>tips</li> <li>https://www.vinta.com.br/blog/2018/celery-wild-tips-and-tricks-run-async-tasks-real-world/</li> </ul> <p>TODO: this all needs to be looked through and re-written</p> <p>https://github.com/pydanny/cookiecutter-django/</p> <p>git init pre-commit install</p> <p>docker-compose -f local.yml build docker-compose -f local.yml up</p>"},{"location":"django/contributing/#create-a-new-django-app","title":"Create a new Django app","text":"<p><code>docker-compose -f local.yml run --rm django python manage.py startapp new-app-name</code></p>"},{"location":"django/contributing/#migrate-dbs","title":"Migrate DBs","text":"<p>docker-compose -f local.yml run --rm django python manage.py migrate</p>"},{"location":"django/django/","title":"Django","text":""},{"location":"django/django/#create-a-superuseradmin-account","title":"Create a superuser/admin account","text":"<p>Must be done at least once after running <code>make up</code> so the postgres database gets created.</p> <p>Run the following at the top directory of socialgene:</p> shell <pre><code>make manage createsuperuser\n</code></pre>"},{"location":"django/django/#create-super-user","title":"Create Super User","text":"<p>docker-compose -f local.yml run --rm django python manage.py createsuperuser</p> <p>https://www.csestack.org/create-html-form-insert-data-database-django/</p>"},{"location":"django/quickstart/","title":"Quickstart","text":"<p>First you need to create directories for postgres and redis data, otherwise docker-compose will create them as root user</p> shell <pre><code>mkdir local_postgres_data local_postgres_data_backups redis-data\n</code></pre> <p>You also need to set the environment variable <code>HMM_LOCATION</code> to the path of the hmms file used to create the Neo4j database</p> <p>Run the docker compose/build all containers</p> shell <pre><code>make django_up\n</code></pre> <p>Another Make command <code>upq</code> is a quicker shortcut that also doesn't run the build stage</p> <p>Stop all containers</p> shell <pre><code>make down \n</code></pre> <p><code>make up</code> runs everything in the background, <code>make upq</code> doesn't build, just runs in foreground</p> <p>(base) chase@titan:~/Documents/github/socialgene/django/socialgeneweb$ npm install</p> <p>make makemigrations</p>"},{"location":"hasher/hasher/","title":"hasher","text":"FASTA Hasher <pre><code>&lt;p&gt;This tool hashes the sequence of each FASTA entry using sha512t24, described in:&lt;/p&gt;\n&lt;cite&gt;Hart, R. K. &amp;amp; Prli\u0107, A. SeqRepo: A system for managing local collections of biological sequences. PLoS One 15, (2020).&lt;/cite&gt;\n\n&lt;br&gt;&lt;br&gt;\n&lt;textarea id=\"fastaInput\" rows=\"10\" cols=\"50\" placeholder=\"Paste your FASTA sequence here\"&gt;&lt;/textarea&gt;&lt;br&gt;\n&lt;button onclick=\"processFasta()\"&gt;Generate Hashed FASTA&lt;/button&gt;\n&lt;button onclick=\"clearAll()\"&gt;Clear All&lt;/button&gt;\n&lt;h2&gt;FASTA with deflines replaced with sha512t24 hash&lt;/h2&gt;\n&lt;textarea id=\"fastaOutput\" rows=\"10\" cols=\"50\" readonly&gt;&lt;/textarea&gt;&lt;br&gt;\n&lt;button onclick=\"downloadFasta()\"&gt;Download FASTA&lt;/button&gt;\n&lt;h2&gt;TSV mapping input deflines to sha512t24 hash&lt;/h2&gt;\n&lt;textarea id=\"deflineMap\" rows=\"10\" cols=\"50\" readonly&gt;&lt;/textarea&gt;&lt;br&gt;\n&lt;button onclick=\"downloadMap()\"&gt;Download Defline Map&lt;/button&gt;\n\n&lt;script&gt;\n    async function sha512t24u(input) {\n        const encoder = new TextEncoder();\n        const data = encoder.encode(input.toUpperCase());\n        const hashBuffer = await crypto.subtle.digest('SHA-512', data);\n        const first24Bytes = hashBuffer.slice(0, 24);\n        const base64String = btoa(String.fromCharCode(...new Uint8Array(first24Bytes)))\n            .replace(/\\+/g, '-').replace(/\\//g, '_').replace(/=+$/, '');\n        return base64String;\n    }\n\n    async function processFasta() {\n        const fastaInput = document.getElementById(\"fastaInput\").value;\n        const lines = fastaInput.split(\"\\n\");\n\n        let result = \"\";\n        let deflineMap = \"\"; // No header\n        let currentDefline = \"\";\n        let sequenceBuffer = \"\";\n\n        for (let i = 0; i &lt; lines.length; i++) {\n            const line = lines[i].trim();\n\n            if (line.startsWith(\"&gt;\")) {\n                if (currentDefline &amp;&amp; sequenceBuffer) {\n                    // Normalize sequence by removing all non-alphabetic characters\n                    const normalizedSequence = sequenceBuffer.replace(/[^A-Za-z]/g, \"\");\n                    const hash = await sha512t24u(normalizedSequence); // Hash the normalized sequence\n                    result += `&gt;${hash}\\n${sequenceBuffer}\\n`;\n                    deflineMap += `${currentDefline}\\t${hash}\\n`;\n                }\n\n                // Start a new defline and reset sequence buffer\n                currentDefline = line.slice(1).replace(/\\t/g, \"\").trim();  // Remove \"&gt;\", tabs, and trim whitespace\n                sequenceBuffer = \"\";\n            } else {\n                sequenceBuffer += line;  // Accumulate sequence lines into a single sequence\n            }\n        }\n\n        // Process the last sequence and defline\n        if (currentDefline &amp;&amp; sequenceBuffer) {\n            const normalizedSequence = sequenceBuffer.replace(/[^A-Za-z]/g, \"\");\n            const hash = await sha512t24u(normalizedSequence); // Hash the normalized sequence\n            result += `&gt;${hash}\\n${sequenceBuffer}\\n`;\n            deflineMap += `${currentDefline}\\t${hash}\\n`;\n        }\n\n        document.getElementById(\"fastaOutput\").value = result.trim();\n        document.getElementById(\"deflineMap\").value = deflineMap.trim();\n    }\n\n    function downloadFasta() {\n        const fastaOutput = document.getElementById(\"fastaOutput\").value;\n        const blob = new Blob([fastaOutput], { type: 'text/plain' });\n        const url = URL.createObjectURL(blob);\n        const a = document.createElement('a');\n        a.href = url;\n        a.download = 'output.fasta';\n        document.body.appendChild(a);\n        a.click();\n        document.body.removeChild(a);\n    }\n\n    function downloadMap() {\n        const deflineMap = document.getElementById(\"deflineMap\").value;\n        const blob = new Blob([deflineMap], { type: 'text/tab-separated-values' });\n        const url = URL.createObjectURL(blob);\n        const a = document.createElement('a');\n        a.href = url;\n        a.download = 'defline_map.tsv';\n        document.body.appendChild(a);\n        a.click();\n        document.body.removeChild(a);\n    }\n\n    function clearAll() {\n        document.getElementById(\"fastaInput\").value = \"\";\n        document.getElementById(\"fastaOutput\").value = \"\";\n        document.getElementById(\"deflineMap\").value = \"\";\n    }\n&lt;/script&gt;\n</code></pre>"},{"location":"large_scale/large_scale_annotation/","title":"Large scale annotation","text":""},{"location":"large_scale/large_scale_annotation/#running-hmmsearch-at-scale-on-chtc","title":"Running hmmsearch at scale on CHTC","text":"<p>These instructions cover how to annotate large numbers of proteins with HMMER, using the University of Wisconsin-Madison's Center for High Throughput Computing (CHTC) facilities. If your facility uses HTCondor many of the steps may be similar.</p> <p>While Nextflow does have support for HTCondor, CHTC's filesystems aren't configured for that kind of use. Hopefully this will change in a future version of Nextflow (https://github.com/nextflow-io/nextflow/issues/1473). For this reason SocialGene has a custom implementation for running large numbers of hmmsearch jobs on CHTC.</p> <p>Note: While this is specific to CHTC, the general principles and files can be applied to other HTCondor and additional high-throughput compute environments.</p> <pre><code>outdir='/tmp/socialgene_data/ultraquickstart'\noutdir_download_cache='/tmp/socialgene_data/cache'\n\nnextflow run socialgene/sgnf \\\n    -profile ultraquickstart,docker \\\n    --outdir $outdir \\\n    --outdir_download_cache $outdir_download_cache \\\n    --max_cpus 4 \\\n    --max_memory 4.GB \\\n    --htcondor \n</code></pre> <p>And we can see the following progress:</p> <pre><code>Manifest's pipeline version: ultraquickstart,docker\nexecutor &gt;  local (18)\n[82/f30982] process &gt; SOCIALGENE_SGNF:SOCIALGENE:PARAMETER_EXPORT_FOR_NEO4J                                   [100%] 1 of 1 \u2714\n[skipped  ] process &gt; SOCIALGENE_SGNF:SOCIALGENE:GENOME_HANDLING:NCBI_GENOME_DOWNLOAD                         [100%] 1 of 1, stored: 1 \u2714\n[1f/90b022] process &gt; SOCIALGENE_SGNF:SOCIALGENE:GENOME_HANDLING:PROCESS_GENBANK_FILES (1)                    [100%] 1 of 1 \u2714\n[b3/185427] process &gt; SOCIALGENE_SGNF:SOCIALGENE:GENOME_HANDLING:DEDUPLICATE_GENOMIC_INFO (assembly_to_locus) [100%] 5 of 5 \u2714\n[6d/9382b1] process &gt; SOCIALGENE_SGNF:SOCIALGENE:GENOME_HANDLING:DEDUPLICATE_PROTEIN_INFO (protein_ids)       [100%] 2 of 2 \u2714\n[d2/bbae62] process &gt; SOCIALGENE_SGNF:SOCIALGENE:DEDUPLICATE_AND_INDEX_FASTA                                  [100%] 1 of 1 \u2714\n[f9/6d31f6] process &gt; SOCIALGENE_SGNF:SOCIALGENE:SEQKIT_SPLIT                                                 [100%] 1 of 1 \u2714\n[skipped  ] process &gt; SOCIALGENE_SGNF:SOCIALGENE:HMM_PREP:GATHER_HMMS:DOWNLOAD_HMM_DATABASE (amrfinder)       [100%] 1 of 1, stored: 1 \u2714\n[27/690ff7] process &gt; SOCIALGENE_SGNF:SOCIALGENE:HMM_PREP:HMM_HASH                                            [100%] 1 of 1 \u2714\n[3c/6a59e2] process &gt; SOCIALGENE_SGNF:SOCIALGENE:HTCONDOR_PREP                                                [100%] 1 of 1 \u2714\n[-        ] process &gt; SOCIALGENE_SGNF:SOCIALGENE:HMMSEARCH_PARSE                                              -\n[-        ] process &gt; SOCIALGENE_SGNF:SOCIALGENE:MERGE_PARSED_DOMTBLOUT                                       -\n[b9/44ec67] process &gt; SOCIALGENE_SGNF:SOCIALGENE:NEO4J_HEADERS                                                [100%] 1 of 1 \u2714\n[83/9e580b] process &gt; SOCIALGENE_SGNF:SOCIALGENE:DOWNLOAD_GOTERMS                                             [100%] 1 of 1 \u2714\n[a5/95eed1] process &gt; SOCIALGENE_SGNF:SOCIALGENE:NEO4J_ADMIN_IMPORT_DRYRUN (Building Neo4j database)          [100%] 1 of 1 \u2714\n[01/543381] process &gt; SOCIALGENE_SGNF:SOCIALGENE:CUSTOM_DUMPSOFTWAREVERSIONS (1)                              [100%] 1 of 1 \u2714\n[02/1230ce] process &gt; SOCIALGENE_SGNF:SOCIALGENE:MULTIQC                                                      [100%] 1 of 1 \u2714\n-[socialgene/sgnf] Pipeline completed successfully-\n</code></pre> <p>After the pipeline is completed, the output directory will contain a new directory called <code>htcondor_cache</code>:</p> <pre><code>ls -l /tmp/socialgene_data/ultraquickstart/htcondor_cache\n</code></pre> <p>Which will contain the following files:</p> <pre><code>chtc_submission_file.sub\nfasta.tar\nhmmsearch.sh\nhmm.tar\ninstructions.txt\nsample_matrix.csv\nsubmit_server_finish.sh\nsubmit_server_setup.sh\nversions.yml\n</code></pre> <p>The <code>instructions.txt</code> file will contain the instructions to submit the job on CHTC.</p> <p>After the last step on HTCondor (running <code>submit_server_finish.sh</code>) there will be a file called <code>chtc_results.tar</code>. Transfer the file to your local machine and extract it (preferably to an empty directory ). For every job that was run on CHTC, there will be a file ending in <code>.domtblout.gz</code>. </p> <p>For example, to extract the files to <code>/tmp/chtc_results</code>:</p> <pre><code>tar -xvf chtc_results.tar -C /tmp/chtc_results\n</code></pre> <p>The pipeline can will then be run a second time with the <code>--domtblout_path</code> parameter pointing to the directory where the <code>.domtblout.gz</code> files are located (make sure to enclose path in single quotes <code>'</code>, not double <code>\"</code>); the <code>--htcondor</code> parameter removed; and the <code>-resume</code> parameter added. For example:</p> <pre><code>outdir='/tmp/socialgene_data/ultraquickstart'\noutdir_download_cache='/tmp/socialgene_data/cache'\n\nnextflow run socialgene/sgnf \\\n    -profile ultraquickstart,docker \\\n    --outdir $outdir \\\n    --outdir_download_cache $outdir_download_cache \\\n    --max_cpus 4 \\\n    --max_memory 4.GB \\\n    --domtblout_path '/tmp/chtc_results/*.domtblout.gz' \\\n    -resume\n</code></pre>"},{"location":"misc/estimating_data/","title":"Estimating data","text":"<p>You can use the NCBI's Taxonomy Browser to get somewhat of an estimate of how big your dataset will be when downloading taxa from NCBI (https://www.ncbi.nlm.nih.gov/taxonomy). The number of \"Nucleotide\", \"Protein\", \"Identical Protein Groups\", and \"Assembly\" numbers correlate to:</p> <ul> <li>Nucleotide</li> <li>(:nucleotide) nodes</li> <li>Protein</li> <li>[:ENCODES] relationships</li> <li>Identical Protein Groups</li> <li>(:protein) nodes</li> <li>Assembly</li> <li>(:assembly) nodes</li> </ul> <p>An example for Actinomycetota is shown below</p> <p></p>"},{"location":"neo4j/database_backups/","title":"Database Backups","text":""},{"location":"neo4j/database_backups/#create-a-full-database-dumpbackup","title":"Create a full database dump/backup","text":"<p>The following takes a database named \"neo4j\" found in <code>database/location/socialgene_neo4j</code> and creates a single dump file at <code>database/location/socialgene_neo4j/backups/neo4j.dump</code>. While it will be smaller than the space occupied by the <code>database/location/socialgene_neo4j</code> directory, the file can still be quite large.</p> shell <pre><code>sg_neoloc='/database/location/socialgene_neo4j'\n# mkdir because the docker image will create dirs as root if they don't exist\nmkdir -p $sg_neoloc/backups\n\ndocker run \\\n    --user=$(id -u):$(id -g) \\\n    --interactive \\\n    --tty \\\n    --rm \\\n    --volume=$sg_neoloc/data:/data \\\n    --volume=$sg_neoloc/backups:/backups \\\n    --env NEO4J_AUTH=neo4j/test \\\n    neo4j/neo4j-admin:5.16.0 \\\n        neo4j-admin database dump \\\n            --to-path=/backups \\\n            neo4j\n</code></pre>"},{"location":"neo4j/database_backups/#restore-from-a-full-database-dumpbackup","title":"Restore from a full database dump/backup","text":"<p>Given a Neo4j database dump file at path <code>$dump_path</code>, rehydrate the database inside directory <code>$sg_neoloc</code>.</p> shell <pre><code>dump_path='/path/to/neo4j.dump'\nsg_neoloc='/path/to/new/db/directory'\npipeline_version='latest'\n\n# mkdir because the docker image will create dirs as root if they don't exist\nmkdir -p $sg_neoloc/data\nmkdir -p $sg_neoloc/logs\nmkdir -p $sg_neoloc/plugins\nmkdir -p $sg_neoloc/conf\nmkdir -p $sg_neoloc/import\n\ndocker run \\\n    --user=$(id -u):$(id -g) \\\n    --interactive \\\n    --tty \\\n    --rm \\\n    --volume=$sg_neoloc/data:/opt/conda/bin/neo4j/data \\\n    --volume=$sg_neoloc/plugins:/opt/conda/bin/neo4j/plugins \\\n    --volume=$sg_neoloc/logs:/opt/conda/bin/neo4j/logs \\\n    --volume=$dump_path:/opt/conda/bin/neo4j/neo4j.dump \\\n    --env NEO4J_AUTH=neo4j/test \\\n    chasemc2/sgnf-sgpy:$pipeline_version \\\n        neo4j-admin database load \\\n            --from-path=. \\\n            neo4j         \n</code></pre> <p>Note</p> <p>The script below will create the database named as \"neo4j\", no matter what the $dump_path file name is. To change the db name you would have to modify both <code>--volume=$dump_path:/opt/conda/bin/neo4j/neo4j.dump \\</code> and the last <code>neo4j</code> in the Docker command. Unless you are familiar with Neo4j, and want to load multiple databases at once, you should probably leave it as \"neo4j\".</p>"},{"location":"neo4j/database_backups/#restore-faster-please","title":"Restore faster please","text":"<p>The rehydration step is quite I/O intensive. Therefore, for larger database dumps, and if you have enough spare RAM, it may be beneficial to copy the database dump file onto RAM first and then load/rehydrate so that read and write won't be occuring on the same hard drive. On Ubuntu Linux that would look something like this:</p> shell <pre><code>dump_path='/path/to/neo4j.dump'\nsg_neoloc='/path/to/new/db/directory'\npipeline_version='latest'\n\n# copy the dump file to RAM\nmkdir -p /dev/shm/social_gene_dump\ncp $dump_path /dev/shm/social_gene_dump\n\n# Change the $dump_path\ndump_path='/dev/shm/social_gene_dump/neo4j.dump'\n\n# mkdir because the docker image will create dirs as root if they don't exist\nmkdir -p $sg_neoloc/data\nmkdir -p $sg_neoloc/logs\nmkdir -p $sg_neoloc/plugins\nmkdir -p $sg_neoloc/conf\nmkdir -p $sg_neoloc/import\n\ndocker run \\\n    --user=$(id -u):$(id -g) \\\n    --interactive \\\n    --tty \\\n    --rm \\\n    --volume=$sg_neoloc/data:/opt/conda/bin/neo4j/data \\\n    --volume=$sg_neoloc/plugins:/opt/conda/bin/neo4j/plugins \\\n    --volume=$sg_neoloc/logs:/opt/conda/bin/neo4j/logs \\\n    --volume=$dump_path:/opt/conda/bin/neo4j/neo4j.dump \\\n    --env NEO4J_AUTH=neo4j/test \\\n    chasemc2/sgnf-sgpy:$pipeline_version \\\n        neo4j-admin database load \\\n            --from-path=. \\\n            neo4j\n</code></pre>"},{"location":"neo4j/database_backups/#more-info","title":"More info","text":"<p>https://neo4j.com/docs/operations-manual/current/backup-restore/offline-backup/</p>"},{"location":"neo4j/database_backups/#ultraquickstart-example","title":"Ultraquickstart Example","text":"<p>A database dump generated from the ultraquickstart example can be found at https://github.com/socialgene/sgnf/releases/download/v0.2.4/ultraquickstart.dump</p>"},{"location":"neo4j/database_backups/#launch-the-database","title":"Launch the Database","text":"<p>After hydrating a database dump you can launch it using the directions in Database Launch</p>"},{"location":"neo4j/database_launch/","title":"Database Launch","text":""},{"location":"neo4j/database_launch/#neo4j-memory-configuration","title":"Neo4j memory configuration","text":"<p>You will need to select the amount of memory Neo4j will be allowed to use, this is set within the <code>NEO4J MEMORY LIMITS</code> section of the <code>common_parameters.env</code> file; or through the docker run command.</p> <p>It's hard to estimate but for optimal performance <code>ram_to_provide_to_neo4j</code> below should probably be around the size of the database on disk +10-20%, however it will run with significantly less than that (e.g. A 50-gene BGC was searched against the 500GB RefSeq database using only <code>ram_to_provide_to_neo4j=5GB</code>), it will just require more disk I/O which means it will become dependent on hard drive speed.</p> <p>The size of the database on disk can be found by looking at the size of the \"socialgene_neo4j/data\" directory which is located where you specified <code>--outdir</code> when running the Nextflow workflow:</p> shell <pre><code>du -sh $outdir/socialgene_neo4j/data\n10G    ./socialgene_neo4j/data\n</code></pre> <p>Noe4j can help you determine memory settings based on a given total RAM to make available. To do so run the following, substituting the amount of RAM you want to dedicate to Neo4j in <code>ram_to_provide_to_neo4j</code>. Continuing the example above, we'll set <code>ram_to_provide_to_neo4j</code> to 10GB plus 20%</p> shell <pre><code>ram_to_provide_to_neo4j=12G\npipeline_version='latest'\n\ndocker run \\\n    --user=\"$(id -u)\":\"$(id -g)\" \\\n    --env NEO4J_AUTH=neo4j/test \\\n    --interactive \\\n    --tty \\\n    chasemc2/sgnf-sgpy:$pipeline_version \\\n        neo4j-admin \\\n            server memory-recommendation \\\n                --memory=$ram_to_provide_to_neo4j \\\n                --verbose \\\n                --docker\n</code></pre> <p>That will spit out a lot of text, but the important lines we'll steal are:</p> shell <pre><code>NEO4J_server_memory_heap_initial__size='4600m'\nNEO4J_server_memory_heap_max__size='4600m'\nNEO4J_server_memory_pagecache_size='4g'\n</code></pre> <p>Which can be used to modifiy the respective lines of the docker command under the \"Launching Neo4j\" section below.</p> <p>For more detailed info on memory settings in Neo4j refer to the Neo4j memrec documentation page.</p>"},{"location":"neo4j/database_launch/#launching-neo4j","title":"Launching Neo4j","text":"<p>An example of launching the database directly: Set sg_neoloc below to the \"neo4j\" directory, this is the directory created by Nextflow workflow containing the folders \"import\", \"data\", etc And set the memory values to be the results from \"Neo4j memory configuration\" above.</p> shell <pre><code>sg_neoloc='/my/nextflow_outdir/socialgene_neo4j'\n\nNEO4J_server_memory_heap_initial__size='4600m'\nNEO4J_server_memory_heap_max__size='4600m'\nNEO4J_server_memory_pagecache_size='4g'\n\nmkdir -p $sg_neoloc/conf\n# Allow import and export of files from database\necho 'apoc.export.file.enabled=true' &gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.import.file.enabled=true' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.export.file.use_neo4j_config=false' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.import.file.use_neo4j_config=false' &gt;&gt; $sg_neoloc/conf/apoc.conf\n# Set import/export of files from database to $sg_neoloc/import\necho 'server.directories.import=/opt/conda/bin/neo4j/import' &gt;&gt; $sg_neoloc/conf/neo4j.conf\necho 'server.directories.export=/opt/conda/bin/neo4j/import' &gt;&gt; $sg_neoloc/conf/neo4j.conf\n</code></pre> shell <pre><code>docker run \\\n    --user=$(id -u):$(id -g) \\\n    -p7474:7474 -p7687:7687 \\\n    -v $sg_neoloc/data:/data \\\n    -v $sg_neoloc/logs:/logs \\\n    -v $sg_neoloc/import:/opt/conda/bin/neo4j/import \\\n    -v $sg_neoloc/plugins:/plugins \\\n    -v $sg_neoloc/conf:/opt/conda/bin/neo4j/conf \\\n        --env NEO4J_AUTH=neo4j/test12345 \\\n        --env NEO4J_PLUGINS='[\"apoc\", \"graph-data-science\"]' \\\n        --env NEO4J_dbms_security_procedures_unrestricted=algo.*,apoc.*,n10s.*,gds.*, \\\n        --env NEO4J_dbms_security_procedures_allowlist=algo.*,apoc.*,n10s.*,gds.* \\\n        --env NEO4J_server_config_strict__validation_enabled=false \\\n        --env NEO4J_server_memory_heap_initial__size=$NEO4J_server_memory_heap_initial__size \\\n        --env NEO4J_server_memory_heap_max__size=$NEO4J_server_memory_heap_max__size \\\n        --env NEO4J_server_memory_pagecache_size=$NEO4J_server_memory_pagecache_size \\\n        --env NEO4J_server_jvm_additional='-XX:+ExitOnOutOfMemoryError' \\\n    neo4j:5.17.0\n</code></pre> <p>If you have paid for the enterprise version of Neo4j you can use the following:</p> <pre><code>docker run \\\n    --user=$(id -u):$(id -g) \\\n    -p7474:7474 -p7687:7687 \\\n    -v $sg_neoloc/data:/data \\\n    -v $sg_neoloc/logs:/logs \\\n    -v $sg_neoloc/import:/var/lib/neo4j/import \\\n    -v $sg_neoloc/plugins:/plugins \\\n    -v $sg_neoloc/conf:/var/lib/neo4j/conf \\\n        --env NEO4J_AUTH=neo4j/test12345 \\\n        --env NEO4J_PLUGINS='[\"apoc\", \"graph-data-science\"]' \\\n        --env NEO4J_dbms_security_procedures_unrestricted=algo.*,apoc.*,n10s.*,gds.*, \\\n        --env NEO4J_dbms_security_procedures_allowlist=algo.*,apoc.*,n10s.*,gds.* \\\n        --env NEO4J_server_config_strict__validation_enabled=false \\\n        --env NEO4J_server_memory_heap_initial__size=$NEO4J_server_memory_heap_initial__size \\\n        --env NEO4J_server_memory_heap_max__size=$NEO4J_server_memory_heap_max__size \\\n        --env NEO4J_server_memory_pagecache_size=$NEO4J_server_memory_pagecache_size \\\n        --env NEO4J_server_jvm_additional='-XX:+ExitOnOutOfMemoryError' \\\n        --env NEO4J_ACCEPT_LICENSE_AGREEMENT='yes' \\\n    neo4j:5.17.0-enterprise\n</code></pre> <p>You can then open the Neo4j database in a web browser by typing <code>localhost:7474</code> in the url bar.</p>"},{"location":"neo4j/interacting/","title":"Interacting","text":""},{"location":"neo4j/interacting/#querying-the-database-directly-from-neo4js-browser-view","title":"Querying the database directly, from Neo4j's browser view","text":"<p>One of the easiest methods to start playing with your database is viewing it within Neo4j's web browser (but you'll need to know the graph query language \"Cypher\")</p>"},{"location":"neo4j/interacting/#from-vscode","title":"From VSCode","text":"<p>You can connect to and query the database from inside VSCode editor. This is helpful for writing/debugging code that interacts with the database. See: https://neo4j.com/developer-blog/run-cypher-without-leaving-your-ide-with-neo4j-vscode-extension/</p>"},{"location":"neo4j/interacting/#cytoscape","title":"Cytoscape","text":"<p>A running Neo4j database can be queried from within Cytoscape for visualizing and creating presentable subgraphs</p> <ul> <li>Cytoscape can be downloaded here: https://cytoscape.org/download.html</li> <li>And the Neo4j Cytoscape plugin can be installed from here: https://apps.cytoscape.org/apps/cytoscapeneo4jplugin</li> </ul>"},{"location":"neo4j/interacting/#import-into-cytoscape","title":"Import into Cytoscape","text":"<p>In Cytoscape go to the <code>Apps</code> menu at the top of the window and select <code>Cypher Queries</code>, within that submenu select <code>Import Cypher Query</code>. A popup should appear asking for credentials. These are what were provided as arguments to the Docker command that started up the database. If you didn't modify the Docker command then all you should have to enter is the password which is <code>test12345</code></p> <p></p> <p>Then enter your beautiful Cypher query (in this example all nodes and relationships within 3-hops from BGC0000001 will be imported) </p> <p>And witness your creation </p>"},{"location":"neo4j/slurm/","title":"Running the Database with Slurm","text":"<p>Save the script below as <code>launch-sg-database.sh</code>.</p> <p>Modify <code>sg_neoloc</code>variable to the filepath of the database (directory containing <code>data</code>, <code>logs</code>, <code>import</code>, <code>plugins</code>, and <code>conf</code>) Modify cpu, memory, and time requirements as needed.</p> <p>And then launch with <code>srun launch-sg-database.sh</code> To launch in the background use <code>sbatch launch-sg-database.sh</code></p> <pre><code>#!/bin/bash\n#\n#SBATCH --job-name=sg-database\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 4\n#SBATCH --mem-per-cpu=60G\n#SBATCH --time=24:00:00\n#SBATCH -o sg-database.%N.%j.out # STDOUT\n#SBATCH -e sg-database.%N.%j.err # STDERR\n\nsg_neoloc='/tmp/sg'\n\n\n# ram_to_provide_to_neo4j=60G\n\n# docker run \\\n#     --user=\"$(id -u)\":\"$(id -g)\" \\\n#     --env NEO4J_AUTH=neo4j/test \\\n#     --interactive \\\n#     --tty \\\n#     neo4j:5.16.0 \\\n#         neo4j-admin \\\n#             server memory-recommendation \\\n#                 --memory=$ram_to_provide_to_neo4j \\\n#                 --verbose \\\n#                 --docker\n\n\n\nNEO4J_server_memory_heap_initial__size='23000m'\nNEO4J_server_memory_heap_max__size='23000m'\nNEO4J_server_memory_pagecache_size='26g'\n\n\nmkdir -p $sg_neoloc/conf\n# Allow import and export of files from database\necho 'apoc.export.file.enabled=true' &gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.import.file.enabled=true' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.export.file.use_neo4j_config=false' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.import.file.use_neo4j_config=false' &gt;&gt; $sg_neoloc/conf/apoc.conf\n# Set import/export of files from database to $sg_neoloc/import\necho 'server.directories.import=/var/lib/neo4j/import' &gt;&gt; $sg_neoloc/conf/neo4j.conf\necho 'server.directories.export=/var/lib/neo4j/import' &gt;&gt; $sg_neoloc/conf/neo4j.conf\n\n\ndocker run \\\n    --user=$(id -u):$(id -g) \\\n    -p7474:7474 -p7687:7687 \\\n    -v $sg_neoloc/data:/data \\\n    -v $sg_neoloc/logs:/logs \\\n    -v $sg_neoloc/import:/var/lib/neo4j/import \\\n    -v $sg_neoloc/plugins:/plugins \\\n    -v $sg_neoloc/conf:/var/lib/neo4j/conf \\\n        --env NEO4J_AUTH=neo4j/test12345 \\\n        --env NEO4J_PLUGINS='[\"apoc\", \"graph-data-science\"]' \\\n        --env NEO4J_dbms_security_procedures_unrestricted=algo.*,apoc.*,n10s.*,gds.*, \\\n        --env NEO4J_dbms_security_procedures_allowlist=algo.*,apoc.*,n10s.*,gds.* \\\n        --env NEO4J_server_config_strict__validation_enabled=false \\\n        --env NEO4J_server_memory_heap_initial__size=$NEO4J_server_memory_heap_initial__size \\\n        --env NEO4J_server_memory_heap_max__size=$NEO4J_server_memory_heap_max__size \\\n        --env NEO4J_server_memory_pagecache_size=$NEO4J_server_memory_pagecache_size \\\n        --env NEO4J_server_jvm_additional='-XX:+ExitOnOutOfMemoryError' \\\n    neo4j:5.16.0\n</code></pre>"},{"location":"nextflow/developing/","title":"Developing","text":"<p>Some docker image versions can be set using workflow parameters, this allows local custom versions to be used/tested instead of the default (the version specified by the workflow manifest)</p> <pre><code>--sgnf_antismash_dockerimage\n--sgnf_hmmer_dockerimage\n--sgnf_hmmer_plus_dockerimage\n--sgnf_sgpy_dockerimage\n--sgnf_minimal_dockerimage\n</code></pre>"},{"location":"nextflow/docker_images/","title":"Docker and the Nextflow Pipeline","text":""},{"location":"nextflow/docker_images/#overview-of-docker-images","title":"Overview of Docker images","text":"<p>The following Docker images are used in the Nextflow pipeline. While <code>sgnf_sgpy</code> is still a bit of an \"everything but the kitchen sink\"  image, the idea is to have separate images for software like antiSMASH which has a large Docker image (to ensure it's only used when antiSMASH is being run) and HMMER which is run in a highly-parallel fashion and should have as small an image as possible.</p> <p>Docker images:</p> <ul> <li>sgnf_sgpy<ul> <li>bit of a kitchen sink container that contains SocialGene's python package (using pip/git), Neo4j, FASTA manipulation tools, DIAMOND, mmseqs2 and more</li> </ul> </li> <li>sgnf_minimal<ul> <li>small image only used for downloading files from the internet and simple file manipulation</li> </ul> </li> <li>sgnf_antismash<ul> <li>conda version of antismash; is labeled by the antismash version it contains</li> </ul> </li> <li>sgnf_hmmer<ul> <li>a minimial HMMER image</li> </ul> </li> <li>sgnf_hmmer_plus_dockerimage<ul> <li>contains HMMER and some extras for downloading/unzipping files, etc.</li> </ul> </li> </ul> <p>All of the docker images are automatically built and uploaded to DockerHub every time a new release of the Nextflow workflow is created. So there should always be a docker image version that matches the SocialGene Nextflow workflow version.</p> <p>These docker image versions can also be set using workflow parameters to allow local custom images to be used/tested instead of the default (the version specified by the workflow manifest)</p> <p>Workflow parameters:</p> shell <pre><code>--sgnf_antismash_dockerimage\n--sgnf_hmmer_dockerimage\n--sgnf_hmmer_plus_dockerimage\n--sgnf_sgpy_dockerimage\n--sgnf_minimal_dockerimage\n</code></pre> <p>For example, if I've created a local dev version of the \"sgnf_sgpy\" docker image  <code>docker build . -t -chasemc2/sgnf-sgpy:dev</code> I could then run the pipeline using this image by using the Nextflow parameter: <code>--sgnf_sgpy_dockerimage 'dev'</code></p>"},{"location":"nextflow/examples/","title":"Examples","text":"<p>The most reproducible way to change how the pipeline should run is to use custom Nextflow configuration files.</p> <p>There are a number of example configuration files here: https://github.com/socialgene/sgnf/tree/main/conf/examples</p>"},{"location":"nextflow/examples/#use-a-profile-included-in-socialgene","title":"Use a profile included in SocialGene","text":"<p>To use a built-in configuration file (the ones listed at the link above) use the <code>-profile whichever-profile-i-want</code> flag.</p> <p>For example, the command below will use the <code>ultraquickstart</code> configuration file (https://github.com/socialgene/sgnf/blob/main/conf/examples/ultraquickstart.config) and run the processes using Docker.</p> shell <pre><code>nextflow run socialgene/sgnf -r main \\\n    -profile ultraquickstart,docker \n</code></pre>"},{"location":"nextflow/inputs/","title":"Inputs","text":"<p>There are a few different ways to input proteins and/or genomes into the Nextflow workflow.</p>"},{"location":"nextflow/inputs/#local-files","title":"Local files","text":""},{"location":"nextflow/inputs/#genomes","title":"Genomes","text":"<p>To run the pipeline using already downloaded/local genbank files (e.g. <code>.gbk</code>or <code>.gbff</code>) provide the path to the files via <code>--local_genbank</code>. This can be a glob pattern.</p> <p>Note</p> <p>If you are entering parameters on the command line and using a glob, make sure to enclose the path/glob in single quotes to prevent expansion.</p> <p>Note</p> <p>The genbank files must already contain protein sequences (SocialGene doesn't currently do any gene/ORF prediction).</p> shell <pre><code>nextflow run \\\n    socialgene/sgnf \\\n  --local_genbank '/path/to/genbank/files/*.gbk' \\\n  ...\n  ...\n</code></pre>"},{"location":"nextflow/inputs/#proteins","title":"Proteins","text":"<p>You can also input non-genomic proteins using local protein FASTA files (e.g. <code>.faa</code>). These will be connected to \"nucleotide\" and \"assembly\" nodes with a filename identifier.</p> <p>Provide the path to the files via <code>--local_fasta</code>. This can be a glob pattern.</p> <p>Note</p> <p>If you are entering parameters on the command line and using a glob, make sure to enclose the path/glob in single quotes to prevent expansion.</p> shell <pre><code>nextflow run \\\n    socialgene/sgnf \\\n  --local_genbank '/path/to/protein/fasta/files/*.fasta' \\\n  ...\n  ...\n</code></pre>"},{"location":"nextflow/inputs/#retrieve-genomes-from-ncbi","title":"Retrieve genomes from NCBI","text":""},{"location":"nextflow/inputs/#ncbi-genome-download-preferred","title":"ncbi-genome-download (preferred)","text":"<p>The Nextflow workflow contains Kai Blin's <code>ncbi-genome-download</code> tool which can be used to retrieve genomes from NCBI. This can be done by using the Nextflow workflow parameter <code>ncbi_genome_download_command</code>, which simply passes an argument string to the ncbi-genome-download commmand. See the tool's website for examples.</p> <p>For example, the following will download and run the workflow on all \"Paraburkholderia acidicola\" genomes available within GenBank.</p> shell <pre><code>nextflow run \\\n    socialgene/sgnf \\\n  --ncbi_genome_download_command 'bacteria --section genbank --genera \"Paraburkholderia acidicola\"' \\\n  ...\n  ...\n</code></pre> <p>Warning</p> <p>It is very easy to download a LOT of genomes/data with this tool. To get an idea of how many genomes a query might return, you can first do an interactive search of a taxon, etc, here: https://www.ncbi.nlm.nih.gov/datasets/genome</p>"},{"location":"nextflow/inputs/#ncbi-datasets","title":"NCBI datasets","text":"<p>NCBI has a new-ish command line tool for downloading genomes, called NCBI datasets. A command may be passed to <code>datasets download</code> by using the Nextflow pipeline <code>ncbi_datasets_command</code>.</p> <p>e.g. For all assemblies within the genus Micromonospora you could use: <code>ncbi_datasets_command = 'genome taxon \"micromonospora\"'</code> e.g. For the strain <code>Micromonospora sp. B006</code> you could use: <code>ncbi_datasets_command = 'genome accession GCF_003408515.1'</code></p> shell <pre><code>nextflow run \\\n    socialgene/sgnf \\\n  --ncbi_datasets_command 'genome taxon \"micromonospora\"' \\\n  ...\n  ...\n</code></pre> <p>Warning</p> <p>It is very easy to download a LOT of genomes/data with this tool. To get an idea of how many genomes a query might return, you can first do an interactive search of a taxon, etc, here: https://www.ncbi.nlm.nih.gov/datasets/genome</p> <p>Related to the above warning, the download step of the workflow may take some time depending on your internet speed and number of genomes to be downloaded.</p>"},{"location":"nextflow/inputs/#hmm-models","title":"HMM models","text":""},{"location":"nextflow/inputs/#prebuilt-models","title":"Prebuilt models","text":"<p>The Nextflow workflow is able to download HMM models from any or all of the following: [\"antismash\",\"amrfinder\",\"bigslice\",\"classiphage\", \"ipresto\",\"pfam\",\"prism\",\"resfams\",\"tigrfam\",\"virus_orthologous_groups\"].</p> <p>These can be selected by using the <code>hmmlist</code> parameter and comma-separated string:</p> <p>e.g. <code>--hmmlist 'resfams,antismash'</code></p> <p>You can also use <code>--hmmlist all</code> to use all models from all of the databases SocialGene knows about. </p> <p>Note</p> <p>Depending on your location/internet speed this step can take some time to download.</p> <p>Because they don't change, HMM models are downloaded and stored for long-term use between workflow runs to <code>--outdir_download_cache</code>. Where possible SocialGene pulls versioned HMM models. The versions used can be modified using workflow parameters found here.</p>"},{"location":"nextflow/inputs/#custom-models","title":"Custom models","text":"<p>To use your own HMM model use the parameter: <code>--custom_hmm_file</code></p> <p>e.g. <code>--custom_hmm_file '/path/to/my/hmm.hmm'</code></p> <p>The file should be a valid HMMER HMM model.</p> <p>For info on HMMs see: https://www.ebi.ac.uk/training/online/courses/pfam-creating-protein-families/</p>"},{"location":"nextflow/installation/","title":"Installation","text":""},{"location":"nextflow/installation/#install-docker","title":"Install Docker","text":"<p>Currently the Nextflow workflow requires the use of Docker, as does running the database.</p> <p>For instructions on installing Docker, see https://docs.docker.com/get-docker.</p>"},{"location":"nextflow/installation/#install-nextflownf-core","title":"Install Nextflow/nf-core","text":""},{"location":"nextflow/installation/#using-conda","title":"Using Conda","text":"<p>While not necessary, I recommend and find it simplest to work within Conda environments. If you don't have Conda installed,you can do that first using the instructions at https://docs.conda.io/en/latest/miniconda.html</p> <p>After installing Conda, use the following command to create a minimal environement with Nextflow and nf-core installed. </p> shell <pre><code>conda create --name nf \"bioconda::nextflow\" \"bioconda::nf-core\"\n</code></pre> <p>To activate and use the newly-created environment:</p> shell <pre><code>conda activate nf\n</code></pre>"},{"location":"nextflow/installation/#not-using-conda","title":"Not using Conda","text":"<ul> <li>See Nextflow's installation documentation</li> </ul>"},{"location":"nextflow/installation/#install-on-windows","title":"Install on Windows","text":"<p>Note: Untested and unsupported</p> <ul> <li>https://www.nextflow.io/blog/2021/setup-nextflow-on-windows.html</li> </ul>"},{"location":"nextflow/installation/#install-socialgene-nextflow-workflow","title":"Install SocialGene Nextflow workflow","text":""},{"location":"nextflow/installation/#using-nextflow","title":"Using Nextflow","text":"shell <pre><code>nextflow pull socialgene/sgnf\n</code></pre>"},{"location":"nextflow/installation/#using-nf-core","title":"Using nf-core","text":"shell <pre><code>nf-core download socialgene/sgnf\n</code></pre>"},{"location":"nextflow/installation/#install-socialgene-python-library-optional","title":"Install SocialGene Python Library (optional)","text":"shell <pre><code>pip install socialgene\n</code></pre>"},{"location":"nextflow/outputs/","title":"Outputs","text":"<ul> <li>When running the Nextflow workflow we provided a <code>--outdir \"my_results_directory\"</code> parameter. This outputs all results into the provided directory; note: if Nextflow can't find the directory it will create the entire filepath provided.</li> <li>Additionally, files that don't change between runs (e.g. downloaded HMM models) are stored for long term use into the path specified by <code>--outdir_download_cache</code>.</li> </ul> <p>Within the <code>--outdir</code>directory (show above as set to \"my_results_directory\") the workflow will create two directories <code>./socialgene_neo4j</code> and <code>./socialgene_neo4j</code>.</p> <pre><code>\u2523 \ud83d\udcc2 socialgene_per_run\n\u2517 \ud83d\udcc2 socialgene_neo4j\n</code></pre> <pre><code>\u2523 \ud83d\udcc2 socialgene_per_run (contains files that are specific to the Nextflow run but not included in the database)\n  \u2523 \ud83d\udcc2 antismash_results\n  \u2523 \ud83d\udcc2 blastp_cache (DIAMOND database)\n  \u2523 \ud83d\udcc2 hmm_cache (processed HMM files)\n  \u2523 \ud83d\udcc2 mmseqs_databases (MMseqs2 cluster database(s))\n  \u2523 \ud83d\udcc2 nonredundant_fasta (non-redundant, indexed protein fasta)\n  \u2523 \ud83d\udcc2 pipeline_info (stats about the workflow run)\n</code></pre> <pre><code>\u2523 \ud83d\udcc2 socialgene_neo4j\n  - contains all the files for import into the Neo4j Database, as well as the Neo4j Database\n  - To run the database, the Neo4j Docker image is pointed directly at the `\ud83d\udcc2 socialgene_neo4j` directory, the structure is important (Neo4j looks for each of the directories). If you delete one of the subdirectories (`data`, `import`, `logs`, `plugins`) and don't recreate it the Neo4j Docker image will make it again, but as root user, and you will only be able to delete if you have sudo permissions.\n  \u2523 \ud83d\udcc2 data (contains the Neo4j database)\n  \u2523 \ud83d\udcc2 import (contains gzipped tsv files that were used to import data into Neo4j)\n  \u2523 \ud83d\udcc2 logs (Neo4j runtime logs)\n  \u2523 \ud83d\udcc2 plugins (Neo4j plugins (apoc, gds, etc.))\n  \u2523 \ud83d\udcc4 import.report (contains report of all warnings and errors during database import creation)  \n</code></pre>"},{"location":"nextflow/parameters/","title":"Parameters galore!","text":"<p>Nextflow workflows tend to be parameter-heavy and this one is no different. The best and most up-to-date way to see the available parameters and their descriptions is to use Nextflow or nf-core directly</p>"},{"location":"nextflow/parameters/#using-nextflow-directly","title":"Using Nextflow directly","text":"<p>To print a short list of parameters available in the Nextflow workflow:</p> shell <pre><code>nextflow run socialgene/sgnf --help\n</code></pre> <p></p> <p>And to see all parameters:</p> shell <pre><code>nextflow run socialgene/sgnf --help --show_hidden_params\n</code></pre>"},{"location":"nextflow/parameters/#using-nf-core","title":"Using nf-core","text":"<p>By running <code>nf-core launch</code> but cancelling instead of launching a run.</p> shell <pre><code>nf-core launch socialgene/sgnf\n</code></pre> <p></p>"},{"location":"nextflow/running_the_pipeline/","title":"Running the Workflow","text":""},{"location":"nextflow/running_the_pipeline/#configuration","title":"Configuration","text":"<p>Nearly all of SocialGene's parameters are contained in one file <code>common_parameters.env</code>. (You may notice the file is seen in a couple of places but it's all the same file and modifications to any one <code>common_parameters.env</code> will modify all <code>common_parameters.env</code> files- the actual file is in <code>./socialgene/src/socialgene/common_parameters.env</code>) Settings specified here have effects in the Python package (and therefore Nextflow) and Django app.</p>"},{"location":"nextflow/running_the_pipeline/#creating-the-neo4j-database","title":"Creating the Neo4j Database","text":""},{"location":"nextflow/running_the_pipeline/#creating-a-nextflow-configuration-file","title":"Creating a Nextflow configuration file","text":"<p>The easiest way to get started is to use the example, basic Nextflow configuration file (it's located in: <code>~/nextflow/conf/examples/simple_run.config</code>)</p> <p>Open that file and take a look at the values within the <code>params{ ... }</code> block</p> <p>Change the memory and cpu values to fit your computer.</p>"},{"location":"nextflow/running_the_pipeline/#selecting-the-modules-you-want-to-use","title":"Selecting the \"modules\" you want to use","text":"<p>Currently you can choose between <code>hmms</code>, <code>mmseqs2</code>, or <code>blastp</code> for making protein-protein connections in the database</p> <p>MMseqs2 is quite fast and can be used for most data sizes. On the other hand, the all-vs-all Diamond Blastp option isn't super fast and will generate larger outputs (~40GB for 200 genomes), so only set it to true if you have a descent number of cpu cores and less than maybe a couple dozen input genomes.</p> <p><code>builddb</code> is set to <code>true</code> or <code>false</code> and tells determines whether the last step, building the Neo4j database, should be performed. If set to <code>false</code>, all data gathering and processing steps will still be performed, only the final database import step will be skipped.</p> <p>If you're using genomes from NCBI and <code>ncbi_taxonomy = true</code> then the NCBI taxonomy database will be downloaded, formatted and included in the graph database.</p>"},{"location":"nextflow/running_the_pipeline/#running-the-nextflow-pipeline","title":"Running the Nextflow pipeline","text":"<p>Set where you want all the files to write to (the <code>outdir</code> below), and run the Nextflow pipeline.</p> shell <pre><code>outdir=\"/home/chase/Documents/socialgene_data/micromonospora\"\n\nnextflow run nextflow \\\n    -profile simple_run,conda \\\n    --outdir_per_run $outdir/run_info \\\n    --outdir_neo4j \"$outdir/neo4j\" \\\n    --outdir_long_cache \"$outdir/long_cache\"  \\\n    -resume\n</code></pre> <ul> <li><code>outdir_per_run</code> will contain information about the run (timings, memory used, etc)</li> <li><code>outdir_neo4j</code> will contain both the neo4j database and the files used to create it/import</li> <li>This directory structure is required for import step when creating the neo4j database. If you won't need to \"resume\" the nextflow pipeline the <code>import</code> directory can be deleted</li> <li><code>outdir_long_cache</code> contains things like the HMM models which shouldn't vary between runs of the workflow.</li> </ul>"},{"location":"nextflow/running_the_pipeline/#making-things-faster-by-parallelizing","title":"Making things faster by parallelizing","text":"<p>Note: The PYHMMER annotation step will finish faster if the proteins are split and provided in parallel. To do this, append <code>--fasta_splits</code> to the Nextflow command above and set the number of splits to perform (e.g. <code>--fasta_splits 12</code>). At the bottom of the `simple_run.config file you'll notice:</p> <pre><code>process {\n withName:PYHMMER {\n        cpus   = { check_max( 2    * task.attempt, 'cpus'    ) }\n        memory = { check_max( 2 * task.attempt, 'memory'  ) }\n        time   = { check_max( 600.h  * task.attempt, 'time'    ) }\n    }\n}\n</code></pre> <p>A good rule of thumb is to set this process' cpus to <code>2</code> as shown here, and to set <code>--fasta_splits</code> to at least 1/2 the <code>max_cpus</code> argument. For large datasets setting this even higher will allow following progress of the task easier and provide more checkpoints if something fails along the way and the pipeline has to be restarted.</p>"},{"location":"nextflow/running_the_pipeline/#nextflow-pipeline-execution-time","title":"Nextflow Pipeline Execution Time","text":"<p>The length of time the pipeline takes relies heavily on the number of cores used (and possibly RAM (when using pyHMMER)), so estimates are difficult. On my work desktop (AMD\u00ae Ryzen 9 3900xt 12-core processor \u00d7 24 | 62.7 GiB RAM) a single genome will finish in a couple minutes (When starting from scratch, usually downloading PFAM is the longest step), and annotating all Micromonospora (~200 genomes) may take a couple of hours. On our lab's server (100 logical cores | 1 TB RAM ) using 40 logical cores, a couple thousand genomes ran through in just under 24 hours (though this was done while under concurrent heavy use by others).</p>"},{"location":"nextflow/running_the_pipeline/#nf-core-gui","title":"nf-core GUI","text":"<p>Begin by launching the workflow from the computer you want the coordinating Nextflow process to run.</p> <p><pre><code>nf-core launch socialgene/sgnf\n</code></pre> </p> <p>Selecting \"Command line\" will walk you through a somewhat onerous CLI-interface where you can set each of the parameters.</p> <p></p> <p>Alternatively, selecting \"Web based\" will open a web-based GUI where you can enter in parameters and then launch the workflow.</p> <p></p>"},{"location":"nextflow/running_the_pipeline/#nextflow-tower","title":"Nextflow Tower","text":"<p>The pipeline can also be run/managed via Nextflow Tower which provides the ability to launch in a number of compute environments but is beyond the scope of this documentation. See https://help.tower.nf/latest/#why-nextflow-tower for more information</p> <p> </p>"},{"location":"nextflow/tower/","title":"Tower","text":""},{"location":"nodes_and_relationships/about_the_database/","title":"Identifiers","text":""},{"location":"nodes_and_relationships/about_the_database/#uids","title":"UIDs","text":"<p>Most nodes have a \"uid\" property which stand for unique identifier. It is unique, but only within the same Node label; e.g. only one uid will be present across all \"protein\" nodes.</p>"},{"location":"nodes_and_relationships/about_the_database/#proteins","title":"Proteins","text":"<p>For proteins the uid is the sha512t24u hash of the protein sequence, this means proteins with identical amino acid sequences are stored only once and referenced by this uid.</p> <p>Protein nodes also have a CRC64 hash which can be used to cross-reference UniProt. The original protein ID from the input FASTA/Genome is found not on the redundant protein nodes but on the relationship between the node representing the nucleotide sequence the protein was found on, and the non-redundant protein.</p> <p>An example of retrieving an input protein's original ID:</p> Cypher <pre><code>MATCH ()-[e1:ENCODES]-&gt;(p1:protein {uid:\"bnI__-NwnuHiyLRDrRcUdrPyYyIjfONy\"}) \nRETURN e1.protein_id as protein_id, p1.uid as uid\nLIMIT 2\n</code></pre> <p>Results in:</p> protein_id protein_uid ALC45_RS09175 bnI__-NwnuHiyLRDrRcUdrPyYyIjfONy ALT75_RS12645 bnI__-NwnuHiyLRDrRcUdrPyYyIjfONy"},{"location":"nodes_and_relationships/about_the_database/#assemblies","title":"Assemblies","text":"<p>For assemblies  SocialGene tries to assign a uid in preference of:</p> <p>1) extract the <code>Assembly:</code> identifier from the <code>dbxrefs</code> section of a genbank file and use it as the uid</p> <p>2) use the name of the input file as the uid (assumption that a single file contains a single assembly/genome)</p> <p>3) assign a random string as the uid</p> <p>Note</p> <p>If a FASTA file was used as input the nucleotide and assembly node uids will be the FASTA file's name.</p>"},{"location":"nodes_and_relationships/about_the_database/#nucleotides","title":"Nucleotides","text":"<p>Nucleotide sequences (\"Locus\" in GBK files) are assigned a unique id by hashing the assembly uid they come from, and the external nucleotide identifier. See the example below. The original nucleotide identifier is stored on the nucleotide node as the property <code>external_id</code>. </p> Cypher <pre><code>MATCH (n1:nucleotide {external_id:\"NZ_CXED01000052.1\"})\nRETURN n1\nLIMIT 1\n</code></pre> <p>Returns the node and its properties:</p> <pre><code>{\n  \"identity\": 43109979,\n  \"labels\": [\n    \"nucleotide\"\n  ],\n  \"properties\": {\n    \"strain\": \"20051272_1361367\",\n    \"collection_date\": \"2005-01-01\",\n    \"country\": \"Egypt\",\n    \"db_xref\": \"taxon:624\",\n    \"uid\": \"zzGRoTd7WPwP8XCrEzdEmnbWtvJvCsap\",\n    \"serovar\": \"NA\",\n    \"external_id\": \"NZ_CXED01000052.1\",\n    \"mol_type\": \"genomic DNA\",\n    \"isolation_source\": \"feces\"\n  },\n  \"elementId\": \"43109979\"\n}\n</code></pre> <p>In this case the uid of the nucleotide node is <code>zzGRoTd7WPwP8XCrEzdEmnbWtvJvCsap</code>, which is the hash of assembly uid \"GCF_001246015.1\" and the nucleotide external_id \"NZ_CXED01000052.1\".</p> <p>This can be verified in the SocialGene python library:</p> Python <pre><code>from socialgene.hashing.hashing import hasher\nhasher(\"GCF_001246015.1___NZ_CXED01000052.1\")\n\n# 'zzGRoTd7WPwP8XCrEzdEmnbWtvJvCsap'\n</code></pre> <p>Note</p> <p>If a FASTA file was used as input the nucleotide and assembly node uids will be the FASTA file's name.</p>"},{"location":"nodes_and_relationships/schema/","title":"Schema","text":"<p>The schema below is auto-generatated via the SocialGene Python Package.</p> <pre><code># pip install socialgene\nsg_schema --nodes  --markdown\nsg_schema --rels  --markdown\n</code></pre>"},{"location":"nodes_and_relationships/schema/#nodes","title":"Nodes","text":"Label Description NF results subdirectory Neo4j header file Unique on properties assembly Represents a single genome/assembly/BGC. If the input was a FASTA file or if assembly wasn't in the genbank metadata then this will represent the file the data came from. genomic_info assembly.header uid ['uid', 'altitude', 'bio_material', 'bioproject', 'biosample', 'cell_line', 'cell_type', 'chromosome', 'clone', 'clone_lib', 'collected_by', 'collection_date', 'country', 'cultivar', 'culture_collection', 'db_xref', 'dev_stage', 'ecotype', 'environmental_sample', 'focus', 'germline', 'haplogroup', 'haplotype', 'host', 'identified_by', 'isolate', 'isolation_source', 'lab_host', 'lat_lon', 'macronuclear', 'map', 'mating_type', 'metagenome_source', 'mol_type', 'note', 'organelle', 'organism', 'pcr_primers', 'plasmid', 'pop_variant', 'proviral', 'rearranged', 'segment', 'serotype', 'serovar', 'sex', 'specimen_voucher', 'strain', 'sub_clone', 'submitter_seqid', 'sub_species', 'sub_strain', 'tissue_lib', 'tissue_type', 'transgenic', 'type_material', 'variety'] assembly:mibig Represents a single Mibig entry genomic_info assembly.header uid ['uid'] chebi Represents a ChEBI term None None uid ['uid', 'name'] chembl Represents a CHEMBL term None None uid ['uid'] chemical_compound:substrate Mibig substrate (e.g. NRPS monomer) None None inchi, CanonicalSmiles ['uid', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'HeavyAtomCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'AnonymousGraph', 'ElementGraph', 'MurckoScaffold', 'ExtendedMurcko', 'MolFormula', 'AtomBondCounts', 'DegreeVector', 'Mesomer', 'HetAtomTautomer', 'HetAtomProtomer', 'RedoxPair', 'Regioisomer', 'NetCharge', 'SmallWorldIndexBR', 'SmallWorldIndexBRL', 'ArthorSubstructureOrder', 'HetAtomTautomerv2', 'inchi', 'CanonicalSmiles'] chemical_compound Represents a chemical compound None None inchi, CanonicalSmiles ['uid', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'HeavyAtomCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'AnonymousGraph', 'ElementGraph', 'MurckoScaffold', 'ExtendedMurcko', 'MolFormula', 'AtomBondCounts', 'DegreeVector', 'Mesomer', 'HetAtomTautomer', 'HetAtomProtomer', 'RedoxPair', 'Regioisomer', 'NetCharge', 'SmallWorldIndexBR', 'SmallWorldIndexBRL', 'ArthorSubstructureOrder', 'HetAtomTautomerv2', 'inchi', 'CanonicalSmiles'] classyfire Represents a classyfire chemical ontology term None None uid ['uid', 'name', 'definition'] gnps_cluster Represents a GNPS molecular networking cluster None None cluster_index, workflow_uuid, task ['task', 'defaultgroups', 'g1', 'g2', 'g3', 'g4', 'g5', 'g6', 'gnpslinkout_cluster', 'gnpslinkout_network', 'mqscore', 'mzerrorppm', 'massdiff', 'rtmean', 'rtmean_min', 'rtstderr', 'uniquefilesources', 'uniquefilesourcescount', 'cluster_index', 'componentindex', 'number_of_spectra', 'parent_mass', 'precursor_charge', 'precursor_mass', 'sumprecursor_intensity', 'workflow_uuid'] gnps_library_spectrum Represents a GNPS library spectrum None None uid ['uid', 'compound_name', 'compound_source', 'pi', 'data_collector', 'adduct', 'precursor_mz', 'exactmass', 'charge', 'cas_number', 'pubmed_id', 'smiles', 'inchi', 'inchi_aux', 'library_class', 'ionmode', 'libraryqualitystring', 'mqscore', 'tic_query', 'rt_query', 'mzerrorppm', 'sharedpeaks', 'massdiff', 'libmz', 'specmz', 'speccharge', 'moleculeexplorerdatasets', 'moleculeexplorerfiles', 'molecular_formula', 'inchikey', 'inchikey_planar'] gnps_organism Represents an organism (as defined by GNPS) None None uid ['uid'] goterm Represent a GO term goterms goterms.header uid ['uid', 'name', 'namespace'] hmm Represents a single non-redundant HMM model hmm_info sg_hmm_nodes.header uid ['uid'] hmm_source Represents the source of an HMM model (e.g. PFAM) hmm_info hmm_source.header uid ['uid', ':LABEL', 'rel_path', 'name', 'acc', 'notes', 'description', 'date', 'hash', 'hash_used', 'model_length', 'super_category', 'category', 'subcategory', 'ga', 'tc', 'nc'] instrument Represents an instrument None None uid ['uid'] ion_source Represents an ion source None None uid ['uid'] mass_spectrum_file Represents a GNPS molecular networking spectrum file None None original_filename, workflow_uuid ['filename', 'gnps_filename', 'workflow_uuid'] mibig_activity Represents a single Mibig bioactivity None None uid ['uid'] mibig_biosynthetic_class Represents a single Mibig biosynthetic class None None uid ['uid'] mibig_compound Represents a single Mibig compound None None uid ['name', 'smiles', 'inchi'] ms2_spectrum Represents a GNPS molecular networking spectrum None None original_filename, specidx, workflow_uuid ['specidx', 'original_filename', 'parentmass', 'charge', 'rettime', 'workflow_uuid'] npatlas Represents a single NPAtlas entry None None uid ['uid', 'original_name', 'mol_formula', 'mol_weight', 'exact_mass', 'inchikey', 'smiles', 'cluster_id', 'node_id', 'synonyms', 'inchi', 'm_plus_h', 'm_plus_na', 'genus', 'species'] npclassifier_class Represents a NPClassifier class None None uid ['uid'] npclassifier_pathway Represents a NPClassifier pathway None None uid ['uid'] npclassifier_superclass Represents a NPClassifier superclass None None uid ['uid'] npmrd Represents a single NP-MRD entry None None uid ['uid'] nucleotide Represents a single nucleotide sequence (e.g. a contig/scaffold/chromosome) genomic_info locus.header uid ['uid', 'external_id', 'altitude', 'bio_material', 'bioproject', 'biosample', 'cell_line', 'cell_type', 'chromosome', 'clone', 'clone_lib', 'collected_by', 'collection_date', 'country', 'cultivar', 'culture_collection', 'db_xref', 'dev_stage', 'ecotype', 'environmental_sample', 'focus', 'germline', 'haplogroup', 'haplotype', 'host', 'identified_by', 'isolate', 'isolation_source', 'lab_host', 'lat_lon', 'macronuclear', 'map', 'mating_type', 'metagenome_source', 'mol_type', 'note', 'organelle', 'organism', 'pcr_primers', 'plasmid', 'pop_variant', 'proviral', 'rearranged', 'segment', 'serotype', 'serovar', 'sex', 'specimen_voucher', 'strain', 'sub_clone', 'submitter_seqid', 'sub_species', 'sub_strain', 'tissue_lib', 'tissue_type', 'transgenic', 'type_material', 'variety'] parameters Parameters and environmental variables used during database creation parameters parameters.header uid ['uid', 'SG_LOC_NEO4J', 'SG_LOC_HMMS', 'NEO4J_dbms_memory_pagecache_size', 'NEO4J_dbms_memory_heap_initial__size', 'NEO4J_dbms_memory_heap_max__size', 'HMMSEARCH_IEVALUE', 'HMMSEARCH_BACKGROUND', 'HMMSEARCH_BIASFILTER', 'HMMSEARCH_NULL2', 'HMMSEARCH_SEED', 'HMMSEARCH_Z', 'HMMSEARCH_DOMZ', 'HMMSEARCH_F1', 'HMMSEARCH_F2', 'HMMSEARCH_F3', 'HMMSEARCH_E', 'HMMSEARCH_DOME', 'HMMSEARCH_INCE', 'HMMSEARCH_INCDOME', 'HMMSEARCH_BITCUTOFFS', 'platform', 'architecture', 'py_executable', 'py_version', 'genome_download_command'] protein Represents a non-redundant protein protein_info protein_ids.header uid ['uid', 'crc64', 'sequence'] publication Represents a publication None None doi ['doi', 'pmid', 'authors', 'title', 'journal', 'year'] publication Represents a publication None None doi ['doi', 'pmid', 'authors', 'title', 'journal', 'year'] substructure Represents a chemical substructure None None inchi, CanonicalSmiles ['uid', 'inchi', 'CanonicalSmiles'] taxid Represents a single taxon within NCBI taxonomy taxdump_process taxid.header uid ['uid', 'name', 'rank'] tigrfam_mainrole Represents a TIGRFAM main role tigrfam_info tigrfam_mainrole.header uid ['uid'] tigrfam_role Represents a TIGRFAM role tigrfam_info tigrfam_role.header uid ['uid'] tigrfam_subrole Represents a TIGRFAM sub role tigrfam_info tigrfam_subrole.header uid ['uid']"},{"location":"nodes_and_relationships/schema/#relationships","title":"Relationships","text":"Label Relationship NF results subdirectory Neo4j header file ANNOTATES (:hmm)-[:ANNOTATES]-&gt;(:protein) parsed_domtblout protein_to_hmm_header.header ANALYSIS_OF (:mass_spectrum_file)-[:ANALYSIS_OF]-&gt;(:assembly) None None ALTERNATIVE_PARENTS (:npatlas)-[:ALTERNATIVE_PARENTS]-&gt;(:classyfire) None None ASSEMBLES_TO (:nucleotide)-[:ASSEMBLES_TO]-&gt;(:assembly) genomic_info assembly_to_locus.header BLASTP (:protein)-[:BLASTP]-&gt;(:protein) diamond_blastp blastp.header CLUSTERS_TO (:ms2_spectrum)-[:CLUSTERS_TO]-&gt;(:gnps_cluster) None None DIRECT_PARENT (:npatlas)-[:DIRECT_PARENT]-&gt;(:classyfire) None None ENCODES (:nucleotide)-[:ENCODES]-&gt;(:protein) genomic_info locus_to_protein.header FROM (:gnps_library_spectrum)-[:FROM]-&gt;(:gnps_organism) None None FROM (:gnps_library_spectrum)-[:FROM]-&gt;(:instrument) None None FROM (:gnps_library_spectrum)-[:FROM]-&gt;(:ion_source) None None GOTERM_RELS (:goterm)-[:GOTERM_RELS]-&gt;(:goterm) goterms go_to_go.header GO_ANN (:hmm_source)-[:GO_ANN]-&gt;(:goterm) tigrfam_info tigrfam_to_go.header HAS (:npatlas)-[:HAS]-&gt;(:gnps_library_spectrum) None None HAS (:mass_spectrum_file)-[:HAS]-&gt;(:ms2_spectrum) None None HAS (:npatlas)-[:HAS]-&gt;(:npmrd) None None HAS (:npatlas)-[:HAS]-&gt;(:publication) None None IS_A (:gnps_library_spectrum)-[:IS_A]-&gt;(:chemical_compound) None None IS_A (:npatlas)-[:IS_A]-&gt;(:chebi) None None IS_A (:npatlas)-[:IS_A]-&gt;(:chemical_compound) None None IS_A (:npatlas)-[:IS_A]-&gt;(:npclassifier_superclass) None None IS_A (:npatlas)-[:IS_A]-&gt;(:npclassifier_pathway) None None IS_A (:npatlas)-[:IS_A]-&gt;(:npclassifier_class) None None IS_A (:gnps_library_spectrum)-[:IS_A]-&gt;(:npclassifier_pathway) None None IS_TAXON (:assembly)-[:IS_TAXON]-&gt;(:taxid) genomic_info assembly_to_taxid.header IS_A (:classyfire)-[:IS_A]-&gt;(:classyfire) None None IS_A (:gnps_library_spectrum)-[:IS_A]-&gt;(:npclassifier_superclass) None None INTERMEDIATE_NODES (:npatlas)-[:INTERMEDIATE_NODES]-&gt;(:classyfire) None None IS_A (:gnps_library_spectrum)-[:IS_A]-&gt;(:npclassifier_class) None None LOWEST_CLASS (:npatlas)-[:LOWEST_CLASS]-&gt;(:classyfire) None None LIBRARY_HIT (:gnps_cluster)-[:LIBRARY_HIT]-&gt;(:gnps_library_spectrum) None None MAINROLE_ANN (:tigrfam_role)-[:MAINROLE_ANN]-&gt;(:tigrfam_mainrole) tigrfam_info tigrfamrole_to_mainrole.header MOLECULAR_NETWORK (:gnps_cluster)-[:MOLECULAR_NETWORK]-&gt;(:gnps_cluster) None None MCS_SIMILARITY (:chemical_compound)-[:MCS_SIMILARITY]-&gt;(:chemical_compound) None None MMSEQS2 (:protein)-[:MMSEQS2]-&gt;(:protein) mmseqs2_cluster mmseqs2.header PRODUCES (:taxid)-[:PRODUCES]-&gt;(:npatlas) None None PROTEIN_TO_GO (:protein)-[:PROTEIN_TO_GO]-&gt;(:goterm) protein_info protein_to_go.header PRODUCES (:assembly:mibig)-[:PRODUCES]-&gt;(:mibig_compound) None None PRODUCES (:assembly:mibig)-[:PRODUCES]-&gt;(:npatlas) None None ROLE_ANN (:hmm_source)-[:ROLE_ANN]-&gt;(:tigrfam_role) tigrfam_info tigrfam_to_role.header SUBSTRUCTURE (:chemical_compound)-[:SUBSTRUCTURE]-&gt;(:substructure) None None SOURCE_DB (:hmm)-[:SOURCE_DB]-&gt;(:hmm_source) hmm_info hmm_source_relationships.header SYNONYM (:classyfire)-[:SYNONYM]-&gt;(:chebi) None None SUBROLE_ANN (:tigrfam_role)-[:SUBROLE_ANN]-&gt;(:tigrfam_subrole) tigrfam_info tigrfamrole_to_subrole.header TAXON_PARENT (:taxid)-[:TAXON_PARENT]-&gt;(:taxid) taxdump_process taxid_to_taxid.header TANIMOTO_SIMILARITY (:chemical_compound)-[:TANIMOTO_SIMILARITY]-&gt;(:chemical_compound) None None"},{"location":"precomputed_databases/2023_v0.4.1/general/","title":"Precomputed databases, version: 2023_v0.4.1","text":""},{"location":"precomputed_databases/2023_v0.4.1/general/#introduction","title":"Introduction","text":"<p>In addition to the &gt;340,000 genome RefSeq database, I have precomputed several smaller databases I thought the natural product community (and others) may find helpful.</p> <p>The version of the databases is <code>2023_v0.4.1</code> because the databases were built in 2023 using version 0.4.1 of the SocialGene Nextflow workflow.</p> <p>The databases were built using the same set of less-redundant HMM models. Which means you can use the single set of provided HMM files with any of these databases.</p>"},{"location":"precomputed_databases/2023_v0.4.1/general/#databases-and-their-disk-space-requirements","title":"Databases and their disk space requirements","text":"<p>Note</p> <p>Some of the databases are quite large so please read the hardware requirements before downloading.</p> <p>You will need at least enough disk space to accommodate the dump file and rehydrated database. The dump file is not required after rehydrating (can be deleted) but if you have enough space might it be good to keep around in case you want to rebuild the database from scratch without downloading again (e.g. you make a lot of custom database modifications but mess up and want to start over).</p> <ul> <li> <p>All of RefSeq</p> <ul> <li>Database dump file: ~220 GB</li> <li>Rehydrated database: ~650 GB</li> <li>Rehydrated database with indexes: ~663 GB</li> <li>343,381 genomes + MIBiG BGCs</li> </ul> </li> <li> <p>All RefSeq antiSMASH-7.0 BGCs</p> <ul> <li>Database dump file is ~10 GB</li> <li>Rehydrated database is ~29 GB</li> <li>2,105,746 BGCs from 307,469 genomes + MIBiG BGCs </li> </ul> </li> <li> <p>All RefSeq Actinomycetota</p> <ul> <li>Database dump file is ~30 GB</li> <li>Rehydrated database is ~86 GB</li> <li>29,479 genomes + MIBiG BGCs</li> </ul> </li> <li> <p>All RefSeq Streptomyces</p> <ul> <li>Database dump file is ~8 GB</li> <li>Rehydrated database is ~23 GB</li> <li>3,087 genomes + MIBiG BGCs</li> </ul> </li> <li> <p>All RefSeq Micromonospora</p> <ul> <li>Database dump file is ~1 GB</li> <li>Rehydrated database is ~3 GB</li> <li>314 genomes + MIBiG BGCs</li> </ul> </li> <li> <p>Three genomes used for protein similarity method comparisons</p> <ul> <li>Database dump file is ~99 MB</li> <li>Rehydrated database is ~744 MB</li> <li>3 genomes<ul> <li>GCF_000009045.1, GCF_000005845.2, GCF_008931305.1 (Bacillus subtilis subsp. subtilis str. 168, Escherichia coli str. K-12 substr. MG1655, and Streptomyces coelicolor A3(2); downloaded on June 20, 2024)</li> </ul> </li> </ul> </li> </ul> <p>All of the above databases were built with the same set of HMM models, these are found in the two files:</p> <ul> <li>socialgene_nr_hmms_file_without_cutoffs_1_of_1.hmm.gz</li> <li>socialgene_nr_hmms_file_with_cutoffs_1_of_1.hmm.gz</li> </ul>"},{"location":"precomputed_databases/2023_v0.4.1/general/#nextflow-workflow-parameters-used-in-building-the-databases","title":"Nextflow workflow parameters used in building the databases","text":"<ul> <li>All RefSeq <ul> <li>params_refseq.json</li> </ul> </li> <li>All RefSeq Actinomycetota<ul> <li>params_actinomycetota.json</li> </ul> </li> <li>All RefSeq Streptomyces<ul> <li>params_streptomyces.json</li> </ul> </li> <li>All RefSeq Micromonospora<ul> <li>params_micromonospora.json</li> </ul> </li> <li>All RefSeq antiSMASH-7.0 BGCs<ul> <li>params_refseq_antismash_bgcs.json</li> </ul> </li> <li>Three genomes used for protein similarity method comparisons<ul> <li>params_methods_comparison.json</li> </ul> </li> </ul>"},{"location":"precomputed_databases/2023_v0.4.1/general/#instructions-for-downloading-data-from","title":"Instructions for downloading data from...","text":"<ul> <li>Dryad</li> <li>AWS S3</li> </ul>"},{"location":"precomputed_databases/2023_v0.4.1/general/#access-information","title":"Access information","text":"<p>Data was derived from the following sources:</p> <ul> <li> <p>RefSeq https://www.ncbi.nlm.nih.gov/home/about/policies (opens in new window)</p> </li> <li> <p>Resfams http://www.dantaslab.org/resfams (opens in new window)</p> </li> <li> <p>antiSMASH (Many of the HMM models are from PFAM)</p> <ul> <li>GNU Affero General Public License v3.0</li> <li>https://github.com/antismash/antismash/blob/e2d777c6cd035e6bf20f7eec924a350b00b84c7b/LICENSE.txt (opens in new window)</li> </ul> </li> <li> <p>AMRFinder</p> <ul> <li>Public domain</li> <li>https://github.com/ncbi/amr/blob/c5fe35453ee102dcb2e44d6b1198992b1ab7a355/LICENSE (opens in new window)</li> </ul> </li> <li> <p>PFAM</p> <ul> <li>CC0 1.0 License</li> <li>https://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam35.0/relnotes.txt (opens in new window)</li> </ul> </li> <li> <p>resfams</p> <ul> <li>MIT License</li> <li>http://www.dantaslab.org/s/LICENSE.txt (opens in new window)</li> </ul> </li> <li> <p>TIGRFAM</p> <ul> <li>Creative Commons Attribution-ShareAlike 4.0</li> <li>https://www.ncbi.nlm.nih.gov/refseq/annotation_prok/tigrfams</li> </ul> </li> </ul>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/","title":"Precomputed Databases hosted on AWS S3","text":"<p>There is an additional tutorial here that explains how to download and use the databases on an AWS EC2 instance.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/#instructions-for-download-and-use","title":"Instructions for download and use","text":"<p>Install the latest version of the AWS CLI using the instructions on the AWS CLI website.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/#using-neo4j-community-edition","title":"Using Neo4j Community Edition","text":"<p>First, download one of the database dump files from the AWS S3 bucket (files listed below). For example, to download the Micromonospora database dump file to the current directory:</p> <pre><code>aws s3 cp s3://socialgene-open-data/2023_v0.4.1/micromonospora/neo4j_db_micromonospora_base.dump .\n</code></pre> <p>Then follow the instructions for restoring from a full database dump/backup to rehydrate the Neo4j database. Note that the instructions use Docker, so you will need to have Docker installed or manually install Neo4j.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/#using-neo4j-enterprise-edition","title":"Using Neo4j Enterprise Edition","text":"<p>Ensure you have a valid license for Neo4j Enterprise Edition. Install the latest version of the Neo4j Enterprise Edition using the instructions on the Neo4j website.</p> <p>Follow the directions at https://neo4j.com/docs/operations-manual/current/backup-restore/restore-backup/#restore-cloud-storage for restoring a database directly from cloud storage.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/#files-explanation","title":"Files Explanation","text":""},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/#all-files","title":"All files","text":"<p>The following files are included in the AWS S3 bucket:</p> <pre><code>.\n\u2514\u2500\u2500 2023_v0.4.1\n    \u251c\u2500\u2500 actinomycetota\n    \u2502   \u251c\u2500\u2500 neo4j_db_actinomycetota_base.dump\n    \u2502   \u2514\u2500\u2500 run_info\n    \u2502       \u251c\u2500\u2500 execution_report_2023-12-02_06-51-28.html\n    \u2502       \u251c\u2500\u2500 execution_timeline_2023-12-02_06-51-28.html\n    \u2502       \u251c\u2500\u2500 execution_trace_2023-12-02_06-51-28.txt\n    \u2502       \u251c\u2500\u2500 params_2023-12-02_15-33-12.json\n    \u2502       \u2514\u2500\u2500 pipeline_dag_2023-12-02_06-51-28.html\n    \u251c\u2500\u2500 documentation\n    \u2502   \u251c\u2500\u2500 structure.md\n    \u2502   \u2514\u2500\u2500 summary.md\n    \u251c\u2500\u2500 hmm_models\n    \u2502   \u251c\u2500\u2500 hmminfo.gz\n    \u2502   \u251c\u2500\u2500 socialgene_nr_hmms_file_with_cutoffs_1_of_1.hmm.gz\n    \u2502   \u2514\u2500\u2500 socialgene_nr_hmms_file_without_cutoffs_1_of_1.hmm.gz\n    \u251c\u2500\u2500 md5checksums.txt\n    \u251c\u2500\u2500 methods_comparison\n    \u2502   \u251c\u2500\u2500 methods_comparison.dump\n    \u2502   \u2514\u2500\u2500 run_info\n    \u2502       \u251c\u2500\u2500 execution_report_2024-06-20_14-56-41.html\n    \u2502       \u251c\u2500\u2500 execution_timeline_2024-06-20_14-56-41.html\n    \u2502       \u251c\u2500\u2500 execution_trace_2024-06-20_14-56-41.txt\n    \u2502       \u251c\u2500\u2500 params_2024-06-20_15-10-51.json\n    \u2502       \u2514\u2500\u2500 pipeline_dag_2024-06-20_14-56-41.html\n    \u251c\u2500\u2500 micromonospora\n    \u2502   \u251c\u2500\u2500 neo4j_db_micromonospora_base.dump\n    \u2502   \u2514\u2500\u2500 run_info\n    \u2502       \u251c\u2500\u2500 execution_report_2023-12-03_11-49-37.html\n    \u2502       \u251c\u2500\u2500 execution_timeline_2023-12-03_11-49-37.html\n    \u2502       \u251c\u2500\u2500 execution_trace_2023-12-03_11-49-37.txt\n    \u2502       \u251c\u2500\u2500 params_2023-12-03_16-54-34.json\n    \u2502       \u2514\u2500\u2500 pipeline_dag_2023-12-03_11-49-37.html\n    \u251c\u2500\u2500 refseq\n    \u2502   \u251c\u2500\u2500 command_to_build_neo4j_database_with_docker.sh\n    \u2502   \u251c\u2500\u2500 import\n    \u2502   \u2502   \u251c\u2500\u2500 antismash_results.jsonl.gz\n    \u2502   \u2502   \u251c\u2500\u2500 genomic_info\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 2687a0b8f048c5081fbfe919b52c1727.assemblies.gz\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 51f1cf08d20aa569b0822d6c2cf859c9.assembly_to_taxid.gz\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 8bb70ed3e5f7ee8f8d740f2184207c19.locus_to_protein.gz\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 b3ed6e17dba5be04143622d89f77e7dd.loci.gz\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 d29bdf974a8769a329af5cc5dc5f91c6.assembly_to_locus.gz\n    \u2502   \u2502   \u251c\u2500\u2500 goterms\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 2068e8e87a576280156e1ec92161d019.goterm_edgelist\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 e4916a1a9abc084c587c6172f7509118.goterms\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 versions.yml\n    \u2502   \u2502   \u251c\u2500\u2500 hmm_info\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 20263cb059a54d1c773a2e7a23b2c073.sg_hmm_nodes\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 527acf97d838e23ff39ae8df6d8261a2.all.hmminfo\n    \u2502   \u2502   \u251c\u2500\u2500 mmseqs2_cluster\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 4e75f1c51535471c3225a8bd78dd2c32.mmseqs2_results_cluster.tsv.gz\n    \u2502   \u2502   \u251c\u2500\u2500 neo4j_headers\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 assembly.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 assembly_to_locus.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 assembly_to_taxid.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 go_to_go.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 goterms.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 hmm_source.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 hmm_source_relationships.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 locus.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 locus_to_protein.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 mmseqs2.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 parameters.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 protein_ids.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 protein_to_go.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 protein_to_hmm_header.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 sg_hmm_nodes.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 taxid.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 taxid_to_taxid.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 tigrfam_mainrole.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 tigrfam_role.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 tigrfam_subrole.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 tigrfam_to_go.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 tigrfam_to_role.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 tigrfamrole_to_mainrole.header\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 tigrfamrole_to_subrole.header\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 versions.yml\n    \u2502   \u2502   \u251c\u2500\u2500 parameters\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 d89feb1a1348b51bb4bcf295af700f51.socialgene_parameters.gz\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 versions.yml\n    \u2502   \u2502   \u251c\u2500\u2500 parsed_domtblout\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 40dcdeb59968818c8ef3fffa35971947.parseddomtblout.gz\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 versions.yml\n    \u2502   \u2502   \u251c\u2500\u2500 protein_info\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 c04b2d3997942e7cb5ac7c292aa73afb.protein_to_go.gz\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 ecb0e13c7f82cc39004f9e318bdecd98.protein_ids.gz\n    \u2502   \u2502   \u251c\u2500\u2500 taxdump_process\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 842f6c6514f6c81e4ca6a30ce7ec9772.nodes_taxid.gz\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 e1973763d9fb63342e7169968a572b7c.taxid_to_taxid.gz\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 versions.yml\n    \u2502   \u2502   \u2514\u2500\u2500 tigrfam_info\n    \u2502   \u2502       \u251c\u2500\u2500 2deedec3965b082a1536f2a7612820d7.tigrfamrole_to_mainrole.gz\n    \u2502   \u2502       \u251c\u2500\u2500 3a5edc8721c146058e104678b250fff2.tigrfam_subrole.gz\n    \u2502   \u2502       \u251c\u2500\u2500 4da23e7a3d5bb06e3cf41d8a398aeb99.tigrfam_to_go.gz\n    \u2502   \u2502       \u251c\u2500\u2500 5ad847d09afb9716bc3c155cba2f89f3.tigrfam_mainrole.gz\n    \u2502   \u2502       \u251c\u2500\u2500 93ad0a4066afbbf2ceb20a21a73e7178.tigrfam_role.gz\n    \u2502   \u2502       \u251c\u2500\u2500 a0d6530f10e4593fd79ba286a407ac90.tigrfamrole_to_subrole.gz\n    \u2502   \u2502       \u251c\u2500\u2500 fc03559179b378cda7d37ba580601588.tigrfam_to_role.gz\n    \u2502   \u2502       \u2514\u2500\u2500 versions.yml\n    \u2502   \u251c\u2500\u2500 neo4j_db_refseq_base.dump\n    \u2502   \u2514\u2500\u2500 run_info\n    \u2502       \u251c\u2500\u2500 params_2023-11-30_18-13-52.json\n    \u2502       \u2514\u2500\u2500 pipeline_dag_2023-12-01_13-24-10.html\n    \u251c\u2500\u2500 refseq_antismash_bgcs\n    \u2502   \u251c\u2500\u2500 neo4j_db_refseq_antismash_bgcs_base.dump\n    \u2502   \u2514\u2500\u2500 run_info\n    \u2502       \u251c\u2500\u2500 execution_report_2023-12-11_15-30-04.html\n    \u2502       \u251c\u2500\u2500 execution_timeline_2023-12-11_15-30-04.html\n    \u2502       \u251c\u2500\u2500 execution_trace_2023-12-11_15-30-04.txt\n    \u2502       \u251c\u2500\u2500 params_2023-12-12_06-00-22.json\n    \u2502       \u2514\u2500\u2500 pipeline_dag_2023-12-11_15-30-04.html\n    \u2514\u2500\u2500 streptomyces\n        \u251c\u2500\u2500 neo4j_db_streptomyces_base.dump\n        \u2514\u2500\u2500 run_info\n            \u251c\u2500\u2500 execution_report_2023-12-02_18-46-19.html\n            \u251c\u2500\u2500 execution_timeline_2023-12-02_18-46-19.html\n            \u251c\u2500\u2500 execution_trace_2023-12-02_18-46-19.txt\n            \u251c\u2500\u2500 params_2023-12-03_15-35-49.json\n            \u2514\u2500\u2500 pipeline_dag_2023-12-02_18-46-19.html\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/#database-files","title":"Database files","text":"<p>The included database dumps and disk space requirements are described in precomputed_databases/2023_v0.4.1/general.</p> <p>The paths to just the dumps are:</p> <ul> <li>All RefSeq:<ul> <li><code>s3://socialgene-open-data/2023_v0.4.1/refseq/neo4j_db_refseq_base.dump</code></li> </ul> </li> <li>All RefSeq Actinomycetota:<ul> <li><code>s3://socialgene-open-data/2023_v0.4.1/actinomycetota/neo4j_db_actinomycetota_base.dump</code></li> </ul> </li> <li>All RefSeq Streptomyces:<ul> <li><code>s3://socialgene-open-data/2023_v0.4.1/streptomyces/neo4j_db_streptomyces_base.dump</code></li> </ul> </li> <li>All RefSeq Micromonospora:<ul> <li><code>s3://socialgene-open-data/2023_v0.4.1/micromonospora/neo4j_db_micromonospora_base.dump</code></li> </ul> </li> <li>All RefSeq antiSMASH-7.0 BGCs:<ul> <li><code>s3://socialgene-open-data/2023_v0.4.1/refseq_antismash_bgcs/neo4j_db_refseq_antismash_bgcs_base.dump</code></li> </ul> </li> <li>Three genomes used for protein similarity method comparisons<ul> <li><code>s3://socialgene-open-data/2023_v0.4.1/methods_comparison/methods_comparison.dump</code></li> </ul> </li> </ul>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/#hmm-models","title":"HMM models","text":"<p>The files in <code>2023_v0.4.1/hmm_models</code> are the less-redundant HMM models used to annotate proteins in each of the <code>2023_v0.4.1</code> databases. Therefore, the <code>socialgene_nr_hmms_file_with_cutoffs_1_of_1.hmm.gz</code> and <code>socialgene_nr_hmms_file_without_cutoffs_1_of_1.hmm.gz</code> files are the same for all of the databases. For functions like the SocialGene BGC search, these files are required. The <code>hmminfo.gz</code> file is a gzipped file containing the metadata for the less redundant HMM models.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws/#flat-files","title":"Flat files","text":"<p>The TSV flat files included in the <code>2023_v0.4.1/refseq/import</code> directory may be useful for building custom databases (including non-Neo4j databases) or other analyses. The associations of individual flat files to their column header files are in the tables below. All of the flat files are gzip compressed even if the <code>.gz</code> extension is not present in the filename.</p> <p>The paths in the table below start with the <code>import</code> directory which is located in the <code>refseq</code> database directory (<code>2023_v0.4.1/refseq/import</code>).</p> neo4j_type neo4j_label neo4j_header_path flat_file_path node tigrfam_mainrole import/neo4j_headers/tigrfam_mainrole.header import/tigrfam_info/*.tigrfam_mainrole.* node tigrfam_subrole import/neo4j_headers/tigrfam_subrole.header import/tigrfam_info/*.tigrfam_subrole.* node parameters import/neo4j_headers/parameters.header import/parameters/*.socialgene_parameters.* node hmm import/neo4j_headers/sg_hmm_nodes.header import/hmm_info/*.sg_hmm_nodes.* node assembly import/neo4j_headers/assembly.header import/genomic_info/*.assemblies.* node hmm_source import/neo4j_headers/hmm_source.header import/hmm_info/*.hmminfo.* node tigrfam_role import/neo4j_headers/tigrfam_role.header import/tigrfam_info/*.tigrfam_role.* node goterm import/neo4j_headers/goterms.header import/goterms/*.goterms.* node protein import/neo4j_headers/protein_ids.header import/protein_info/*.protein_ids.* node taxid import/neo4j_headers/taxid.header import/taxdump_process/*.nodes_taxid.* node nucleotide import/neo4j_headers/locus.header import/genomic_info/*.loci.* neo4j_type neo4j_label neo4j_header_path flat_file_path relationship GO_ANN import/neo4j_headers/tigrfam_to_go.header import/tigrfam_info/*.tigrfam_to_go.* relationship SUBROLE_ANN import/neo4j_headers/tigrfamrole_to_subrole.header import/tigrfam_info/*.tigrfamrole_to_subrole.* relationship MMSEQS2 import/neo4j_headers/mmseqs2.header import/mmseqs2_cluster/*.mmseqs2_results_cluster.tsv.* relationship ANNOTATES import/neo4j_headers/protein_to_hmm_header.header import/parsed_domtblout/*.parseddomtblout.* relationship IS_TAXON import/neo4j_headers/assembly_to_taxid.header import/genomic_info/*.assembly_to_taxid.* relationship ROLE_ANN import/neo4j_headers/tigrfam_to_role.header import/tigrfam_info/*.tigrfam_to_role.* relationship ENCODES import/neo4j_headers/locus_to_protein.header import/genomic_info/*.locus_to_protein.* relationship SOURCE_DB import/neo4j_headers/hmm_source_relationships.header import/hmm_info/*..hmminfo.* relationship TAXON_PARENT import/neo4j_headers/taxid_to_taxid.header import/taxdump_process/*.taxid_to_taxid.* relationship PROTEIN_TO_GO import/neo4j_headers/protein_to_go.header import/protein_info/*.protein_to_go.* relationship ASSEMBLES_TO import/neo4j_headers/assembly_to_locus.header import/genomic_info/*.assembly_to_locus.* relationship MAINROLE_ANN import/neo4j_headers/tigrfamrole_to_mainrole.header import/tigrfam_info/*.tigrfamrole_to_mainrole.*"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/","title":"Create a compute instance and install Neo4j","text":"<p>This guide will walk you through getting a very basic setup of getting Neo4j Community Edition installed on an AWS EC2 instance and rehydrating a SocialGene database on it. This guide is not intended for production use, but rather as a starting point (i.e. you are responsible for securing the instance, setting up backups, any costs, etc.). AWS open data datasets are free to access but you will be charged for the EC2 instance, storage, etc.</p> <p>The instance is created with an attached EBS volume that is formatted and mounted at /data. The Neo4j configuration file is updated to use the /data directory for the database and other directories.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#launch-a-new-ec2-instance","title":"Launch a new EC2 instance","text":"<ul> <li>Search for AMI <code>ami-0da49bfc774c02019</code>  (amzn2-ami-ecs-kernel-5.10-hvm-2.0.20240725-x86_64-ebs) (This is the basic Amazon AMI Linux 2 but with Amazon JDK 17 preinstalled which is required for Neo4j)</li> </ul> <ul> <li>Select an instance type. Here a t2.micro is chosen for the example but you will likely want a larger instance type for the larger databases</li> </ul> <ul> <li>Select a key pair login or create a new one</li> <li>In network settings select \"Allow SSH traffic from and \"My IP\" to restrict access to your IP address</li> </ul> <ul> <li>In \"Configure Storage\" select \"Add new volume\" of size enough to hold the database dump and the rehydrated database. For example, the RefSeq database dump is 220 GB and the rehydrated database is 663 GB, so you'd probably want a ~1TB volume. Select edit file systems (bottom right \"eidt\" in screenshot) and select the device name (e.g. /dev/sdb).</li> </ul> <ul> <li>ssh into the instance (replace the .pem filepath below with the one you downloaded in the \"key pair (login)\" step above and the public DNS (e.g. <code>ec2-111-11-11-111.compute-1.amazonaws.com</code>) of your instance)</li> </ul> <p><code>ssh -i socialgene-image.pem ec2-user@ec2-111-11-11-111.compute-1.amazonaws.com</code></p> <p><code>ssh -i REPLACE-ME ec2-user@REPLACE-ME</code></p> <p>Note</p> <p>The second EBS volume defaults to not deleting on termination. If you want to delete the volume when the EC2 instance is terminated, you will need to change the setting in the EC2 console. Without delete on termination you have to manually delete the volume in the EC2 console after terminating the instance. If you don't delete the volume you will continue to be charged for the storage.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#within-the-ec2-instance","title":"Within the EC2 instance","text":"<p>Terminal commands in rest of this tutorial are run within the EC2 instance unless otherwise noted.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#format-and-mount-the-ebs-volume","title":"Format and mount the EBS volume","text":"<p>The second attached EBS volume is mounted at /dev/xvdb (or whichever device name you assigned to the volume during EC2 creation). But the volume is not automatically mounted or formatted. Format the volume with the XFS file system and mount it at /data.</p> <pre><code>sudo mkfs -t xfs /dev/xvdb\nsudo mkdir /data\nsudo mount /dev/xvdb /data\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#install-neo4j","title":"Install Neo4j","text":"<p>Run the following commands to install Neo4j Community Edition v5 on Amazon Linux 2.</p> <p>Setup the Neo4j repository <pre><code>sudo rpm --import https://debian.neo4j.com/neotechnology.gpg.key\nsudo tee /etc/yum.repos.d/neo4j.repo &lt;&lt; EOF\n[neo4j]\nname=Neo4j RPM Repository\nbaseurl=https://yum.neo4j.com/stable/5\nenabled=1\ngpgcheck=1\nEOF\n</code></pre></p> <p>Install Neo4j</p> <pre><code>sudo yum install neo4j -y\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#setup-the-neo4j-configuration-file","title":"Setup the Neo4j configuration file","text":"<p>For simplicity we will mostly use the default Neo4j configuration file in this tutorial. The default configuration file is located at <code>/etc/neo4j/neo4j.conf</code>. We will update the configuration file so that the database data and logs, plugins, etc will be stored and run from the <code>/data</code> directory (the second EBS volume). For production use, you will want to adjust the configuration settings to meet your specific needs.</p> <pre><code>DATA_DIR=/data/neo4j/data\nPLUGINS_DIR=/data/neo4j/plugins\nLOGS_DIR=/data/neo4j/logs\nLIB_DIR=/data/neo4j/lib\nIMPORT_DIR=/data/neo4j/import\nTRANSACTION_LOGS_DIR=/data/neo4j/transaction-logs\n\n# Function to update or add configuration entries\nupdate_config() {\n    local key=$1\n    local value=$2\n    local config_file=\"/etc/neo4j/neo4j.conf\"\n\n    # Use sed to update the configuration if it exists, otherwise add it\n    if sudo grep -q \"^${key}=\" $config_file; then\n        sudo sed -i \"s|^${key}=.*|${key}=${value}|\" $config_file\n    else\n        echo \"${key}=${value}\" | sudo tee -a $config_file &gt; /dev/null\n    fi\n}\n\n# Update or add configuration entries\nupdate_config \"server.directories.data\" $DATA_DIR\nupdate_config \"server.directories.plugins\" $PLUGINS_DIR\nupdate_config \"server.directories.logs\" $LOGS_DIR\nupdate_config \"server.directories.lib\" $LIB_DIR\nupdate_config \"server.directories.import\" $IMPORT_DIR\nupdate_config \"server.directories.transaction.logs.root\" $TRANSACTION_LOGS_DIR\n</code></pre> <p>For the smaller databases you can stream the dump file from S3 and rehydrate the database in one step. For the larger databases you will likely want to download the dump file to the instance before rehydrating the database.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#stream-the-dump-file-from-s3-and-rehydrate-the-database","title":"Stream the dump file from S3 and rehydrate the database","text":"<pre><code>curl -s https://my_bucket.s3.amazonaws.com/socialgene/neo4j.dump - |\\\n    sudo neo4j-admin database load --from-stdin neo4j --overwrite-destination=true\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#download-the-dump-file-from-s3-and-rehydrate-the-database","title":"Download the dump file from S3 and rehydrate the database","text":"<pre><code># Download the dump file from S3\ncurl https://my_bucket.s3.amazonaws.com/socialgene/neo4j.dump &gt; /data/neo4j.dump\n# Rehydrate the database\nsudo neo4j-admin database load --from-path=/data/neo4j.dump -neo4j --overwrite-destination=true\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#run-neo4j","title":"Run Neo4j","text":"<p>Start the Neo4j service and check the status.</p> <pre><code>sudo start neo4j\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/aws_instructions/#access-neo4j","title":"Access Neo4j","text":"<p>Now there is a running SocialGene database on the EC2 instance. The default username and password for the Neo4j browser are <code>neo4j</code>. On first login you will be prompted to change the password.</p> <p>From here you can:</p> <ul> <li>Download SocialGene's Python package and use the <code>socialgene</code> command line tool to interact with the database from within the EC2 instance.</li> <li>Modify the security group settings to allow access to the Neo4j browser from your IP address and access the Neo4j browser from your local machine.<ul> <li>This requires you to open TCP port 7474 and 7687 in the security group settings for the EC2 instance.</li> <li>You can then access the Neo4j browser at <code>http://&lt;public-dns&gt;:7474</code> (replace <code>&lt;public-dns&gt;</code> with the public DNS of your EC2 instance).</li> </ul> </li> <li>Use an SSH tunnel to access the Neo4j browser from your local machine.<ul> <li>Run the following command on your local machine to create an SSH tunnel to the EC2 instance. Replace <code>/path/to/your.pem</code> with the path to your .pem file and <code>&lt;public-dns&gt;</code> with the public DNS of your EC2 instance.</li> <li><code>ssh -i /path/to/your.pem -L 7474:localhost:7474 -L 7687:localhost:7687 ec2-user@ec2-111-11-11-111.compute-1.amazonaws.com</code></li> <li>You can then access the Neo4j browser at <code>http://localhost:7474</code> in your local browser.</li> </ul> </li> </ul> <p>Any example Cypher queries from the documentation can be run in the Neo4j browser. For example, to get 10 genome assemblies you can run the following query:</p> <pre><code>MATCH (b:assembly) RETURN b LIMIT 10\n</code></pre> <p>You will probably want to add indexes to the database. This can be done by installing the SocialGene Python package and then running <code>sg_index</code> in the command line. For example, to add indexes for the assembly, nucleotide, protein, and taxid nodes you can run the following command:</p> <pre><code>NEO4J_USER=neo4j\nNEO4J_PASSWORD=neo4j\nNEO4J_URI=bolt://localhost:7687 # change this if you are not running on localhost\n\nsg_index --labels assembly nucleotide protein taxid\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/local_use/","title":"Introduction to SocialGene using Precomputed databases on AWS","text":"<p>First a reminder that you can always make your own SocialGene database using the Nextflow workflow (https://github.com/socialgene/sgnf) which allows you to easily pull public genomes from NCBI or use your own. It also automates the download and annotation of genomic (or other) proteins with well known HMM databases, and/or your own models.</p> <p>And while I engineered the pipeline to handle hundreds of thousands of genomes, large inputs do require a lot compute, memory, and storage. So, I precomputed a few databases for the community to use and am happy to announce that these are now available on AWS for free download.</p> <p>Those databases include 340,000 genomes from NCBI RefSeq and smaller subsets of that for Actinomycetota, Streptomyces, and Micromonospora.  Additionally, antiSMASH v7 was used to compute BGC regions across all 340,000 RefSeq genomes and we have made that SocialGene database available as well.</p> <p>You can find the space requirements for each of the databases here</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/local_use/#getting-started","title":"Getting started","text":"<p>Note:    To run this tutorial you will need Docker installed (this isn't mandatory, you can install Neo4j manually but it won't be covered here)    I also will assume we are using a Linux machine, but the commands should be similar for MacOS and Windows WSL2.</p> <p>All of the files available on AWS are listed here.</p> <p>We can also see that using the AWS CLI:</p> <pre><code>aws s3 ls socialgene-open-data --recursive --human-readable --no-sign-request\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/local_use/#downloading-the-smallest-database","title":"Downloading the smallest database","text":"<p>Download the smallest database to the local machine: <pre><code>wget https://socialgene-open-data.s3.amazonaws.com/2023_v0.4.1/methods_comparison/methods_comparison.dump\n</code></pre></p> <p>Now we will have a single file in the current directory called <code>methods_comparison.dump</code>. This is a Neo4j database dump file that we will use to create a Neo4j database. More information about how to create and restore SocialGene database dumps can be found here.</p>"},{"location":"precomputed_databases/2023_v0.4.1/aws/local_use/#rehydrating-the-database","title":"Rehydrating the database","text":"<p>To restore the database, we will follow the directions and code provided here.</p> <p>You can replace <code>${PWD}</code> with the path to the directory where you downloaded the database dump file.</p> <pre><code>dump_path=\"${PWD}/methods_comparison.dump\"\nsg_neoloc=\"${PWD}\"\npipeline_version='latest'\n\n# mkdir because the docker image will create directories as root if they don't exist\nmkdir -p $sg_neoloc/data\nmkdir -p $sg_neoloc/logs\nmkdir -p $sg_neoloc/plugins\nmkdir -p $sg_neoloc/conf\nmkdir -p $sg_neoloc/import\n\ndocker run \\\n    --user=$(id -u):$(id -g) \\\n    --interactive \\\n    --tty \\\n    --rm \\\n    --volume=$sg_neoloc/data:/opt/conda/bin/neo4j/data \\\n    --volume=$sg_neoloc/plugins:/opt/conda/bin/neo4j/plugins \\\n    --volume=$sg_neoloc/logs:/opt/conda/bin/neo4j/logs \\\n    --volume=$dump_path:/opt/conda/bin/neo4j/neo4j.dump \\\n    --env NEO4J_AUTH=neo4j/test \\\n    chasemc2/sgnf-sgpy:1.2.2 \\\n        neo4j-admin database load \\\n            --from-path=. \\\n            neo4j\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/aws/local_use/#launching-the-database","title":"Launching the database","text":"<p>Now we can launch the database. Full documentation on this step is available here.</p> <pre><code>sg_neoloc=$PWD\n\nNEO4J_server_memory_heap_initial__size='4600m'\nNEO4J_server_memory_heap_max__size='4600m'\nNEO4J_server_memory_pagecache_size='4g'\n\nmkdir -p $sg_neoloc/conf\n# Allow import and export of files from database\necho 'apoc.export.file.enabled=true' &gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.import.file.enabled=true' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.export.file.use_neo4j_config=false' &gt;&gt; $sg_neoloc/conf/apoc.conf\necho 'apoc.import.file.use_neo4j_config=false' &gt;&gt; $sg_neoloc/conf/apoc.conf\n# Set import/export of files from database to $sg_neoloc/import\necho 'server.directories.import=/opt/conda/bin/neo4j/import' &gt;&gt; $sg_neoloc/conf/neo4j.conf\necho 'server.directories.export=/opt/conda/bin/neo4j/import' &gt;&gt; $sg_neoloc/conf/neo4j.conf\n\ndocker run \\\n    --user=$(id -u):$(id -g) \\\n    -p7474:7474 -p7687:7687 \\\n    -v $sg_neoloc/data:/data \\\n    -v $sg_neoloc/logs:/logs \\\n    -v $sg_neoloc/import:/opt/conda/bin/neo4j/import \\\n    -v $sg_neoloc/plugins:/plugins \\\n    -v $sg_neoloc/conf:/opt/conda/bin/neo4j/conf \\\n        --env NEO4J_AUTH=neo4j/test12345 \\\n        --env NEO4J_PLUGINS='[\"apoc\", \"graph-data-science\"]' \\\n        --env NEO4J_dbms_security_procedures_unrestricted=algo.*,apoc.*,gds.*, \\\n        --env NEO4J_dbms_security_procedures_allowlist=algo.*,apoc.*,gds.* \\\n        --env NEO4J_server_config_strict__validation_enabled=false \\\n        --env NEO4J_server_memory_heap_initial__size=$NEO4J_server_memory_heap_initial__size \\\n        --env NEO4J_server_memory_heap_max__size=$NEO4J_server_memory_heap_max__size \\\n        --env NEO4J_server_memory_pagecache_size=$NEO4J_server_memory_pagecache_size \\\n        --env NEO4J_server_jvm_additional='-XX:+ExitOnOutOfMemoryError' \\\n    neo4j:5.17.0\n</code></pre>"},{"location":"precomputed_databases/2023_v0.4.1/dryad/dryad/","title":"Dryad Data Access","text":"<p>The data for first manuscript has been deposited in the Dryad repository for longer term preservation.</p> <p>The data can be accessed at the following link: https://doi.org/10.5061/dryad.ns1rn8q2k</p>"},{"location":"precomputed_databases/2023_v0.4.1/dryad/dryad/#data-description","title":"Data Description","text":"<p>The included databases are described here.</p> <p>One difference is the full SocialGene RefSeq Neo4j database and Actinomycetota database had to be split into smaller parts for hosting on Dryad. The resulting split files follow the naming convention <code>neo4j_db_refseq_base.dump_split_0</code>, <code>neo4j_db_refseq_base.dump_split_01</code>, ..., <code>neo4j_db_refseq_base.dump_split_23</code>.</p> <p>Before using these two database dumps the files must be merged by downloading all of the parts and concatenating them together.</p> <p>For example: <pre><code>cat neo4j_db_refseq_base.dump_split_* &gt; neo4j_db_refseq_base.dump\nmad5sum -c md5checksums.txt\n</code></pre></p> <p>Dryad only allows for a flat file structure so the files are all in the same \"directory\". The included files are:</p> <ul> <li>md5 checksums<ul> <li><code>md5checksums.txt</code></li> </ul> </li> <li>HMM files (for use with any of the 2023_v0.4.1 dataabses)<ul> <li><code>socialgene_nr_hmms_file_with_cutoffs_1_of_1.hmm.gz</code></li> <li><code>socialgene_nr_hmms_file_without_cutoffs_1_of_1.hmm.gz</code></li> <li><code>hmminfo.tsv.gz</code></li> </ul> </li> <li>Neo4j database dump files<ul> <li>All RefSeq<ul> <li><code>neo4j_db_refseq_base.dump_split_00</code></li> <li><code>neo4j_db_refseq_base.dump_split_01</code></li> <li><code>neo4j_db_refseq_base.dump_split_02</code></li> <li><code>neo4j_db_refseq_base.dump_split_03</code></li> <li><code>neo4j_db_refseq_base.dump_split_04</code></li> <li><code>neo4j_db_refseq_base.dump_split_05</code></li> <li><code>neo4j_db_refseq_base.dump_split_06</code></li> <li><code>neo4j_db_refseq_base.dump_split_07</code></li> <li><code>neo4j_db_refseq_base.dump_split_08</code></li> <li><code>neo4j_db_refseq_base.dump_split_09</code></li> <li><code>neo4j_db_refseq_base.dump_split_10</code></li> <li><code>neo4j_db_refseq_base.dump_split_11</code></li> <li><code>neo4j_db_refseq_base.dump_split_12</code></li> <li><code>neo4j_db_refseq_base.dump_split_13</code></li> <li><code>neo4j_db_refseq_base.dump_split_14</code></li> <li><code>neo4j_db_refseq_base.dump_split_15</code></li> <li><code>neo4j_db_refseq_base.dump_split_16</code></li> <li><code>neo4j_db_refseq_base.dump_split_17</code></li> <li><code>neo4j_db_refseq_base.dump_split_18</code></li> <li><code>neo4j_db_refseq_base.dump_split_19</code></li> <li><code>neo4j_db_refseq_base.dump_split_20</code></li> <li><code>neo4j_db_refseq_base.dump_split_21</code></li> <li><code>neo4j_db_refseq_base.dump_split_22</code></li> <li><code>neo4j_db_refseq_base.dump_split_23</code></li> </ul> </li> <li>All RefSeq Actinomycetota<ul> <li><code>neo4j_db_actinomycetota_base.dump</code></li> <li><code>params_actinomycetota.json</code></li> </ul> </li> <li>All RefSeq Streptomyces<ul> <li><code>neo4j_db_streptomyces_base.dump</code></li> <li><code>params_micromonospora.json</code></li> </ul> </li> <li>All RefSeq Micromonospora<ul> <li><code>neo4j_db_micromonospora_base.dump</code></li> <li><code>params_streptomyces.json</code></li> </ul> </li> <li>All RefSeq antiSMASH-7.0 BGCs<ul> <li><code>neo4j_db_refseq_antismash_bgcs_base.dump</code></li> <li><code>params_refseq_antismash_bgcs.json</code></li> </ul> </li> <li>Three genomes used for protein similarity method comparisons<ul> <li><code>methods_comparison.dump</code></li> <li><code>methods_comparison.json</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"precomputed_databases/2023_v0.4.1/dryad/dryad/#checksums","title":"Checksums","text":"<p>Additionally, there is a single <code>md5checksums.txt</code> file that contains the md5 checksums for all of the files in the dataset. This can be used to verify the integrity of any or all of the files after downloading. This includes the expected md5sum of the concatenated <code>neo4j_db_refseq_base.dump</code> and <code>neo4j_db_actinomycetota_base.dump</code> files.</p>"},{"location":"presentations/presentations/","title":"Posters and Recorded Talks","text":""},{"location":"presentations/presentations/#jasons-talk-at-the-2023-asp-meeting","title":"Jason's talk at the 2023 ASP meeting","text":""},{"location":"presentations/presentations/#chases-poster-at-the-2023-asp-meeting","title":"Chase's poster at the 2023 ASP meeting","text":""},{"location":"presentations/presentations/#chases-poster-at-the-2023-cibm-retreat","title":"Chase's poster at the 2023 CIBM retreat","text":""},{"location":"presentations/presentations/#rosalinds-poster-at-the-2023-uw-madison-psychedelic-symposium","title":"Rosalind's poster at the 2023 UW Madison Psychedelic Symposium","text":""},{"location":"presentations/presentations/#chases-poster-at-the-2022-marine-natural-products-grsgrc","title":"Chase's poster at the 2022 Marine Natural Products GRS/GRC","text":""},{"location":"ultraquickstart/ultraquickstart/","title":"Ultra-Quickstart!","text":""},{"location":"ultraquickstart/ultraquickstart/#preface","title":"Preface","text":"<p>This tutorial assumes you already have Nextflow and Docker installed.</p>"},{"location":"ultraquickstart/ultraquickstart/#create-a-socialgene-database","title":"Create a SocialGene Database","text":""},{"location":"ultraquickstart/ultraquickstart/#pull-the-latest-version-of-socialgenes-nextflow-workflow","title":"Pull the latest version of SocialGene's Nextflow workflow","text":"<pre><code>nextflow pull socialgene/sgnf\n</code></pre>"},{"location":"ultraquickstart/ultraquickstart/#run-the-nextflow-pipeline","title":"Run the Nextflow pipeline","text":"<p>Assign <code>outdir</code> and <code>outdir_download_cache</code> paths below with the paths you want the results to be placed into. Open bash or whatever shell you use, run the commands, and (fingers-crossed) watch the magic happen.</p> shell <pre><code>outdir='/tmp/socialgene_data/ultraquickstart'\noutdir_download_cache='/tmp/socialgene_data/cache'\n\nnextflow run socialgene/sgnf \\\n    -profile ultraquickstart,docker \\\n    --outdir $outdir \\\n    --outdir_download_cache $outdir_download_cache \\\n    --max_cpus 4 \\\n    --max_memory 4.GB \\\n    -resume\n</code></pre> <p>Note: some parameters have changed since video below was recorded</p>"},{"location":"ultraquickstart/ultraquickstart/#nextflow-pipeline-execution-time","title":"Nextflow Pipeline Execution Time","text":"<p>The length of time the pipeline takes relies heavily on the number of cores used and disk speed, so estimates are difficult. On my work desktop (AMD\u00ae Ryzen 9 3900xt 12-core processor) the <code>ultraquickstart</code> config (two genomes) will run start-to-finish in a couple of minutes. For these small runs, downloading Docker images and PFAM can be the longest step if they aren't already cached. MMseqs2 and DIAMOND run times are dependent on the number of input proteins, while HMM annotation depends on both the number of input proteins and number of HMM models.</p> <p>Annotating all Micromonospora genomes (~200) with multiple HMM databases (e.g. PFAM, TIGRFAM, etc.) may take a couple hours. On our server (100 logical cores | 1 TB RAM ) (while also under heavy use by others) using slurm and maximum of 40 logical cores, a couple thousand Streptomyces genomes ran through in just under 24 hours.</p>"},{"location":"ultraquickstart/ultraquickstart/#launch-the-database","title":"Launch the database","text":"<p>Notice that the <code>sg_neoloc</code> path below is the <code>$outdir</code> path from above plus <code>/socialgene_neo4j</code> (the newly created neo4j database directory)</p> shell <pre><code>sg_neoloc='/tmp/socialgene_data/ultraquickstart/socialgene_neo4j'\n\ndocker run \\\n    --user=$(id -u):$(id -g) \\\n    -p7474:7474 -p7687:7687 \\\n    -v $sg_neoloc/data:/data \\\n    -v $sg_neoloc/logs:/logs \\\n    -v $sg_neoloc/import:/var/lib/neo4j/import \\\n    -v $sg_neoloc/plugins:/plugins \\\n    -v $sg_neoloc/conf:/var/lib/neo4j/conf \\\n        --env NEO4J_AUTH=neo4j/test12345 \\\n        --env NEO4J_PLUGINS='[\"apoc\", \"graph-data-science\"]' \\\n        --env NEO4J_dbms_security_procedures_unrestricted=algo.*,apoc.*,n10s.*,gds.*, \\\n        --env NEO4J_dbms_security_procedures_allowlist=algo.*,apoc.*,n10s.*,gds.* \\\n        --env NEO4J_server_config_strict__validation_enabled=false \\\n        --env NEO4J_server_memory_heap_initial__size='4G' \\\n        --env NEO4J_server_memory_heap_max__size='4G' \\\n        --env NEO4J_server_memory_pagecache_size='3G' \\\n        --env NEO4J_server_jvm_additional='-XX:+ExitOnOutOfMemoryError' \\\n    neo4j:5.16.0\n</code></pre> <p>If you get some error about ports being unavailable/used, you'll want to change the line <code>-p7474:7474 -p7687:7687</code>. The first number before the colon is what you'll change, the new number(s) will be what you use for the address below (under \"Look at what you've made!\"). For detailed info about port configuration in NEO4J see: https://neo4j.com/docs/operations-manual/current/configuration/connectors</p>"},{"location":"ultraquickstart/ultraquickstart/#look-at-what-youve-made","title":"Look at what you've made!","text":"<p>Open an internet browser and go to the url: <code>http://localhost:7474</code>.</p> <p>You should see a login screen: </p> <p>The username/password were set inside the <code>docker run</code> command (<code>--env NEO4J_AUTH=neo4j/test12345</code>). In this case the username was <code>neo4j</code> and password was <code>test12345</code>.</p> <p>After authenticating you should be able to see the database entries and start querying the database: </p>"}]}